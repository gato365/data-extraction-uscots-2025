[
  {
    "objectID": "sessions/session_1/libraries_of_packages.html",
    "href": "sessions/session_1/libraries_of_packages.html",
    "title": "Library of Packages",
    "section": "",
    "text": "Chloropleth Map (geoapify.com)\n\n\n\n\nIntroduction to Libraries & Glossary\nThis project, when possible, utilizes tidyverse packages. tidyverse is a collection of open-source packages that are well-integrated to tackle problems of data extraction, manipulation, transformation, exploration, and visualization.\n\n\n\nCollection of Tidyverse packages.\n\n\n\nlibrary(rvest)      # Web scraping\nlibrary(dplyr)      # Data manipulation\nlibrary(stringr)    # String cleaning\nlibrary(rlang)      # Advanced evaluation\nlibrary(purrr)      # Functional tools\nlibrary(ggplot2)    # Visualizations\nlibrary(httr2)      # Makes web requests\nlibrary(tibble)     # Easier and prettier data frames\nlibrary(lubridate)  # Handles dates\nlibrary(dotenv)     # Loads environment variables from .Renviron\nlibrary(glue)       # Easier string concatenation\nlibrary(tigris)     # U.S. shapefiles for mapping\n\nThis is a list of R packages used in this project, accompanied by brief descriptions of what they do.\nrvest\n\nUsed for web scraping: parses HTML/XML documents into a navigable format.\nExtracts structured data using CSS selectors, XPath, or tag-based search (html_table(), html_text(), etc.).\n\n\ndplyr\n\nCore package for tidyverse-style data manipulation (filter(), mutate(), select(), etc.).\nSupports chaining operations with %&gt;% / |&gt;, making data workflows readable and efficient.\n\n\nstringr\n\nProvides a consistent set of functions for string manipulation.\nHandles pattern matching, replacement, splitting, and formatting.\n\n\nrlang\n\nSupports advanced evaluation and programming with tidyverse tools.\nUseful when writing custom functions that dynamically reference or modify variables.\n\n\npurrr\n\nEnables functional programming with mapping tools like map(), map_df(), walk(), etc.\nReplaces (many) for-loops and supports clean iteration over lists and vectors.\n\n\nggplot2\n\nGraphics package for building layered, flexible visualizations.\nSupports various plot types, themes, scales, and faceting for data storytelling.\n\n\nhttr2\n\nModern HTTP client designed for tidyverse-like API interaction.\nReplaces httr with a more intuitive and pipeable interface (request() |&gt; req_perform()).\n\n\ntibble\n\nA modern rethinking of the data.frame that prints cleaner and behaves more predictably.\nDefault in tidyverse workflows; avoids surprises like string-to-factor conversion.\n\n\nlubridate\n\nSimplifies working with dates and times: parsing, formatting, and arithmetic.\nMakes it easy to extract components (e.g., month, weekday) and perform date math.\n\n\ndotenv\n\nLoads environment variables from a .env or .Renviron file into R.\nKeeps sensitive data like API keys out of your scripts and version control.\n\n\nglue\n\nProvides string interpolation (e.g., glue(\"Hello {name}\")).\nCleaner and safer than paste() for building URLs, messages, or SQL queries.\n\n\ntigris\n\nDownloads shapefiles and geographic boundary data (e.g., counties, states) from the U.S. Census Bureau.\nReturns spatial sf data frames, making it easy to map and visualize geographic data."
  },
  {
    "objectID": "sessions/session_4/04_Extraction_FinalActivity.html",
    "href": "sessions/session_4/04_Extraction_FinalActivity.html",
    "title": "Session 4: Final Activity",
    "section": "",
    "text": "Now that we have worked with HTML elements and API calls, let’s put these skills together to create a weather map of Iowa counties based on the maximum temperature at each county’s administrative seat (county seat).\n\n\n\nChloropleth Map (geoapify.com)\n\n\n\n\n\nWe’ll begin by loading the libraries necessary for data manipulation, API calls, and visualization.\n\n# Step 1: Load Libraries\n# Step 1a: General utilities\nlibrary(httr2)       # Makes web requests\nlibrary(tibble)      # Easier and prettier data frames\nlibrary(lubridate)   # Handles dates\nlibrary(ggplot2)     # Data visualization\nlibrary(dplyr)       # Data manipulation\nlibrary(dotenv)      # Loads environment variables from .Renviron\nlibrary(glue)        # Easier string concatenation\nlibrary(purrr)       # Functional programming tools\nlibrary(rvest)       # Web scraping\nlibrary(tigris)      # U.S. shapefiles for mapping\nlibrary(stringr)     # String manipulation and handling\n\n\n# Step 1b: Load API key from .Renviron.txt\ndotenv::load_dot_env(file = \".Renviron.txt\")\n\n\n\n\n\nHere we extract a table from Wikipedia listing all Iowa counties and their corresponding county seats.\n\n# Step 2: Scrape HTML Table from Wikipedia\n# Step 2a: Set URL\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n\n# Step 2b: Read HTML\nwebpage &lt;- url |&gt;  \n  rvest::read_html()\n\n# Step 2c: Extract all HTML tables from the page\nhtml_tables &lt;- webpage |&gt; \n  rvest::html_table()\n\n# Step 2d: Select the correct table (based on inspection)\ntable1 &lt;- html_tables |&gt; \n  purrr::pluck(2)\n\n# Step 2e: Clean and prepare county seat names\n#         Add ', IA, USA' to ensure geolocation works with OpenWeather API\n#         Use only the first County Seat city name when mulitple exist\ncounty_seats_df &lt;- table1 |&gt; \n  mutate(\n    `County seat[4]` = str_split(`County seat[4]`, \" and \") |&gt; sapply(`[`, 1),\n    city = paste0(`County seat[4]`, \", IA, USA\")\n  )\n\nDiscussion: Why would we need to append”, IA, USA” to the city name?\nWhat would happen if we do not?\nA: This helps geocoding APIs return accurate coordinates. (Just “Ames” returns a rural town of 600 in Northern France)\n\n\n\nNot Exactly Ames, IA, USA. Church of Ames, France (Wikipedia)\n\n\n\n\n\n\nWe now define a function to call the OpenWeather Geocoding and One Call APIs. Then we use it to retrieve temperature data for each city.\n\n# Step 3: Define function to get geocoded weather data from OpenWeather\nget_city_weather &lt;- function(city, date = Sys.Date()) {\n  # Step 3a: Get coordinates from geocoding API\n  geo_url &lt;- glue(\n    \"http://api.openweathermap.org/geo/1.0/direct?\",\n    \"q=\", URLencode(city),\n    \"&limit=1&appid=\", Sys.getenv(\"API_KEY\"))\n  geo_response &lt;- req_perform(request(geo_url))\n\n  if (resp_status(geo_response) == 200) {\n    geo_data &lt;- as.data.frame(resp_body_json(geo_response))\n    if (nrow(geo_data) == 0) return(NULL)\n\n    lat &lt;- geo_data$lat\n    lon &lt;- geo_data$lon\n\n    # Step 3b: Call the weather summary API\n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", format(date, \"%Y-%m-%d\"),\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\")\n    weather_response &lt;- req_perform(request(weather_url))\n\n    if (resp_status(weather_response) == 200) {\n      weather_data &lt;- resp_body_json(weather_response)\n      tibble(\n        city = city,\n        date = date,\n        lat = lat,\n        lon = lon,\n        temp_max = weather_data$temperature$max,\n        temp_min = weather_data$temperature$min\n      )\n    } else return(NULL)\n  } else return(NULL)\n}\n\n\n# Step 3c: Apply the function to all county seat cities\ncities &lt;- county_seats_df |&gt; \n  pull(12)\nweather_df &lt;- bind_rows(lapply(cities, get_city_weather))\n\nDiscussion: What happens if one of the cities fails to return a result?\nWe could add error handling or a fallback method, such as purrr::possibly()?\n\n\n\n\nNow we join our temperature data with spatial geometries (map pieces) from the tigris package so we can map it.\n\n# Step 4: Merge weather data into spatial map\n# Step 4a: Join temperature data with counties via county seat\ncounty_temp_df &lt;- county_seats_df |&gt; \n  left_join(weather_df, by = \"city\")  # joins on city (admin chair)\n\n# Step 4b: Load Iowa county shapes from tigris\niowa_counties &lt;- tigris::counties(state = \"IA\", cb = TRUE, class = \"sf\")\n\n# Step 4c: Join temperature data into spatial counties by NAME\n# (Be sure county names match exactly)\niowa_map_filled &lt;- iowa_counties |&gt; \n  left_join(county_temp_df, by = c(\"NAMELSAD\" = \"County\"))\n\nDiscussion: How could we verify that all counties successfully matched?\n\n\n\n\nFinally, we use ggplot2 and geom_sf() to create a choropleth map of Iowa counties filled by temperature.\n\n# Step 5: Plot the map\n# Step 5a: Use fill aesthetic to show temperature per county\n\nggplot(iowa_map_filled) +\n  geom_sf(aes(fill = temp_max), color = \"white\") +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Max Temp (°F)\", na.value = \"grey90\") +\n  labs(\n    title = \"Iowa County Temperatures by County Seat\",\n    subtitle = paste(\"Date:\", unique(weather_df$date)),\n    caption = \"Source: OpenWeather API\"\n  ) +\n  theme_minimal(base_size = 14)\n\nIdeas to expand this visualization:\n- Add interactivity with plotly\n- Animate changes over multiple dates\n- Compare actual temps to historical averages\n- Add city point labels or icons"
  },
  {
    "objectID": "sessions/session_4/04_Extraction_FinalActivity.html#session-4-final-activity",
    "href": "sessions/session_4/04_Extraction_FinalActivity.html#session-4-final-activity",
    "title": "Session 4: Final Activity",
    "section": "",
    "text": "Now that we have worked with HTML elements and API calls, let’s put these skills together to create a weather map of Iowa counties based on the maximum temperature at each county’s administrative seat (county seat).\n\n\n\nChloropleth Map (geoapify.com)\n\n\n\n\n\nWe’ll begin by loading the libraries necessary for data manipulation, API calls, and visualization.\n\n# Step 1: Load Libraries\n# Step 1a: General utilities\nlibrary(httr2)       # Makes web requests\nlibrary(tibble)      # Easier and prettier data frames\nlibrary(lubridate)   # Handles dates\nlibrary(ggplot2)     # Data visualization\nlibrary(dplyr)       # Data manipulation\nlibrary(dotenv)      # Loads environment variables from .Renviron\nlibrary(glue)        # Easier string concatenation\nlibrary(purrr)       # Functional programming tools\nlibrary(rvest)       # Web scraping\nlibrary(tigris)      # U.S. shapefiles for mapping\nlibrary(stringr)     # String manipulation and handling\n\n\n# Step 1b: Load API key from .Renviron.txt\ndotenv::load_dot_env(file = \".Renviron.txt\")\n\n\n\n\n\nHere we extract a table from Wikipedia listing all Iowa counties and their corresponding county seats.\n\n# Step 2: Scrape HTML Table from Wikipedia\n# Step 2a: Set URL\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n\n# Step 2b: Read HTML\nwebpage &lt;- url |&gt;  \n  rvest::read_html()\n\n# Step 2c: Extract all HTML tables from the page\nhtml_tables &lt;- webpage |&gt; \n  rvest::html_table()\n\n# Step 2d: Select the correct table (based on inspection)\ntable1 &lt;- html_tables |&gt; \n  purrr::pluck(2)\n\n# Step 2e: Clean and prepare county seat names\n#         Add ', IA, USA' to ensure geolocation works with OpenWeather API\n#         Use only the first County Seat city name when mulitple exist\ncounty_seats_df &lt;- table1 |&gt; \n  mutate(\n    `County seat[4]` = str_split(`County seat[4]`, \" and \") |&gt; sapply(`[`, 1),\n    city = paste0(`County seat[4]`, \", IA, USA\")\n  )\n\nDiscussion: Why would we need to append”, IA, USA” to the city name?\nWhat would happen if we do not?\nA: This helps geocoding APIs return accurate coordinates. (Just “Ames” returns a rural town of 600 in Northern France)\n\n\n\nNot Exactly Ames, IA, USA. Church of Ames, France (Wikipedia)\n\n\n\n\n\n\nWe now define a function to call the OpenWeather Geocoding and One Call APIs. Then we use it to retrieve temperature data for each city.\n\n# Step 3: Define function to get geocoded weather data from OpenWeather\nget_city_weather &lt;- function(city, date = Sys.Date()) {\n  # Step 3a: Get coordinates from geocoding API\n  geo_url &lt;- glue(\n    \"http://api.openweathermap.org/geo/1.0/direct?\",\n    \"q=\", URLencode(city),\n    \"&limit=1&appid=\", Sys.getenv(\"API_KEY\"))\n  geo_response &lt;- req_perform(request(geo_url))\n\n  if (resp_status(geo_response) == 200) {\n    geo_data &lt;- as.data.frame(resp_body_json(geo_response))\n    if (nrow(geo_data) == 0) return(NULL)\n\n    lat &lt;- geo_data$lat\n    lon &lt;- geo_data$lon\n\n    # Step 3b: Call the weather summary API\n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", format(date, \"%Y-%m-%d\"),\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\")\n    weather_response &lt;- req_perform(request(weather_url))\n\n    if (resp_status(weather_response) == 200) {\n      weather_data &lt;- resp_body_json(weather_response)\n      tibble(\n        city = city,\n        date = date,\n        lat = lat,\n        lon = lon,\n        temp_max = weather_data$temperature$max,\n        temp_min = weather_data$temperature$min\n      )\n    } else return(NULL)\n  } else return(NULL)\n}\n\n\n# Step 3c: Apply the function to all county seat cities\ncities &lt;- county_seats_df |&gt; \n  pull(12)\nweather_df &lt;- bind_rows(lapply(cities, get_city_weather))\n\nDiscussion: What happens if one of the cities fails to return a result?\nWe could add error handling or a fallback method, such as purrr::possibly()?\n\n\n\n\nNow we join our temperature data with spatial geometries (map pieces) from the tigris package so we can map it.\n\n# Step 4: Merge weather data into spatial map\n# Step 4a: Join temperature data with counties via county seat\ncounty_temp_df &lt;- county_seats_df |&gt; \n  left_join(weather_df, by = \"city\")  # joins on city (admin chair)\n\n# Step 4b: Load Iowa county shapes from tigris\niowa_counties &lt;- tigris::counties(state = \"IA\", cb = TRUE, class = \"sf\")\n\n# Step 4c: Join temperature data into spatial counties by NAME\n# (Be sure county names match exactly)\niowa_map_filled &lt;- iowa_counties |&gt; \n  left_join(county_temp_df, by = c(\"NAMELSAD\" = \"County\"))\n\nDiscussion: How could we verify that all counties successfully matched?\n\n\n\n\nFinally, we use ggplot2 and geom_sf() to create a choropleth map of Iowa counties filled by temperature.\n\n# Step 5: Plot the map\n# Step 5a: Use fill aesthetic to show temperature per county\n\nggplot(iowa_map_filled) +\n  geom_sf(aes(fill = temp_max), color = \"white\") +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Max Temp (°F)\", na.value = \"grey90\") +\n  labs(\n    title = \"Iowa County Temperatures by County Seat\",\n    subtitle = paste(\"Date:\", unique(weather_df$date)),\n    caption = \"Source: OpenWeather API\"\n  ) +\n  theme_minimal(base_size = 14)\n\nIdeas to expand this visualization:\n- Add interactivity with plotly\n- Animate changes over multiple dates\n- Compare actual temps to historical averages\n- Add city point labels or icons"
  },
  {
    "objectID": "sessions/session_2/02_Extraction_Weather_Data_API.html",
    "href": "sessions/session_2/02_Extraction_Weather_Data_API.html",
    "title": "Session 2: Weather Data - OpenWeatherAPI",
    "section": "",
    "text": "Explain what an API is and how it supports data extraction Theoretical elements of API\nMake requests to a public API and interpret the JSON response\nUnderstand and apply HTTP status codes and API keys\nWrite clean, readable code to extract and parse API data\n\n\n\n\nPart A. Theoretical ideas of APIs\nNote 1:\n\nThis is not a webdeveloper nor a CS course but with a decent understanding of the logic, you and your students will appreciate the utilizartion of web scrapiing more\nP1.\n\nWhat is an API? (Again)\nIt is the abiluty for software to communicate\n\nQ1: What is its utility of APIs? (multiple choice)\n\nNote 2: A client and server can exist on the same computer. This is often what’s happening in local development (e.g., querying a local database from R)\nP2. Lets go deepeer into understanding Define:\nClient API Server Database\nAction: Clinet makes a request Action: Database provides a response\n(TODO: Create GIF (or Image) of Request and Response that displays actions, its created, you have to actual turn it into a gif becuase it is actual a video)\nP2.\nLets spend some more time on the request and response\nThe client sends a request asking for info (like taylor swift or today’s weather)\nThe server then returns a response which contains:\n\ndata\nmetadata (chatgpt: explain this simply)\nstatus code\n\nSelf Question: Is the request and response (data, metadata, & statuscode) usually a JSON format.\n\nP3. Again JSON is….\nP4. What are Status Codes?\nStatus codes tell you what happened with your request:\n\n100s: Info\n200s: Success (highlight: 200 OK)\n300s: Redirect\n400s: Client error\n500s: Server error\n\n\nNote 3:\n\nEmphasize: In most data APIs, your goal is to get a 200 response.\nUse examples like making up a nonexistent city or artist to show how an API might respond with a 400 or 404.\n\nP5. What type of requests can we make?\nCRUD Framework (Create, Read, Update, Delete)\n\nThough APIs allow all four, Read (GET) is most common in data science.\nRESTful API mapping:\n\nCreate → POST\nRead → GET\nUpdate → PUT/PATCH\nDelete → DELETE\n\n\n(TODO: Create a GIF for each the above that is really illuminating, so GET and POST GIFs, its created, you have to actual turn it into a gif becuase it is actual a video)\nNote 4:\n\nWe’ll focus mostly on GET, and occasionally show POST (e.g., retrieving personalized weather data).\nBriefly mention: Apps like Instagram or Facebook rely on all four CRUD operations—updating posts, deleting comments, etc.\n\nPart B.\n\nRESTful APIs: endpoints, parameters, keys\nAuthentication: tokens, secrets, and environment variables\nStatus codes and error handling (focus on 200, 401, 403, 404)\nJSON structure: nested data and tidy conversion\n\n\n\n\n\nAPI Call (PhoenixNap.com)\n\n\nOverview of different offerings from OpenWeather:\n\n\n\n\n\n\n\n\n\nFeature\nCurrent Weather API\nGeocoding API\nOne Call API\n\n\n\n\nPurpose\nGet current weather for a city or location\nConvert city names to coordinates\nGet full weather data for a coordinate\n\n\nInput\nCity name, city ID, coordinates, or zip\nCity name or zip code\nLatitude & Longitude\n\n\nOutput\nTemperature, conditions, wind, etc.\nLocation info (lat/lon, country, etc.)\nCurrent, hourly, daily, alerts (optionally filtered)\n\n\nCoordinates needed?\nNo\nNo\nYes\n\n\nUnits supported\nstandard, metric, imperial\nN/A\nstandard, metric, imperial\n\n\nEndpoint URL\n/data/2.5/weather\n/geo/1.0/direct\n/data/3.0/onecall\n\n\nUse case\nLightweight current conditions\nFinding lat/lon of cities\nComplete weather view\n\n\n\n\n\n\nlibrary(httr2)       # Makes web requests\nlibrary(tibble)      # Tidyverse version of data.frame\nlibrary(lubridate)   # Time and date handling\nlibrary(ggplot2)     # Visualizations\nlibrary(dplyr)       # Data manipulation\nlibrary(dotenv)      # Load environment variables\nlibrary(glue)        # Attach strings together\n\n\n\n\nUsing dotenv::load_dot_env() we will load our .Renviron.txt that contains our API Key. Using Sys.getenv(\"API_KEY\") we can supply our API Key whenever needed.\n\ndotenv::load_dot_env(file = \".Renviron.txt\")\n# Sys.getenv(\"API_KEY\")\n\n\n\n\n\n\n\ncity_name &lt;- \"San Luis Obispo\"\n\n\n\n\nUsing glue() to attach the base URL, the city name, the API Key, and units together and assign it to current_weather_url\nDiscussion: What is the ‘query’ format vs the URL glue form?\nA: Query format can specify parameters in a more human and modular way, but URL format fosters understanding of HTML and web addresses.\n\n\n\nURL with Query breakdown shown (OneSEOCompany.com)\n\n\n\ncurrent_weather_url &lt;- glue(\"https://api.openweathermap.org/data/2.5/weather?\",\n                              \"q=\", URLencode(city_name),\n                              \"&appid=\", Sys.getenv(\"API_KEY\"),\n                              \"&units=\", \"imperial\")\n# Query Method\n # req &lt;- request(\"https://api.openweathermap.org/data/2.5/weather\") |&gt; \n   #req_url_query(\n     #q = city_name,\n     #appid = Sys.getenv(\"API_KEY\"),\n     #units = \"imperial\"\n  #)\n\n\n\n\nFirst, we assign the URL we created to a request object, called req. Then, we use httr2::req_perform() to call the OpenWeather API and assign the response to a variable. We then use httr2::resp_content_type() to see what our result looks like.\n\nreq &lt;- request(current_weather_url)\nresponse &lt;- req_perform(req)\n\nhttr2::resp_content_type(response)\n\nDiscussion: What does application mean? What is JSON?\n\n\n\nUsing logic, we execute the below code if the status code is 200 (meaning OK, successful). We use httr2::resp_status() to identify the status code. The resulting data is in the JSON form, which appears as a named list sometimes containing other lists within in it, these being “nested.”\nThen, using httr2::resp_body_json(), we get our list of nested lists.\nas.data.frame() converts this list into a data.frame object, and then we print the data frame.\nIf the status code is not 200, we print a message stating that the process has failed.\nDiscussion: What are other status codes we know? What about 404?\n\nif (resp_status(response) == 200) {\n  # Parse JSON\n  result &lt;- resp_body_json(response)\n  \n  # Convert to data frame directly\n  currweather_df &lt;- as.data.frame(result)\n  \n  print(select(currweather_df, name, coord.lon, coord.lat, weather.main, main.temp))\n} else {\n  cat(\"Failed. Status code:\", resp_status(response), \"\\n\")\n}\n\n\n\n\n\nThe Geocode API from OpenWeather retrieves the latitude and longitude for a given city.\n\n\n\n# Step 1: Define function \"geocode\" that accepts the parameter \"city\"\n\ngeocode &lt;- function(city){\n  \n# Step 2: Create API request URL\n  \n  geo_url &lt;- glue(\n  \"http://api.openweathermap.org/geo/1.0/direct?\",\n  \"q=\", URLencode(city),\n  \"&limit=1&appid=\", Sys.getenv(\"API_KEY\")\n)\n # Step 3: Use req_perform() and request() to call the API with the URL request \n\ngeo_response &lt;- req_perform(request(geo_url))\n  \n\n # Step 4: If the status code is 200 (OK), use resp_body_json() to parse our response and as.data.frame to coerce it to data.frame.\n\nif (resp_status(geo_response) == 200) {\n  geo_data_df &lt;- resp_body_json(geo_response) |&gt; \n    as.data.frame()\n  \n  \n# Step 5: Assess if the output has 0 length, meaning no result. If so, stop and display an error message.  \n  \n  if (length(geo_data_df) == 0) {\n    stop(\"City not found. Please check the city name.\")\n  }\n  \n# Step 6: Assign latitude and longitude to variables, and use round() to clip it down to 2 decimal places.\n  \n  lat &lt;- round(geo_data_df$lat, digits = 2)\n  lon &lt;- round(geo_data_df$lon, digits = 2)\n  \n# Step 7: Print a string displaying the city name and latitude / longitude.  \n  \n  return(cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\"))\n   }\n}\n\nDiscussion: What else could be added here? Subtracted? Why the output string?\n\n\n\n\ngeocode(\"Ames, IA, USA\")\n\nGet Coordinates for this City:\n\n\n\nHint: This is Central Park (cuddlynest.com)\n\n\n\n\n\nNow, using Geocoding API, as well as the OneCall 3.0 API, we will get the past 5 days of weather for a city of our choosing.\n\n# Step 1: Use Geocoding API to get lattitude and longitude\n# Step 1a: Construct URL query using city name and API key\ngeo_url &lt;- glue(\n  \"http://api.openweathermap.org/geo/1.0/direct?\",\n  \"q=\", URLencode(city_name),\n  \"&limit=1&appid=\", Sys.getenv(\"API_KEY\")\n)\n\n\n# Step 1b: Define 'numdays' variable to the amount of days back to include weather data \n# Ex: numdays = 5 will retrieve the past 5 days of weather\nnumdays &lt;- 5\n\n\n# Step 1c: \n# Use req_perform() and request() to call the API with the URL request \n\ngeo_response &lt;- req_perform(request(geo_url))\n\n# Step 1d: If the status code is 200 (OK), use resp_body_json() to parse our response and as.data.frame to coerce it to data.frame.\n\nif (resp_status(geo_response) == 200) {\n  geo_data_df &lt;- resp_body_json(geo_response) |&gt; \n    as.data.frame()\n  \n# Step 1e: Assess if the output has 0 length, meaning no result. If so, stop and display an error message.  \n  if (length(geo_data_df) == 0) {\n    stop(\"City not found. Please check the city name.\")\n  }\n  \n# Step 1f: Assign latitude and longitude to variables, and use round() to clip it down to 2 decimal places.\n  \n  lat &lt;- round(geo_data_df$lat, digits = 2)\n  lon &lt;- round(geo_data_df$lon, digits = 2)\n  \n# Optional: Print a string displaying the city name and latitude / longitude.  \n  cat(\"Coordinates for\", city_name, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n  \n  \n# Step 2: Use the One Call API to get the past 5 days of weather data  \n  \n# Step 2a: Define the date range using variable 'numdays'\n\n  date_range &lt;- as.character(lubridate::today() - days(1:numdays))\n  \n# Step 2b: Initialize data frame to hold the outputs. \"hist_weather\" for historical weather data.\n  hist_weather_df &lt;- data.frame()\n  \n# Step 2c: Loop over dates and make an API call for each day. For every date in the date vector, supply latitude, longitude, the different date, API key, and provide unit preference.\n  for (date in date_range) {\n    \n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", date,\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\"\n    )\n    \n# Step 2d: Make the API call using the different weather_url queries for each date. Store these in weather_response.    \n    \n    weather_response &lt;- req_perform(request(weather_url))\n    \n# Step 2e: Use logic to evaluate the response and use resp_body_JSON() and as.data.frame. to parse our response and coerce to data.frame.\n    \n    if (resp_status(weather_response) == 200) {\n      daily_weather_df &lt;- resp_body_json(weather_response) |&gt; \n        as.data.frame()\n      \n# Step 2f: Add date and city name columns using mutate()\n      daily_weather_df &lt;- daily_weather_df |&gt; \n        mutate(\n      city =city_name,\n      date = date)\n      \n      \n# Step 2g: Use bind_rows to add all the rows to the hist_weather data frame.\n      hist_weather_df &lt;- bind_rows(hist_weather_df, daily_weather_df)\n      \n    } else {\n# Step 2i: Use logic (else) to print an error message for when weather data is not obtained.\n      warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(weather_response)))\n    }\n  }\n  \n  print(hist_weather_df)\n  \n} else {\n  stop(\"Geocoding failed. Check your API key or city name.\")\n}\n\nDiscussion: Why wrap as.character() around the date range?\nA: If not, dates will mathematically evaluate and become integers.\n\n\n\nJSON Key\nR Type\nNotes\n\n\n\n\ntemperature.min\nnumeric\nMinimum temp for the day\n\n\ntemperature.max\nnumeric\nMaximum temp for the day\n\n\nwind.max.speed\nnumeric\nPeak wind speed\n\n\ndate (added)\nDate\nFrom loop date\n\n\ncity (added)\ncharacter\nCity name from geocoding\n\n\n\nThe API returns a JSON object representing summary statistics for a specific location and date. The root of the JSON is a named list.\nThe list contains other lists such as temperature, which have multiple items related to temperature.\nThese all flatten into column names like precipitation.total, dew_point.afternoon when as.data.frame() is used to coerce.\n\n\n\n\n\n\n\n\n\n\ntemperature.min\ntemperature.max\nwind.max.speed\ndate\ncity\n\n\n\n\n50.1\n75.6\n12.3\n2025-04-23\nSan Luis Obispo\n\n\n\nDiscussion: Any thoughts at this point? What benefits do we get from inspecting the data structure?\n\n\n\n\nprev_weather &lt;- function(city, days){\n\n# Step 1: Use Geocoding API to get lattitude and longitude\n# Step 1a: Construct URL query using city name and API key\ngeo_url &lt;- glue(\n  \"http://api.openweathermap.org/geo/1.0/direct?\",\n  \"q=\", URLencode(city),\n  \"&limit=1&appid=\", Sys.getenv(\"API_KEY\")\n)\n# Step 1c: \ngeo_response &lt;- req_perform(request(geo_url))\n\n# Step 1d: If the status code is 200 (OK), use resp_body_json() to parse our response and as.data.frame to coerce it to data.frame.\n\nif (resp_status(geo_response) == 200) {\n  geo_data_df&lt;- resp_body_json(geo_response) |&gt; \n    as.data.frame()\n  \n# Step 1e: Assess if the output has 0 length, meaning no result. If so, stop and display an error message.  \n  if (length(geo_data_df) == 0) {\n    stop(\"City not found. Please check the city name.\")\n  }\n  \n# Step 1f: Assign latitude and longitude to variables, and use round() to clip it down to 2 decimal places.\n  \n  lat &lt;- round(geo_data_df$lat, digits = 2)\n  lon &lt;- round(geo_data_df$lon, digits = 2)\n  \n# Optional: Print a string displaying the city name and latitude / longitude.  \n  cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n  \n  \n# Step 2: Use the One Call API to get the past 5 days of weather data  \n  \n# Step 2a: Define the date range using variable 'numdays'\n\n  date_range &lt;- as.character(lubridate::today() - days(1:days))\n  \n# Step 2b: Initialize data frame to hold the outputs. \"hist_weather\" for historical weather data.\n  hist_weather_df &lt;- data.frame()\n  \n# Step 2c: Loop over dates and make an API call for each day. For every date in the date vector, supply latitude, longitude, the different date, API key, and provide unit preference.\n  for (date in date_range) {\n    \n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", date,\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\"\n    )\n    \n# Step 2d: Make the API call using the different weather_url queries for each date. Store these in weather_response.    \n    \n    weather_response &lt;- req_perform(request(weather_url))\n    \n# Step 2e: Use logic to evaluate the response and use fromJSON() to get the content from the JSON output and use \"flatten = TRUE\" to unnest the data.\n    \n    if (resp_status(weather_response) == 200) {\n      daily_weather_df &lt;- resp_body_json(weather_response) |&gt; \n        as.data.frame()\n      \n# Step 2f: Add date and city name columns using mutate()\n      daily_weather_df &lt;- daily_weather_df |&gt; \n        mutate(\n      city =city,\n      date = date)\n      \n      \n# Step 2g: Use bind_rows to add all the rows to the hist_weather data frame.\n      hist_weather_df &lt;- bind_rows(hist_weather_df, daily_weather_df)\n      \n    } else {\n# Step 2i: Use logic (else) to print an error message for when weather data is not obtained.\n      warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(weather_response)))\n    }\n  }\n  return(hist_weather_df)\n \n} else {\n  stop(\"Geocoding failed. Check your API key or city name.\")\n}\n\n}\n\nDiscussion: What is UTF-8 encoding?\n\n\n\nComparing Unicode and UTF-8 (Hello-Algo.com)\n\n\n\nprev_weather(\"San Luis Obispo\", 4)\n\n\n\n\n\ndf4 &lt;- prev_weather(\"Ames, IA, USA\", 4) |&gt;  mutate(date = as.Date(date))\n\nggplot(df4, aes(x = date)) +\n  geom_line(aes(y = temperature.max, color = \"High\"), linewidth = 1.2) +\n  geom_line(aes(y = temperature.min, color = \"Low\"), linewidth = 1.2) +\n  scale_color_manual(values = c(\"High\" = \"red\", \"Low\" = \"blue\")) +\n  labs(\n    title = paste(\"High and Low Temperatures in\", unique(df4$city)),\n    x = \"Date\",\n    y = \"Temperature (°F)\",\n    color = \"Temperature Type\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n\n\ncities &lt;- c(\"San Luis Obispo\", \"Santa Barbara\", \"San Francisco\")\n\n# Function to get coordinates + weather summary\nget_city_weather &lt;- function(city, date = Sys.Date()) {\n  geo_url &lt;- glue(\n    \"http://api.openweathermap.org/geo/1.0/direct?\",\n    \"q=\", URLencode(city),\n    \"&limit=1&appid=\", Sys.getenv(\"API_KEY\")\n  )\n  \n  geo_response &lt;- req_perform(request((geo_url)))\n  \n  if (resp_status(geo_response) == 200) {\n    geo_data_df&lt;- as.data.frame(resp_body_json(geo_response))\n    \n    if (length(geo_data_df) == 0) return(NULL)\n    \n    lat &lt;- geo_data_df$lat\n    lon &lt;- geo_data_df$lon\n    \n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", format(date, \"%Y-%m-%d\"),\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\"\n    )\n    \n    weather_response &lt;- req_perform(request((weather_url)))\n    \n    if (resp_status(weather_response) == 200) {\n      weather_data &lt;- (resp_body_json(weather_response))\n      \n      tibble(\n        city = city,\n        date = date,\n        lat = lat,\n        lon = lon,\n        temp_max = weather_data$temperature$max,\n        temp_min = weather_data$temperature$min\n      )\n    } else return(NULL)\n  } else return(NULL)\n}\n\n# Fetch for all cities\nweather_df &lt;- bind_rows(lapply(cities, get_city_weather))\n\n\nweather_sf &lt;- weather_df %&gt;%\n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n\n\nca &lt;- ne_states(country = \"United States of America\", returnclass = \"sf\") %&gt;%\n  filter(name == \"California\")\n\nggplot() +\n  geom_sf(data = ca, fill = \"gray95\", color = \"gray60\") +\n  geom_sf(data = weather_sf, aes(color = temp_max), size = 6) +\n  geom_sf_label(data = weather_sf, aes(label = city), nudge_y = 0.5, size = 2.5) +\n  scale_color_viridis_c(option = \"plasma\", name = \"High Temp (°F)\") +\n  labs(\n    title = \"California City Temperatures\",\n    subtitle = paste(\"Date:\", Sys.Date())\n  ) +\n  theme_minimal()\n\n\n\n\nMap of California With Cities\n\n\n\n\n\n\n\n\nWeather API (e.g., OpenWeatherMap):\n\nRetrieve current weather for participant’s hometown\nModify query parameters (e.g., units, location)\nParse and visualize simple results (e.g., temperature, humidity)\n\nScaffold activity: prewritten functions + one blank section\n\n\n\n\n\nWhere could API data naturally integrate in your curriculum?\nWhat are the pitfalls (rate limits, authentication) students need to know?\n\n\n\n\n\nWhat is an API? Examples (Spotify, Weather, one bonus example)\n\n\nImportance of code flexibility and the fragility of external sources (e.g., Spotify anecdote)\nTidy data principles: naming conventions, structure, and data types\n\n\nBasic API structure: request URLs, endpoints, tokens\nHTTP protocols and status codes\nCRUD operations (Create, Read, Update, Delete)\nAPI best practices (e.g., pagination, authentication, caching)\nTidyverse-friendly workflows (avoid deep nesting, use readable steps)\nActivity:\n\nModify API request (e.g., hometown weather)\nScaffolded practice (fill-in-the-blank)\nOptional take-home transformation/visualization"
  },
  {
    "objectID": "sessions/session_2/02_Extraction_Weather_Data_API.html#session-2-api-fundamentals",
    "href": "sessions/session_2/02_Extraction_Weather_Data_API.html#session-2-api-fundamentals",
    "title": "Session 2: Weather Data - OpenWeatherAPI",
    "section": "",
    "text": "Explain what an API is and how it supports data extraction Theoretical elements of API\nMake requests to a public API and interpret the JSON response\nUnderstand and apply HTTP status codes and API keys\nWrite clean, readable code to extract and parse API data\n\n\n\n\nPart A. Theoretical ideas of APIs\nNote 1:\n\nThis is not a webdeveloper nor a CS course but with a decent understanding of the logic, you and your students will appreciate the utilizartion of web scrapiing more\nP1.\n\nWhat is an API? (Again)\nIt is the abiluty for software to communicate\n\nQ1: What is its utility of APIs? (multiple choice)\n\nNote 2: A client and server can exist on the same computer. This is often what’s happening in local development (e.g., querying a local database from R)\nP2. Lets go deepeer into understanding Define:\nClient API Server Database\nAction: Clinet makes a request Action: Database provides a response\n(TODO: Create GIF (or Image) of Request and Response that displays actions, its created, you have to actual turn it into a gif becuase it is actual a video)\nP2.\nLets spend some more time on the request and response\nThe client sends a request asking for info (like taylor swift or today’s weather)\nThe server then returns a response which contains:\n\ndata\nmetadata (chatgpt: explain this simply)\nstatus code\n\nSelf Question: Is the request and response (data, metadata, & statuscode) usually a JSON format.\n\nP3. Again JSON is….\nP4. What are Status Codes?\nStatus codes tell you what happened with your request:\n\n100s: Info\n200s: Success (highlight: 200 OK)\n300s: Redirect\n400s: Client error\n500s: Server error\n\n\nNote 3:\n\nEmphasize: In most data APIs, your goal is to get a 200 response.\nUse examples like making up a nonexistent city or artist to show how an API might respond with a 400 or 404.\n\nP5. What type of requests can we make?\nCRUD Framework (Create, Read, Update, Delete)\n\nThough APIs allow all four, Read (GET) is most common in data science.\nRESTful API mapping:\n\nCreate → POST\nRead → GET\nUpdate → PUT/PATCH\nDelete → DELETE\n\n\n(TODO: Create a GIF for each the above that is really illuminating, so GET and POST GIFs, its created, you have to actual turn it into a gif becuase it is actual a video)\nNote 4:\n\nWe’ll focus mostly on GET, and occasionally show POST (e.g., retrieving personalized weather data).\nBriefly mention: Apps like Instagram or Facebook rely on all four CRUD operations—updating posts, deleting comments, etc.\n\nPart B.\n\nRESTful APIs: endpoints, parameters, keys\nAuthentication: tokens, secrets, and environment variables\nStatus codes and error handling (focus on 200, 401, 403, 404)\nJSON structure: nested data and tidy conversion\n\n\n\n\n\nAPI Call (PhoenixNap.com)\n\n\nOverview of different offerings from OpenWeather:\n\n\n\n\n\n\n\n\n\nFeature\nCurrent Weather API\nGeocoding API\nOne Call API\n\n\n\n\nPurpose\nGet current weather for a city or location\nConvert city names to coordinates\nGet full weather data for a coordinate\n\n\nInput\nCity name, city ID, coordinates, or zip\nCity name or zip code\nLatitude & Longitude\n\n\nOutput\nTemperature, conditions, wind, etc.\nLocation info (lat/lon, country, etc.)\nCurrent, hourly, daily, alerts (optionally filtered)\n\n\nCoordinates needed?\nNo\nNo\nYes\n\n\nUnits supported\nstandard, metric, imperial\nN/A\nstandard, metric, imperial\n\n\nEndpoint URL\n/data/2.5/weather\n/geo/1.0/direct\n/data/3.0/onecall\n\n\nUse case\nLightweight current conditions\nFinding lat/lon of cities\nComplete weather view\n\n\n\n\n\n\nlibrary(httr2)       # Makes web requests\nlibrary(tibble)      # Tidyverse version of data.frame\nlibrary(lubridate)   # Time and date handling\nlibrary(ggplot2)     # Visualizations\nlibrary(dplyr)       # Data manipulation\nlibrary(dotenv)      # Load environment variables\nlibrary(glue)        # Attach strings together\n\n\n\n\nUsing dotenv::load_dot_env() we will load our .Renviron.txt that contains our API Key. Using Sys.getenv(\"API_KEY\") we can supply our API Key whenever needed.\n\ndotenv::load_dot_env(file = \".Renviron.txt\")\n# Sys.getenv(\"API_KEY\")\n\n\n\n\n\n\n\ncity_name &lt;- \"San Luis Obispo\"\n\n\n\n\nUsing glue() to attach the base URL, the city name, the API Key, and units together and assign it to current_weather_url\nDiscussion: What is the ‘query’ format vs the URL glue form?\nA: Query format can specify parameters in a more human and modular way, but URL format fosters understanding of HTML and web addresses.\n\n\n\nURL with Query breakdown shown (OneSEOCompany.com)\n\n\n\ncurrent_weather_url &lt;- glue(\"https://api.openweathermap.org/data/2.5/weather?\",\n                              \"q=\", URLencode(city_name),\n                              \"&appid=\", Sys.getenv(\"API_KEY\"),\n                              \"&units=\", \"imperial\")\n# Query Method\n # req &lt;- request(\"https://api.openweathermap.org/data/2.5/weather\") |&gt; \n   #req_url_query(\n     #q = city_name,\n     #appid = Sys.getenv(\"API_KEY\"),\n     #units = \"imperial\"\n  #)\n\n\n\n\nFirst, we assign the URL we created to a request object, called req. Then, we use httr2::req_perform() to call the OpenWeather API and assign the response to a variable. We then use httr2::resp_content_type() to see what our result looks like.\n\nreq &lt;- request(current_weather_url)\nresponse &lt;- req_perform(req)\n\nhttr2::resp_content_type(response)\n\nDiscussion: What does application mean? What is JSON?\n\n\n\nUsing logic, we execute the below code if the status code is 200 (meaning OK, successful). We use httr2::resp_status() to identify the status code. The resulting data is in the JSON form, which appears as a named list sometimes containing other lists within in it, these being “nested.”\nThen, using httr2::resp_body_json(), we get our list of nested lists.\nas.data.frame() converts this list into a data.frame object, and then we print the data frame.\nIf the status code is not 200, we print a message stating that the process has failed.\nDiscussion: What are other status codes we know? What about 404?\n\nif (resp_status(response) == 200) {\n  # Parse JSON\n  result &lt;- resp_body_json(response)\n  \n  # Convert to data frame directly\n  currweather_df &lt;- as.data.frame(result)\n  \n  print(select(currweather_df, name, coord.lon, coord.lat, weather.main, main.temp))\n} else {\n  cat(\"Failed. Status code:\", resp_status(response), \"\\n\")\n}\n\n\n\n\n\nThe Geocode API from OpenWeather retrieves the latitude and longitude for a given city.\n\n\n\n# Step 1: Define function \"geocode\" that accepts the parameter \"city\"\n\ngeocode &lt;- function(city){\n  \n# Step 2: Create API request URL\n  \n  geo_url &lt;- glue(\n  \"http://api.openweathermap.org/geo/1.0/direct?\",\n  \"q=\", URLencode(city),\n  \"&limit=1&appid=\", Sys.getenv(\"API_KEY\")\n)\n # Step 3: Use req_perform() and request() to call the API with the URL request \n\ngeo_response &lt;- req_perform(request(geo_url))\n  \n\n # Step 4: If the status code is 200 (OK), use resp_body_json() to parse our response and as.data.frame to coerce it to data.frame.\n\nif (resp_status(geo_response) == 200) {\n  geo_data_df &lt;- resp_body_json(geo_response) |&gt; \n    as.data.frame()\n  \n  \n# Step 5: Assess if the output has 0 length, meaning no result. If so, stop and display an error message.  \n  \n  if (length(geo_data_df) == 0) {\n    stop(\"City not found. Please check the city name.\")\n  }\n  \n# Step 6: Assign latitude and longitude to variables, and use round() to clip it down to 2 decimal places.\n  \n  lat &lt;- round(geo_data_df$lat, digits = 2)\n  lon &lt;- round(geo_data_df$lon, digits = 2)\n  \n# Step 7: Print a string displaying the city name and latitude / longitude.  \n  \n  return(cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\"))\n   }\n}\n\nDiscussion: What else could be added here? Subtracted? Why the output string?\n\n\n\n\ngeocode(\"Ames, IA, USA\")\n\nGet Coordinates for this City:\n\n\n\nHint: This is Central Park (cuddlynest.com)\n\n\n\n\n\nNow, using Geocoding API, as well as the OneCall 3.0 API, we will get the past 5 days of weather for a city of our choosing.\n\n# Step 1: Use Geocoding API to get lattitude and longitude\n# Step 1a: Construct URL query using city name and API key\ngeo_url &lt;- glue(\n  \"http://api.openweathermap.org/geo/1.0/direct?\",\n  \"q=\", URLencode(city_name),\n  \"&limit=1&appid=\", Sys.getenv(\"API_KEY\")\n)\n\n\n# Step 1b: Define 'numdays' variable to the amount of days back to include weather data \n# Ex: numdays = 5 will retrieve the past 5 days of weather\nnumdays &lt;- 5\n\n\n# Step 1c: \n# Use req_perform() and request() to call the API with the URL request \n\ngeo_response &lt;- req_perform(request(geo_url))\n\n# Step 1d: If the status code is 200 (OK), use resp_body_json() to parse our response and as.data.frame to coerce it to data.frame.\n\nif (resp_status(geo_response) == 200) {\n  geo_data_df &lt;- resp_body_json(geo_response) |&gt; \n    as.data.frame()\n  \n# Step 1e: Assess if the output has 0 length, meaning no result. If so, stop and display an error message.  \n  if (length(geo_data_df) == 0) {\n    stop(\"City not found. Please check the city name.\")\n  }\n  \n# Step 1f: Assign latitude and longitude to variables, and use round() to clip it down to 2 decimal places.\n  \n  lat &lt;- round(geo_data_df$lat, digits = 2)\n  lon &lt;- round(geo_data_df$lon, digits = 2)\n  \n# Optional: Print a string displaying the city name and latitude / longitude.  \n  cat(\"Coordinates for\", city_name, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n  \n  \n# Step 2: Use the One Call API to get the past 5 days of weather data  \n  \n# Step 2a: Define the date range using variable 'numdays'\n\n  date_range &lt;- as.character(lubridate::today() - days(1:numdays))\n  \n# Step 2b: Initialize data frame to hold the outputs. \"hist_weather\" for historical weather data.\n  hist_weather_df &lt;- data.frame()\n  \n# Step 2c: Loop over dates and make an API call for each day. For every date in the date vector, supply latitude, longitude, the different date, API key, and provide unit preference.\n  for (date in date_range) {\n    \n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", date,\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\"\n    )\n    \n# Step 2d: Make the API call using the different weather_url queries for each date. Store these in weather_response.    \n    \n    weather_response &lt;- req_perform(request(weather_url))\n    \n# Step 2e: Use logic to evaluate the response and use resp_body_JSON() and as.data.frame. to parse our response and coerce to data.frame.\n    \n    if (resp_status(weather_response) == 200) {\n      daily_weather_df &lt;- resp_body_json(weather_response) |&gt; \n        as.data.frame()\n      \n# Step 2f: Add date and city name columns using mutate()\n      daily_weather_df &lt;- daily_weather_df |&gt; \n        mutate(\n      city =city_name,\n      date = date)\n      \n      \n# Step 2g: Use bind_rows to add all the rows to the hist_weather data frame.\n      hist_weather_df &lt;- bind_rows(hist_weather_df, daily_weather_df)\n      \n    } else {\n# Step 2i: Use logic (else) to print an error message for when weather data is not obtained.\n      warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(weather_response)))\n    }\n  }\n  \n  print(hist_weather_df)\n  \n} else {\n  stop(\"Geocoding failed. Check your API key or city name.\")\n}\n\nDiscussion: Why wrap as.character() around the date range?\nA: If not, dates will mathematically evaluate and become integers.\n\n\n\nJSON Key\nR Type\nNotes\n\n\n\n\ntemperature.min\nnumeric\nMinimum temp for the day\n\n\ntemperature.max\nnumeric\nMaximum temp for the day\n\n\nwind.max.speed\nnumeric\nPeak wind speed\n\n\ndate (added)\nDate\nFrom loop date\n\n\ncity (added)\ncharacter\nCity name from geocoding\n\n\n\nThe API returns a JSON object representing summary statistics for a specific location and date. The root of the JSON is a named list.\nThe list contains other lists such as temperature, which have multiple items related to temperature.\nThese all flatten into column names like precipitation.total, dew_point.afternoon when as.data.frame() is used to coerce.\n\n\n\n\n\n\n\n\n\n\ntemperature.min\ntemperature.max\nwind.max.speed\ndate\ncity\n\n\n\n\n50.1\n75.6\n12.3\n2025-04-23\nSan Luis Obispo\n\n\n\nDiscussion: Any thoughts at this point? What benefits do we get from inspecting the data structure?\n\n\n\n\nprev_weather &lt;- function(city, days){\n\n# Step 1: Use Geocoding API to get lattitude and longitude\n# Step 1a: Construct URL query using city name and API key\ngeo_url &lt;- glue(\n  \"http://api.openweathermap.org/geo/1.0/direct?\",\n  \"q=\", URLencode(city),\n  \"&limit=1&appid=\", Sys.getenv(\"API_KEY\")\n)\n# Step 1c: \ngeo_response &lt;- req_perform(request(geo_url))\n\n# Step 1d: If the status code is 200 (OK), use resp_body_json() to parse our response and as.data.frame to coerce it to data.frame.\n\nif (resp_status(geo_response) == 200) {\n  geo_data_df&lt;- resp_body_json(geo_response) |&gt; \n    as.data.frame()\n  \n# Step 1e: Assess if the output has 0 length, meaning no result. If so, stop and display an error message.  \n  if (length(geo_data_df) == 0) {\n    stop(\"City not found. Please check the city name.\")\n  }\n  \n# Step 1f: Assign latitude and longitude to variables, and use round() to clip it down to 2 decimal places.\n  \n  lat &lt;- round(geo_data_df$lat, digits = 2)\n  lon &lt;- round(geo_data_df$lon, digits = 2)\n  \n# Optional: Print a string displaying the city name and latitude / longitude.  \n  cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n  \n  \n# Step 2: Use the One Call API to get the past 5 days of weather data  \n  \n# Step 2a: Define the date range using variable 'numdays'\n\n  date_range &lt;- as.character(lubridate::today() - days(1:days))\n  \n# Step 2b: Initialize data frame to hold the outputs. \"hist_weather\" for historical weather data.\n  hist_weather_df &lt;- data.frame()\n  \n# Step 2c: Loop over dates and make an API call for each day. For every date in the date vector, supply latitude, longitude, the different date, API key, and provide unit preference.\n  for (date in date_range) {\n    \n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", date,\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\"\n    )\n    \n# Step 2d: Make the API call using the different weather_url queries for each date. Store these in weather_response.    \n    \n    weather_response &lt;- req_perform(request(weather_url))\n    \n# Step 2e: Use logic to evaluate the response and use fromJSON() to get the content from the JSON output and use \"flatten = TRUE\" to unnest the data.\n    \n    if (resp_status(weather_response) == 200) {\n      daily_weather_df &lt;- resp_body_json(weather_response) |&gt; \n        as.data.frame()\n      \n# Step 2f: Add date and city name columns using mutate()\n      daily_weather_df &lt;- daily_weather_df |&gt; \n        mutate(\n      city =city,\n      date = date)\n      \n      \n# Step 2g: Use bind_rows to add all the rows to the hist_weather data frame.\n      hist_weather_df &lt;- bind_rows(hist_weather_df, daily_weather_df)\n      \n    } else {\n# Step 2i: Use logic (else) to print an error message for when weather data is not obtained.\n      warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(weather_response)))\n    }\n  }\n  return(hist_weather_df)\n \n} else {\n  stop(\"Geocoding failed. Check your API key or city name.\")\n}\n\n}\n\nDiscussion: What is UTF-8 encoding?\n\n\n\nComparing Unicode and UTF-8 (Hello-Algo.com)\n\n\n\nprev_weather(\"San Luis Obispo\", 4)\n\n\n\n\n\ndf4 &lt;- prev_weather(\"Ames, IA, USA\", 4) |&gt;  mutate(date = as.Date(date))\n\nggplot(df4, aes(x = date)) +\n  geom_line(aes(y = temperature.max, color = \"High\"), linewidth = 1.2) +\n  geom_line(aes(y = temperature.min, color = \"Low\"), linewidth = 1.2) +\n  scale_color_manual(values = c(\"High\" = \"red\", \"Low\" = \"blue\")) +\n  labs(\n    title = paste(\"High and Low Temperatures in\", unique(df4$city)),\n    x = \"Date\",\n    y = \"Temperature (°F)\",\n    color = \"Temperature Type\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n\n\ncities &lt;- c(\"San Luis Obispo\", \"Santa Barbara\", \"San Francisco\")\n\n# Function to get coordinates + weather summary\nget_city_weather &lt;- function(city, date = Sys.Date()) {\n  geo_url &lt;- glue(\n    \"http://api.openweathermap.org/geo/1.0/direct?\",\n    \"q=\", URLencode(city),\n    \"&limit=1&appid=\", Sys.getenv(\"API_KEY\")\n  )\n  \n  geo_response &lt;- req_perform(request((geo_url)))\n  \n  if (resp_status(geo_response) == 200) {\n    geo_data_df&lt;- as.data.frame(resp_body_json(geo_response))\n    \n    if (length(geo_data_df) == 0) return(NULL)\n    \n    lat &lt;- geo_data_df$lat\n    lon &lt;- geo_data_df$lon\n    \n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", format(date, \"%Y-%m-%d\"),\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\"\n    )\n    \n    weather_response &lt;- req_perform(request((weather_url)))\n    \n    if (resp_status(weather_response) == 200) {\n      weather_data &lt;- (resp_body_json(weather_response))\n      \n      tibble(\n        city = city,\n        date = date,\n        lat = lat,\n        lon = lon,\n        temp_max = weather_data$temperature$max,\n        temp_min = weather_data$temperature$min\n      )\n    } else return(NULL)\n  } else return(NULL)\n}\n\n# Fetch for all cities\nweather_df &lt;- bind_rows(lapply(cities, get_city_weather))\n\n\nweather_sf &lt;- weather_df %&gt;%\n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n\n\nca &lt;- ne_states(country = \"United States of America\", returnclass = \"sf\") %&gt;%\n  filter(name == \"California\")\n\nggplot() +\n  geom_sf(data = ca, fill = \"gray95\", color = \"gray60\") +\n  geom_sf(data = weather_sf, aes(color = temp_max), size = 6) +\n  geom_sf_label(data = weather_sf, aes(label = city), nudge_y = 0.5, size = 2.5) +\n  scale_color_viridis_c(option = \"plasma\", name = \"High Temp (°F)\") +\n  labs(\n    title = \"California City Temperatures\",\n    subtitle = paste(\"Date:\", Sys.Date())\n  ) +\n  theme_minimal()\n\n\n\n\nMap of California With Cities\n\n\n\n\n\n\n\n\nWeather API (e.g., OpenWeatherMap):\n\nRetrieve current weather for participant’s hometown\nModify query parameters (e.g., units, location)\nParse and visualize simple results (e.g., temperature, humidity)\n\nScaffold activity: prewritten functions + one blank section\n\n\n\n\n\nWhere could API data naturally integrate in your curriculum?\nWhat are the pitfalls (rate limits, authentication) students need to know?\n\n\n\n\n\nWhat is an API? Examples (Spotify, Weather, one bonus example)\n\n\nImportance of code flexibility and the fragility of external sources (e.g., Spotify anecdote)\nTidy data principles: naming conventions, structure, and data types\n\n\nBasic API structure: request URLs, endpoints, tokens\nHTTP protocols and status codes\nCRUD operations (Create, Read, Update, Delete)\nAPI best practices (e.g., pagination, authentication, caching)\nTidyverse-friendly workflows (avoid deep nesting, use readable steps)\nActivity:\n\nModify API request (e.g., hometown weather)\nScaffolded practice (fill-in-the-blank)\nOptional take-home transformation/visualization"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "",
    "text": "Empowering statistics educators with real-world data skills.\nThis site houses all the materials for our hands-on workshop at USCOTS 2025.\nWe’re learning how to extract, clean, and use live data from the web and APIs—bringing authentic data science into your classroom."
  },
  {
    "objectID": "index.html#workshop-goals",
    "href": "index.html#workshop-goals",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Workshop Goals",
    "text": "Workshop Goals\n\nUnderstand why extracting dynamic data is essential in modern statistics education.\nLearn how to read HTML tables and use web APIs to collect real-world data.\nUse tidyverse tools to transform and clean this data for analysis.\nBuild confidence integrating unstructured data into your courses."
  },
  {
    "objectID": "index.html#who-this-workshop-is-for",
    "href": "index.html#who-this-workshop-is-for",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Who This Workshop Is For",
    "text": "Who This Workshop Is For\n\nStatistics educators and data science instructors\nComfortable with R and tidyverse\nCurious about web scraping, APIs, and teaching modern data practices"
  },
  {
    "objectID": "index.html#whats-inside",
    "href": "index.html#whats-inside",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "What’s Inside",
    "text": "What’s Inside\n\nSession 1 – Why and How We Extract Data\n\n️ Session 2 – Working with APIs (Weather Data)\n\nSession 3 – Scraping Sports Data from HTML\n\nSession 4 – Final Challenge and Reflection\n\nExplore these under the Sessions tab above."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Getting Started",
    "text": "Getting Started\n\nHead over to Session 1 - Introduction to begin your learning journey.\nOr, check out the Organized Outline for a roadmap of the workshop.\n\n\n##️ Tools We’ll Use\n\nrvest, httr, jsonlite, purrr, tidyverse\nJupyter via CourseKata CKHub\nRStudio / Quarto"
  },
  {
    "objectID": "index.html#quote-to-frame-the-work",
    "href": "index.html#quote-to-frame-the-work",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Quote to Frame the Work",
    "text": "Quote to Frame the Work\n\n“The world is one big dataset—if we learn how to extract it.”\n— Inspired by data educators like you"
  },
  {
    "objectID": "index.html#lets-get-started",
    "href": "index.html#lets-get-started",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Let’s Get Started!",
    "text": "Let’s Get Started!\nReady to transform how your students work with data?\n📎 Navigate using the menu above and join us in making data science real, relevant, and resilient."
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#deliverable",
    "href": "outlines_and_organization/organized-outline.html#deliverable",
    "title": "Organized Workshop Outline",
    "section": "Deliverable",
    "text": "Deliverable\n\nFinal output: R Quarto Document(s) (4–6 files aligned with each session and additional activities)"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#general-timeline",
    "href": "outlines_and_organization/organized-outline.html#general-timeline",
    "title": "Organized Workshop Outline",
    "section": "General Timeline",
    "text": "General Timeline\n\n8:30 – 10:15 Session 1: Introduction to Data Extraction\n10:15 – 10:45 Snack Break 1\n10:45 – 12:30 Session 2: APIs and Practice\n12:30 – 1:30 Lunch\n1:30 – 2:30 Session 3: Web Scraping with HTML\n2:30 – 3:30 Snack Break 2\n3:30 – 4:15 Session 4: Deep Dives into Complete Lessons"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#session-1-introduction-to-data-extraction",
    "href": "outlines_and_organization/organized-outline.html#session-1-introduction-to-data-extraction",
    "title": "Organized Workshop Outline",
    "section": "Session 1: Introduction to Data Extraction",
    "text": "Session 1: Introduction to Data Extraction\n\nSet expectations and workshop goals\nWhy data extraction matters: relevance to real-world education\nOverview of the layout / table of contents\nDiscuss libraries used (tidyverse, rvest, httr, etc.)\nBest practices (e.g., avoiding hardcoding, consistent comments)\nAdapting to changing APIs/websites\nAnecdote: Spotify example of lost API access\nExplain tidy data: snake_case column names, correct data types\nEmphasize code flexibility — developers can change APIs overnight\nActivity: Scaffolding + Code review using example(s)\n\n\nGoals & Objectives\n\nIdentify the value of real-world data in statistics education\nDescribe the distinction between extraction, transformation, and visualization (ETv)\nRecognize challenges associated with pulling live data from the web\nApply tidy data principles to imported datasets\n\n\n\nConceptual Foundation\n\nWhy use live data?\nWhat is extraction and why it matters for teaching modern statistics\nETv framework: introduction to the first stage (Extraction)\nImportance of code flexibility and the fragility of external sources (e.g., Spotify anecdote)\nTidy data principles: naming conventions, structure, and data types\n\n\n\nHands-On Coding Activity\n\nExtracting from accessible sources such as:\n\nA static .csv hosted online (warm-up)\nA Wikipedia table using rvest and janitor\n\nIntroduce read_csv() and rvest::html_table()\nAdd cleaning steps to enforce tidy principles (snake_case, correct types)\n\n\n\nReflection\n\nHow can you introduce real-world messiness without overwhelming students?\nHow would you scaffold tidy principles at the intro-level?"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#session-2-api-fundamentals",
    "href": "outlines_and_organization/organized-outline.html#session-2-api-fundamentals",
    "title": "Organized Workshop Outline",
    "section": "Session 2: API Fundamentals",
    "text": "Session 2: API Fundamentals\n\nWhat is an API? Examples (Spotify, Weather, one bonus example)\nBasic API structure: request URLs, endpoints, tokens\nHTTP protocols and status codes\nCRUD operations (Create, Read, Update, Delete)\nAPI best practices (e.g., pagination, authentication, caching)\nTidyverse-friendly workflows (avoid deep nesting, use readable steps)\nActivity:\n\nModify API request (e.g., hometown weather)\nScaffolded practice (fill-in-the-blank)\nOptional take-home transformation/visualization\n\n\n\nGoals & Objectives\n\nExplain what an API is and how it supports data extraction\nMake requests to a public API and interpret the JSON response\nUnderstand and apply HTTP status codes and API keys\nWrite clean, readable code to extract and parse API data\n\n\n\nConceptual Foundation\n\nRESTful APIs: endpoints, parameters, keys\nAuthentication: tokens, secrets, and environment variables\nStatus codes and error handling (focus on 200, 401, 403, 404)\nJSON structure: nested data and tidy conversion\n\n\n\nHands-On Coding Activity\n\nWeather API (e.g., OpenWeatherMap):\n\nRetrieve current weather for participant’s hometown\nModify query parameters (e.g., units, location)\nParse and visualize simple results (e.g., temperature, humidity)\n\nScaffold activity: prewritten functions + one blank section\n\n\n\nReflection\n\nWhere could API data naturally integrate in your curriculum?\nWhat are the pitfalls (rate limits, authentication) students need to know?"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#session-3-html-scraping-in-r",
    "href": "outlines_and_organization/organized-outline.html#session-3-html-scraping-in-r",
    "title": "Organized Workshop Outline",
    "section": "Session 3: HTML Scraping in R",
    "text": "Session 3: HTML Scraping in R\n\nWhat is HTML and why it’s useful?\nExamples: Wikipedia, sports sites (NFL, Olympics)\nStructured vs unstructured web data\nReading the webpage source and locating tables/divs\nPractice: Going back and forth between R and browser to inspect structure\nTidy HTML scraping practices using rvest and janitor\nDifferent approaches:\n\nFull walkthrough\nPartial scaffold\n\nActivity:\n\nScrape 2 sources (in pairs), compare\nClean the data: name 3 needed transformations\nUse visualization and interpretation\nDiscuss hardcoding and fragile selectors\n\n\n\nGoals & Objectives\n\nIdentify basic HTML structure relevant for scraping\nScrape tables and text from structured web pages\nClean scraped data using tidyverse tools\nCompare different websites in terms of data accessibility\n\n\n\nConceptual Foundation\n\nHTML basics: tags, attributes, structure of web tables\nUsing rvest to read web pages and extract data\nThe importance of inspecting elements with browser tools\nStructured vs. unstructured sites: Wikipedia vs. ESPN\n\n\n\nHands-On Coding Activity\n\nScrape sports statistics from a reliable table:\n\nExample: Wikipedia table of Olympic medal counts or NBA season stats\nClean using janitor::clean_names()\nCompare scraped data from 2 sites (optional pair task)\n\n\n\n\nReflection\n\nHow could students use scraped data in a final project?\nWhat scaffolds would help students inspect and trust their source?"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#session-4-deep-dives-into-lessons",
    "href": "outlines_and_organization/organized-outline.html#session-4-deep-dives-into-lessons",
    "title": "Organized Workshop Outline",
    "section": "Session 4: Deep Dives into Lessons",
    "text": "Session 4: Deep Dives into Lessons\n\nWork through 2 complete lessons — each with:\n\nAPI-based extraction and visualization\nHTML-based extraction and transformation\n\nSplit class into two groups: API vs HTML, then reconvene\nHighlight pedagogical framing: how this can be implemented in class\nBuild reflection and discussion time: What will you bring into your course?\n\n\nGoals & Objectives\n\nReview key takeaways from API and HTML extraction\nCollaborate with peers on a structured mini-project\nReflect on how to implement extraction in your own course\nShare classroom-ready ideas with other educators\n\n\n\nRecap (Conceptual Foundation)\n\nExtraction is not “just tech” — it’s pedagogy\nAPI vs. HTML: strengths, limitations, educational value\nDesigning learning activities around messy data: student engagement, real-world relevance\n\n\n\nHands-On Coding Activity\n\nParticipants are randomly assigned:\n\nGroup A: Use an API (weather, Spotify, etc.)\nGroup B: Scrape HTML data (sports, Wikipedia, etc.)\n\nWork in small groups to clean, transform, and visualize\nPrepare a brief “teaching demo” of how this could be used in class\n\n\n\nDiscussion & Reflection\n\nWhat worked in your group?\nWhat teaching goals does this type of project help support?\nHow would you modify it for your students’ level and context?"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#supporting-infrastructure",
    "href": "outlines_and_organization/organized-outline.html#supporting-infrastructure",
    "title": "Organized Workshop Outline",
    "section": "Supporting Infrastructure",
    "text": "Supporting Infrastructure\n\nUse recent versions of all packages\nAll materials hosted on:\n\nCourseKata for workshop delivery\nGitHub repo for behind-the-scenes development organization\n\nEach session includes:\n\nCode chunk scaffolds (empty/fill-in versions)\nSolution files\nPedagogical notes and discussion questions\n\nOutput files:\n\n4–6 R Quarto files\nMatching Jupyter notebooks for hands-on use\n\nBonus activities:\n\nCombine scraping + API for multi-source projects (e.g., sports + weather + sales)"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#workshop-strategy-logistics",
    "href": "outlines_and_organization/organized-outline.html#workshop-strategy-logistics",
    "title": "Organized Workshop Outline",
    "section": "Workshop Strategy & Logistics",
    "text": "Workshop Strategy & Logistics\n\nOutcomes After Workshop\n\nInvite participants to join a newsletter and mailing list\nPromote:\n\nMonthly subscription model ($25/month) for materials\nTeaching-focused seminars\nCulturally relevant virtual textbook\nConference workshops\n\n\n\n\nPlanning Tips for Success\n\nMake goals explicit from the start\nAlign each activity with stated goals\nInclude participant work time and discussions\nSchedule breaks for casual conversation and social connection\nKnow your audience (collect pre-attendance data)\nSend prep info (software setup, expectations) in advance\nAnticipate and plan for no-shows\n\n\n\nAdditional Notes\n\nProvide API tokens ahead of time (Spotify key for all)\nExplain API rate limits and possible costs\nTeach foundational status codes\nClarify complexity differences:\n\nHTML: Cleaning/structure focus\nAPI: Parsing/logic focus (JSON)\n\nRaffle and candy: build fun and engagement\nEncourage pair programming and peer instruction\nAllow participants to present their work at the end"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#rationale-for-combinations",
    "href": "outlines_and_organization/organized-outline.html#rationale-for-combinations",
    "title": "Organized Workshop Outline",
    "section": "Rationale for Combinations",
    "text": "Rationale for Combinations\n\nSession 1 & General Thoughts: Merged all teaching philosophy related to data extraction fundamentals here.\nAPI Examples & API Best Practices: Combined into Session 2 for cohesion and clarity.\nHTML Examples & Cleaning Strategies: Combined into Session 3 for a unified focus on web scraping.\nSession 4 & Misc Deep Dives: Naturally fit as a concluding session to synthesize all techniques and apply in pedagogically rich lessons.\nOutcomes, Planning, and Misc Strategy: Organized into coherent post-workshop and infrastructure support categories.\n\nLet me know if you want me to turn this into a .qmd, Google Doc, or a GitHub README.md."
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html",
    "href": "outlines_and_organization/unorganized-outline.html",
    "title": "Unorganized Thoughts About Workshop",
    "section": "",
    "text": "Deliverables: R Quarto Document\n\n\n\n8:30 - 10:15 Session 1\n10:15 - 10:45 Snack 1\n10:45 - 12:30 - Session 2\n12:30 -1:30 Lunch\n1:30 - 2:30 - Session 3\n2:30 - 3:30 - Snack 2\n3:30 - 4:15 - Session 4"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html#general-timeline",
    "href": "outlines_and_organization/unorganized-outline.html#general-timeline",
    "title": "Unorganized Thoughts About Workshop",
    "section": "",
    "text": "8:30 - 10:15 Session 1\n10:15 - 10:45 Snack 1\n10:45 - 12:30 - Session 2\n12:30 -1:30 Lunch\n1:30 - 2:30 - Session 3\n2:30 - 3:30 - Snack 2\n3:30 - 4:15 - Session 4"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html#part-1-outcomes-after-workshop",
    "href": "outlines_and_organization/unorganized-outline.html#part-1-outcomes-after-workshop",
    "title": "Unorganized Thoughts About Workshop",
    "section": "Part 1: Outcomes after workshop",
    "text": "Part 1: Outcomes after workshop\n\nJoin Newsletter and become subsribers\nPatrons puchchase product\n\n\nSubscription model where they pay $25 a month to have access information\nSeminars where I teach how to teach the material\nA textbook/virtual book\nSeminars at conferences"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html#part-2-how-to-have-great-workshopnotes-for-great-scots-presentation",
    "href": "outlines_and_organization/unorganized-outline.html#part-2-how-to-have-great-workshopnotes-for-great-scots-presentation",
    "title": "Unorganized Thoughts About Workshop",
    "section": "Part 2: How to have great workshopNotes for great Scots presentation",
    "text": "Part 2: How to have great workshopNotes for great Scots presentation\n\nMake clear goals in the beginning**\nMake activities relate to goals\nInclude time for them to do actual work\nSchedule in break time for talking about\nI will have a list of where they are coming from\nSend them message about prep\nSend them information about lelve of content to do so before hand via email\nPrepare for No Shows"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html#part-3-misc",
    "href": "outlines_and_organization/unorganized-outline.html#part-3-misc",
    "title": "Unorganized Thoughts About Workshop",
    "section": "Part 3: Misc",
    "text": "Part 3: Misc\n\nGet API Access Token, use univerisal API, to make sure process goes by fast\nCost of API, need to make sure they know it possible to get more info but it costs\nStatus codes, teach them about the extent of status codes that will leave them with a foundation\nThe guts of the HTML complexity really is within the code to make the data clean whereas the guts of the API complexity is how the JSON is brought into R that requires deeper thought and intention around\nwebsite/coursekata will house everything, this github will be for me to organize everything\nProcess of viewers to client should be obvious and easy done\nMaterials: 4 Separate quarto files –&gt; 4 separate jupyter notebook (potentiall 6, two additional notebooks for activity, split class to do API and others do html so they do not feel overwhelm, bring together and how them presnt their finding and how it can be imlplemnted in tjheir classes)\nHave questions, discussion questions, best practice within each quarto\nCombine strategies, i.e web scrape for ice cream sales, sports, other, and pull in weather API to join and look for patterns, Transformations and visualizations for each\nBring variety of candy\nHvae prizes for raffle\n\nHave this paradigm within each session - Goals & Objectives - Theory (I do not know what should be here for the introduction & conclusion) - Practice (what we do with API & HTML extraction) - (I do not know what to put here)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html",
    "href": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html",
    "title": "Session 3: Extraction of NFL Data - HTML Scraping",
    "section": "",
    "text": "Identify basic HTML structure relevant for scraping\nScrape tables and text from structured web pages\nClean scraped data using tidyverse tools\nCompare different websites in terms of data accessibility\n\n\n\n\n\nHTML basics: tags, attributes, structure of web tables\nUsing rvest to read web pages and extract data\nThe importance of inspecting elements with browser tools\nStructured vs. unstructured sites: Wikipedia vs. ESPN\n\n\n\n\nlibrary(rvest)      # Web scraping\nlibrary(dplyr)      # Data manipulation\nlibrary(stringr)    # String cleaning\nlibrary(rlang)      # Advanced evaluation\nlibrary(purrr)      # Functional tools\nlibrary(ggplot2)    # Visualizations\n\nDiscussion: Why are some of these packages (like purrr or rlang) useful for scraping tasks?\n\n\n\n\nWe start by creating the target URL for a given team and year.\n\n# Step 2: Define team and year\nteam_name &lt;- \"was\"\nyear &lt;- 2023\n\n# Step 2a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n\nDiscussion: How could we make this part of a function so it is reusable?\nSee Also: https://www.pro-football-reference.com/teams/was/2023.htm#games\n\n\n\n\n\n# Step 3: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n\nWhen you run read_html(url), it returns an HTML node pointer, not human-readable content.\nThis pointer references the structure of the web page in memory, but doesn’t display actual text or data.\nIf you try to print this object directly, you’ll see something such as:\nwebpage[[1]] &lt;pointer: 0x00000225...&gt;\n\n\n\nR is showing memory addresses of HTML elements, not the content.\nThis is because the HTML content must still be parsed or extracted.\n\nUse rvest!\n\nhtml_table() : extracts data from &lt;table&gt; elements.\nhtml_text() : extracts plain text from HTML nodes.\nhtml_nodes() or html_elements() : selects multiple nodes using CSS or XPath.\nhtml_element() : selects a single node.\n\nDiscussion: Why do you think web scraping tools separate “structure” from “content”? What are the pros and cons of working with HTML nodes directly?\nFrom the webpage, grab the HTML tables using rvest::html_table().\n\n# Step 3a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\nThe result is a list containing HTML table elements.\nDiscussion: What does this data structure look like?\nSelect the desired table, the 2023 Regular Season Table, which is the second table on the webpage. Use purrr::pluck() to select the table.\n\n\n\nThis is the table we are after\n\n\n\n# Step 3b: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n\nDiscussion: Why might this index (2) break in the future? What alternatives could we use to select the correct table more reliably?\n\n\n\n\nOur first row contains more information regarding the columns than the header of the actual table. The merged cells in the header end up being repeated over the entire column group they represent, without providing useful information.\n\n# Step 4a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\nDiscussion: Why can’t we use dplyr slice()?\n\n# Step 4b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n\n# Step 4c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n\n# Step 4d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\nBecause these columns are neither labeled in the first row or the header, we must manually assign them names.\n\n\n\nNotice the unlabeled columns?\n\n\n\n# Step 4e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n\nDiscussion: What are the risks or tradeoffs in hardcoding columns like result and game_location? How could this break?\n\n\n\n\nHere we will use dplyr select and filter to drop columns that are not relevant, as well as the Bye Week where the team does not play a game.\n\n# Step 5: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n\n# Step 5a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n\n# Step 5b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n\n# Step 5c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\nDiscussion: Why convert categorical variables like score_rslt or game_location to factors? What impact could that have on modeling or plotting?\n\n\n\n\nBy putting it all together, we can input a year for the Washington Commanders and get an extracted and cleaned table out.\n\n# Step 6: Year-only function\nwas_year &lt;- function(year) {\n  # Step 1: Define team and year\nteam_name &lt;- \"was\"\n\n# Step 1a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n  \n # Step 2: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n# Step 2a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\n# Step 3: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n # Step 3a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\n# Step 3b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n# Step 3c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n# Step 3d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\n# Step 3e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n# Step 4: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n# Step 4a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n# Step 4b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n# Step 4c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\n  return(table_7)\n}\n\nTest Year Only Function\n\nhead(was_year(2022))\n\n\n\n\nNow we will do the same task but while supplying team_name as a parameter as well as year.\n\n# Step 7: Generalized function\nfn_team_year &lt;- function(team_name, year) {\n\n# Step 2a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n  \n # Step 3: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n# Step 3a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\n# Step 3b: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n # Step 4a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\n# Step 4b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n# Step 4c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n# Step 4d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\n# Step 4e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n# Step 5: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n# Step 5a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n# Step 5b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n# Step 5c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\n  return(table_7)\n}\n\nTest Function (Team + Year)\n\nhead(fn_team_year(\"sfo\", 2024))\n\n\n\n\n\nUse ggplot2 to create simple and insightful visualizations.\n\n# Step 8: Line plot of points scored by Week\nggplot(fn_team_year(\"sfo\", 2024), aes(x = week, y = tm)) +\n  geom_line(color = \"steelblue\", linewidth = 1.2) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Points Scored Over Time\",\n    x = \"Week\",\n    y = \"Points Scored\"\n  ) +\n  theme_minimal()\n\n\n# Step 8a: Compare performance by game location\nggplot(fn_team_year(\"sfo\", 2024), aes(x = game_location, y = tm, fill = game_location)) +\n  geom_boxplot() +\n  labs(\n    title = \"Points Scored: Home vs Away\",\n    x = \"Location\",\n    y = \"Points Scored\"\n  ) +\n  theme_minimal()\n\nDiscussion: How might you visualize win/loss trends over the season? Could you include opponent information or passing yards?\nNow that you’re familiar with HTML elements and scraping, this activity will walk through extracting, cleaning, and visualizing NFL team performance data.\n\n\n\n\n\n\nScrape sports statistics from a reliable table:\n\nExample: Wikipedia table of Olympic medal counts or NBA season stats\nClean using janitor::clean_names()\nCompare scraped data from 2 sites (optional pair task)\n\n\n\n\n\n\nHow could students use scraped data in a final project?\nWhat scaffolds would help students inspect and trust their source?\n\n\n\n\n\nWhat is HTML and why it’s useful?\nExamples: Wikipedia, sports sites (NFL, Olympics)\nStructured vs unstructured web data\nReading the webpage source and locating tables/divs\nPractice: Going back and forth between R and browser to inspect structure\nTidy HTML scraping practices using rvest and janitor\nDifferent approaches:\n\nFull walkthrough\nPartial scaffold\n\nActivity:\n\nScrape 2 sources (in pairs), compare\nClean the data: name 3 needed transformations\nUse visualization and interpretation\nDiscuss hardcoding and fragile selectors"
  },
  {
    "objectID": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html#session-3-html-web-scraping---nfl-data-extraction",
    "href": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html#session-3-html-web-scraping---nfl-data-extraction",
    "title": "Session 3: Extraction of NFL Data - HTML Scraping",
    "section": "",
    "text": "Identify basic HTML structure relevant for scraping\nScrape tables and text from structured web pages\nClean scraped data using tidyverse tools\nCompare different websites in terms of data accessibility\n\n\n\n\n\nHTML basics: tags, attributes, structure of web tables\nUsing rvest to read web pages and extract data\nThe importance of inspecting elements with browser tools\nStructured vs. unstructured sites: Wikipedia vs. ESPN\n\n\n\n\nlibrary(rvest)      # Web scraping\nlibrary(dplyr)      # Data manipulation\nlibrary(stringr)    # String cleaning\nlibrary(rlang)      # Advanced evaluation\nlibrary(purrr)      # Functional tools\nlibrary(ggplot2)    # Visualizations\n\nDiscussion: Why are some of these packages (like purrr or rlang) useful for scraping tasks?\n\n\n\n\nWe start by creating the target URL for a given team and year.\n\n# Step 2: Define team and year\nteam_name &lt;- \"was\"\nyear &lt;- 2023\n\n# Step 2a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n\nDiscussion: How could we make this part of a function so it is reusable?\nSee Also: https://www.pro-football-reference.com/teams/was/2023.htm#games\n\n\n\n\n\n# Step 3: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n\nWhen you run read_html(url), it returns an HTML node pointer, not human-readable content.\nThis pointer references the structure of the web page in memory, but doesn’t display actual text or data.\nIf you try to print this object directly, you’ll see something such as:\nwebpage[[1]] &lt;pointer: 0x00000225...&gt;\n\n\n\nR is showing memory addresses of HTML elements, not the content.\nThis is because the HTML content must still be parsed or extracted.\n\nUse rvest!\n\nhtml_table() : extracts data from &lt;table&gt; elements.\nhtml_text() : extracts plain text from HTML nodes.\nhtml_nodes() or html_elements() : selects multiple nodes using CSS or XPath.\nhtml_element() : selects a single node.\n\nDiscussion: Why do you think web scraping tools separate “structure” from “content”? What are the pros and cons of working with HTML nodes directly?\nFrom the webpage, grab the HTML tables using rvest::html_table().\n\n# Step 3a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\nThe result is a list containing HTML table elements.\nDiscussion: What does this data structure look like?\nSelect the desired table, the 2023 Regular Season Table, which is the second table on the webpage. Use purrr::pluck() to select the table.\n\n\n\nThis is the table we are after\n\n\n\n# Step 3b: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n\nDiscussion: Why might this index (2) break in the future? What alternatives could we use to select the correct table more reliably?\n\n\n\n\nOur first row contains more information regarding the columns than the header of the actual table. The merged cells in the header end up being repeated over the entire column group they represent, without providing useful information.\n\n# Step 4a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\nDiscussion: Why can’t we use dplyr slice()?\n\n# Step 4b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n\n# Step 4c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n\n# Step 4d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\nBecause these columns are neither labeled in the first row or the header, we must manually assign them names.\n\n\n\nNotice the unlabeled columns?\n\n\n\n# Step 4e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n\nDiscussion: What are the risks or tradeoffs in hardcoding columns like result and game_location? How could this break?\n\n\n\n\nHere we will use dplyr select and filter to drop columns that are not relevant, as well as the Bye Week where the team does not play a game.\n\n# Step 5: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n\n# Step 5a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n\n# Step 5b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n\n# Step 5c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\nDiscussion: Why convert categorical variables like score_rslt or game_location to factors? What impact could that have on modeling or plotting?\n\n\n\n\nBy putting it all together, we can input a year for the Washington Commanders and get an extracted and cleaned table out.\n\n# Step 6: Year-only function\nwas_year &lt;- function(year) {\n  # Step 1: Define team and year\nteam_name &lt;- \"was\"\n\n# Step 1a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n  \n # Step 2: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n# Step 2a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\n# Step 3: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n # Step 3a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\n# Step 3b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n# Step 3c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n# Step 3d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\n# Step 3e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n# Step 4: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n# Step 4a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n# Step 4b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n# Step 4c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\n  return(table_7)\n}\n\nTest Year Only Function\n\nhead(was_year(2022))\n\n\n\n\nNow we will do the same task but while supplying team_name as a parameter as well as year.\n\n# Step 7: Generalized function\nfn_team_year &lt;- function(team_name, year) {\n\n# Step 2a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n  \n # Step 3: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n# Step 3a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\n# Step 3b: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n # Step 4a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\n# Step 4b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n# Step 4c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n# Step 4d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\n# Step 4e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n# Step 5: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n# Step 5a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n# Step 5b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n# Step 5c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\n  return(table_7)\n}\n\nTest Function (Team + Year)\n\nhead(fn_team_year(\"sfo\", 2024))\n\n\n\n\n\nUse ggplot2 to create simple and insightful visualizations.\n\n# Step 8: Line plot of points scored by Week\nggplot(fn_team_year(\"sfo\", 2024), aes(x = week, y = tm)) +\n  geom_line(color = \"steelblue\", linewidth = 1.2) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Points Scored Over Time\",\n    x = \"Week\",\n    y = \"Points Scored\"\n  ) +\n  theme_minimal()\n\n\n# Step 8a: Compare performance by game location\nggplot(fn_team_year(\"sfo\", 2024), aes(x = game_location, y = tm, fill = game_location)) +\n  geom_boxplot() +\n  labs(\n    title = \"Points Scored: Home vs Away\",\n    x = \"Location\",\n    y = \"Points Scored\"\n  ) +\n  theme_minimal()\n\nDiscussion: How might you visualize win/loss trends over the season? Could you include opponent information or passing yards?\nNow that you’re familiar with HTML elements and scraping, this activity will walk through extracting, cleaning, and visualizing NFL team performance data.\n\n\n\n\n\n\nScrape sports statistics from a reliable table:\n\nExample: Wikipedia table of Olympic medal counts or NBA season stats\nClean using janitor::clean_names()\nCompare scraped data from 2 sites (optional pair task)\n\n\n\n\n\n\nHow could students use scraped data in a final project?\nWhat scaffolds would help students inspect and trust their source?\n\n\n\n\n\nWhat is HTML and why it’s useful?\nExamples: Wikipedia, sports sites (NFL, Olympics)\nStructured vs unstructured web data\nReading the webpage source and locating tables/divs\nPractice: Going back and forth between R and browser to inspect structure\nTidy HTML scraping practices using rvest and janitor\nDifferent approaches:\n\nFull walkthrough\nPartial scaffold\n\nActivity:\n\nScrape 2 sources (in pairs), compare\nClean the data: name 3 needed transformations\nUse visualization and interpretation\nDiscuss hardcoding and fragile selectors"
  },
  {
    "objectID": "sessions/session_1/01_Extraction_Introduction.html",
    "href": "sessions/session_1/01_Extraction_Introduction.html",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "",
    "text": "CourseKata\norganization of workshop of lecture and questions throughout\n\n\n\n\n\nUnderstand the importance of extracting dynamic data (via HTML and APIs) in modern data analysis and teaching\nLearn how to access and work with APIs to retrieve structured, real-time data\nLearn how to bring HTML-based data (e.g., web tables) into R using scraping tools\nApply these skills through hands-on coding practice and discussion of classroom integration\n\n\n\n(need to place)\n\n\n\n\n\nP1. My mentor, Allan, says ask good questions…\nP2. Statistical Question: Who had the most impactful first season in terms of points: Michael Jordan, LeBron James, or Kobe Bryant?\nP3. I recently submitted a manuscript on this exact dataset, so let’s use it as our starting point.\nP4. We’ll begin by working with a static Excel file that contains per-game stats for each player’s 15 seasons in the NBA.\nP5. Let’s Load the pertinent libraries\nT1. ____ Fill in the code (chatgpt)\n\n\nlibrary(readxl)      ## Data Extraction      --- E\nlibrary(dplyr)       ## Data Transformation  --- T\nlibrary(ggplot2)     ## Data Visualization   --- V\n\n\nP6. Load in the data\nT2. ____ Fill in the code (chatgpt)\n\n\nnba_df &lt;- read_xlsx(\"nba_data.xlsx\", sheet = \"modern_nba_legends_08302019\")\n\n\nP7. Let’s view the data …\nT3. Use the glimpse function to view the data\n\n\nglimpse(nba_df)\n\nRows: 3,400\nColumns: 38\n$ Name          &lt;chr&gt; \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"M…\n$ Season        &lt;chr&gt; \"season_1\", \"season_1\", \"season_1\", \"season_1\", \"season_…\n$ Game_Location &lt;chr&gt; \"Home\", \"Away\", \"Home\", \"Away\", \"Away\", \"Away\", \"Away\", …\n$ Game_Outcome  &lt;chr&gt; \"W\", \"L\", \"W\", \"W\", \"L\", \"W\", \"W\", \"W\", \"W\", \"L\", \"L\", \"…\n$ Point_Margin  &lt;dbl&gt; 16, -2, 6, 5, -16, 4, 15, 2, 3, -20, -9, -17, -10, 19, -…\n$ Rk            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ G             &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ Date          &lt;dttm&gt; 1984-10-26, 1984-10-27, 1984-10-29, 1984-10-30, 1984-11…\n$ Age           &lt;chr&gt; \"21-252\", \"21-253\", \"21-255\", \"21-256\", \"21-258\", \"21-26…\n$ Tm            &lt;chr&gt; \"CHI\", \"CHI\", \"CHI\", \"CHI\", \"CHI\", \"CHI\", \"CHI\", \"CHI\", …\n$ Opp           &lt;chr&gt; \"WSB\", \"MIL\", \"MIL\", \"KCK\", \"DEN\", \"DET\", \"NYK\", \"IND\", …\n$ GS            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ MP            &lt;dbl&gt; 40, 34, 34, 36, 33, 27, 33, 42, 43, 33, 44, 39, 42, 30, …\n$ FG            &lt;dbl&gt; 5, 8, 13, 8, 7, 9, 15, 9, 18, 12, 4, 11, 11, 9, 10, 6, 9…\n$ FGA           &lt;dbl&gt; 16, 13, 24, 21, 15, 19, 22, 22, 27, 24, 17, 26, 22, 13, …\n$ FG_Percent    &lt;dbl&gt; 0.313, 0.615, 0.542, 0.381, 0.467, 0.474, 0.682, 0.409, …\n$ `3P`          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ `3PA`         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 3, 0, 0, 1, 0, 1, 0, 0,…\n$ `3P_Percent`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 1, 0, NA, 0, NA, NA, 0, …\n$ FT            &lt;dbl&gt; 6, 5, 11, 9, 3, 7, 3, 9, 8, 3, 8, 12, 13, 5, 10, 1, 3, 2…\n$ FTA           &lt;dbl&gt; 7, 5, 13, 9, 4, 9, 4, 12, 11, 3, 8, 16, 14, 6, 10, 1, 4,…\n$ FT_Percent    &lt;dbl&gt; 0.857, 1.000, 0.846, 1.000, 0.750, 0.778, 0.750, 0.750, …\n$ ORB           &lt;dbl&gt; 1, 3, 2, 2, 3, 1, 4, 2, 2, 0, 0, 2, 4, 0, 3, 0, 1, 2, 2,…\n$ DRB           &lt;dbl&gt; 5, 2, 2, 2, 2, 3, 4, 7, 8, 2, 5, 3, 9, 4, 3, 2, 2, 3, 0,…\n$ TRB           &lt;dbl&gt; 6, 5, 4, 4, 5, 4, 8, 9, 10, 2, 5, 5, 13, 4, 6, 2, 3, 5, …\n$ AST           &lt;dbl&gt; 7, 5, 5, 5, 5, 3, 5, 4, 4, 2, 7, 2, 2, 3, 8, 3, 2, 5, 3,…\n$ STL           &lt;dbl&gt; 2, 2, 6, 3, 1, 3, 3, 2, 3, 2, 5, 2, 2, 4, 3, 3, 2, 3, 1,…\n$ BLK           &lt;dbl&gt; 4, 1, 2, 1, 1, 1, 2, 5, 2, 1, 2, 1, 2, 1, 1, 2, 0, 0, 1,…\n$ TOV           &lt;dbl&gt; 5, 3, 3, 6, 2, 5, 5, 3, 4, 1, 4, 3, 6, 4, 4, 4, 2, 4, 4,…\n$ PF            &lt;dbl&gt; 2, 4, 4, 5, 4, 5, 2, 4, 4, 4, 5, 3, 3, 4, 4, 1, 5, 4, 3,…\n$ PTS           &lt;dbl&gt; 16, 21, 37, 25, 17, 25, 33, 27, 45, 27, 16, 34, 35, 23, …\n$ GmSc          &lt;dbl&gt; 12.5, 19.4, 32.9, 14.7, 13.2, 14.9, 29.3, 21.2, 37.5, 17…\n$ number_game   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ DD            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ TD            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Age_Years     &lt;dbl&gt; 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, …\n$ Age_Days      &lt;dbl&gt; 252, 253, 255, 256, 258, 264, 265, 267, 270, 272, 274, 2…\n$ Last_Name     &lt;chr&gt; \"Jordan\", \"Jordan\", \"Jordan\", \"Jordan\", \"Jordan\", \"Jorda…\n\n\n\nQ1. Look through the data, does it look clean? Discuss amongst Your peers. (chatgpt)\nAns Q1: It is clean the numeric variables are supposed to be numeric and the characrets variables are treated as char (chatgpt)\nP8. Now let’s clean the focus on the data frame that we are after\nT4. ____ Fill in the code (chatgpt)\n\n\nseason_1_df &lt;- nba_df %&gt;% \n  filter(Season == \"season_1\")\n\n\nP9. Now lets look at a plot of their points (chatgpt)\n\nT5. ____ Fill in the code (chatgpt)\n\nseason_1_df %&gt;% \n  ggplot(aes(x = Name, y = PTS)) +\n  geom_boxplot() +\n  theme_bw() \n\nWarning: Removed 14 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nNote 1: We could have spruced it up but here we just wanted to answer the question, if you have the urge please do so.\nQ2. What conclusion could be made (chatgpt)\n\nP10. Now what about, Magic Johnson or Wilt Chamberlain Maybe Luka Dončić or Ja Morant If I wanted to add this data I need to go to the originnal source not an excel sheet to do this (chatgpt)\nNote 2: - Shift students from being passive data users to active data seekers - Move beyond the idea of “waiting for clean data” to learning how to access, validate, and clean it themselves - Teach both the skill to extract and the capacity to teach extraction\nNote 3: - Why this matters: We as instructors should not just rely on pre-built packages or static datasets. The digital world changes constantly — websites, APIs, and file structures evolve.\n\nOur responsibility: Teach students (and ourselves) how to adapt and access real-world data sources. Equip learners with skills to extract, not just consume pre-extracted content.\nDespite the growing importance of live data, most introductory courses still rely heavily on static, pre-cleaned datasets. This limits students’ exposure to the realities of modern data work.\nKey idea: Flat files can still be dynamic depending on how they’re maintained — but we use the term “dynamic” here to emphasize external, real-time data access through APIs and web scraping.\nThe availability of dynamic, frequently updated data — especially via web APIs — has grown exponentially in recent years. This shift demands new strategies in how we teach data access.\n\n\n\n\nP11.\nExamples: CSV, Excel files\nTypically unchanging unless manually edited\nOften pre-loaded into classroom activities\nMay still require cleaning (e.g., column names, missing data)\nNote 4: Messy data is not always a bad thing\n\n\n\n\n\nP12.\nDefinition: Data sources that update over time or are externally controlled (i.e., you don’t own the source)\nP13.\nTwo primary types:\n\nApplication Programming Interface APIs – Designed to serve structured data upon request (e.g., player stats, weather)\nHypertext Markup Language HTML/Web Pages – Seen as dynamic when content changes (especially sports, news, etc.)\n\nHTML pages are primarily designed for human readability, while APIs are designed for structured machine access. Both offer pathways to dynamic data, each with different advantages and challenges.\nBridging the gap between classroom exercises and real-world data practice requires that students learn not just how to analyze data — but how to find it, extract it, and prepare it themselves.\nNote 5: HTML can be treated as static or dynamic depending on how frequently the page updates. For this workshop, we treat HTML as dynamic, especially for sports data.\n\n\n\n\nNote 6:\n\nThere are many kinds of APIs, but in this workshop, we’ll focus specifically on web APIs — tools designed to let us request and retrieve data from online sources.\nIn R, we’ll act like a piece of software making those requests, allowing us to access live data programmatically.\n\n\n\n\nFlow of Data via API (vrogue.co)\n\n\n\nP14.\nAPI stand for Application Programming Interfaces\nIt is a way for software to communicate with one another\nOne way it work is that it allow programs to request data directly from external servers in a structured format (most often JSON).\n\n{\n  \"player\": \"LeBron James\",\n  \"points\": 27.1,\n  \"team\": \"Lakers\"\n}\n\nThe keys are players, pointsand team\nThe values for the corresponding keys are LeBron James, 27.1, Lakers\n\nNote 7:\n\nThere are a lot of acronyms\nJSON - Java Script Object Notation - javascript is web developing software (chatgpt)\n\nQ3. What does JSON? Answer: Java script object notation\nNote 8:\n\nDescribe Image Flow of Data via API (vrogue.co)\nA user sends a request via the internet → the API talks to the server → the server queries the database → the API responds with data, often as JSON.\nP15.\nLearning to work with web APIs teaches students more than just how to extract data — it gives them the tools to:\n\nLocate relevant APIs (e.g., weather, sports, music)\nConstruct and test their own API requests\nInterpret JSON responses (including nested structures)\nTransform the results into tidy formats ready for analysis\n\n\nP16\n\nAPIs aren’t just technical tools, they’re increasingly the primary way to access and query data stored in external databases.\nIn today’s fast-changing digital environment, students must be equipped to retrieve and work with information from live, external sources, not just rely on pre-cleaned datasets.\nNote 9:\n\nThis is what pushes students from passive observers of data to active agents in its collection, structure, and use. It aligns closely with what real-world data science jobs require, especially when you’re no longer just analyzing data, but acquiring it.\n\n\n\nVolume of Data Created (Statista)\n\n\nThe use of APIs requires keys, which are unique and secret codes that are used to authorize your request and identify your user and billing information. Consequently, keeping these codes secret is imperative. To do so, store API keys in environment files which reside on your computer, and not coded into variables or available in plain text on your working files.\nshow images creating an app (provide my youtube video) - get id and secret (blurr it out) (make highlighe secret and ID on image )\n[\nid: (eman post in some way) secret: (eman post on screen)\nNote 10: Note all APIs website work the same, getting experience with 1-2, you start to develop a philosophy on how to approach them (chatgpt)\nCode 2\nStep 1: Load the Required Packages\n\nlibrary(spotifyr)\nlibrary(dplyr)\n\n\nStep 2: Set Up Credentials\nBefore using the Spotify API, you need to create an app on the and retrieve your Client ID and Client Secret. (I have a video here to do this on your own but on the screen, you can see the id and secret - (chatgpt))How to make app\n\n# Replace these with your actual credentials (do NOT share them publicly)\nclient_id &lt;- 'your_spotify_client_id_here'\nclient_secret &lt;- 'your_spotify_client_secret_here'\n\n\nStep 3: Get an Access Token\nThe token allows Spotify to recognize both who you are and what you’re requesting.\n\naccess_token &lt;- get_spotify_access_token(\n  client_id = client_id,\n  client_secret = client_secret\n)\n\n\nStep 4: Request Artist Data\nNow that you’re authenticated, you can request track-level data from Spotify’s API.\n\nbeatles_df &lt;- get_artist_audio_features('the beatles')\n\n\nStep 5: Explore the Dataset\nUse glimpse() to quickly examine the structure of the data.\n\nglimpse(beatles_df)\n\n\nStep 6: Try Another Artist\nTry retrieving audio features for a different artist.\n\ntaylor_df &lt;- get_artist_audio_features('taylor swift')\nglimpse(taylor_df)\n\nNote 11: How databases and access can change in that the developers may change access or change the way you access information so you have to be aware of this. (chatgpt)\nData types are usually sotred correctly as chars or numerics so noy much cleaning required (chatgpt)\n\n\n\n\n\n\n\nProcess of HTML Scraping (sstm2.com)\n\n\n\nP17\n\nWebsites are designed using Hypertext Markup Language (HTML) to display information, (chatgpt: say better)\nInformation stored as tables are ideal within a HTML page (chatgpt: say more)\nNote 12: - we do not want to just copy and paste a table into a csv format it then clearn it up again in R, ideally we want to have access to and bring it into R (chatgpt: say better)\n\nP18:\n\nBelow is an image of code for html table and the actual table that it would produce\n{width=“200”, height = “1000”}\n\nNote 13:\n\nHighlight the following concepts:\n\nbeginning and the end of table\nthe column name\neach Row\nHow it translate into a human readable table\n\nEmphasize that we’re only focusing on &lt;table&gt; tags for this workshop\n\nP19.\n\nNow lets see one of the libraries that allows us to scrape in R\n\nlibrary(htmltab)\n\nNote 14: Lets go to the url via webbroswer\n\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n\nP20.\nThis function requires 2 args url and table number we can guess at it and may work\n\niowa_state_counties_df &lt;- htmltab(url,1)\n\n\niowa_state_counties_df &lt;- htmltab(url,2)\n\nNote 15:\n\nUnless you know html and want to look at the source code or you what exactly a table looks like you will have to guess sometimes\nWe can get the warning to go away by …\nP21.\n\nThis is what I would call static because the counties are note changeing but if we wanted baseball data at which currently everyday new data is displayed it is ideal that we have a more robust method of fgetting this data rather using htmltabs\n\nP22.\n\nCheck out article: Web Scraping in the Statistics and Data Science Curriculum: Challenges and Opportunities - Mine Dogucu & Mine Çetinkaya-Rundel\n\nP23.\n\nMuch like APIs, lots of relevant and useful information is available directly on webpages, which are readable by humans rather than APIs which are designed for machine access.\nBy learning this skill, students are able to:\n\nLocate relevant sources (e.g., sports data from Pro Football Reference)\nUnderstand how websites deliver and organize content\nTransform and clean data for analysis and visualization\n\nOften times, HTML tables contain unexpected structures or data types (images, links, etc) and can present a challenge that develops not only data cleaning skills, but intention, planning, and adaptability when handling and analyzing difficult data.\n\n\n\n\n\nSession 1: Introduction\nThis session serves as a discussion of the principles and reasoning behind learning these concepts and how they can benefit the classroom.\nSession 2: Getting Weather Data via OpenWeather API\nIn this session, we dive into OpenWeather API and learn to use packages like httr2 to execute API calls. We will also discuss URLs, queries, data structures, and more.\nSession 3: Scraping NFL Sports Data\nIn this session, we will use Pro-Football Reference to learn how to extract and clean HTML table data for use in statistical analysis and visualizations.\nSession 4: Putting it All Together (Project)\nIn this project, we will use HTML scraping joined with the OpenWeather API to create our own cloropleth map of Iowa.\n\n\n\nChloropleth Map (geoapify.com)\n\n\n\n\n\n\n\nGoal: Engage participants in applying both API and HTML extraction methods.\nPart A: API\n\nUse a new function from the spotifyr package\nCreate a simple plot using the data\nAsk comprehension questions:\n\nHow does this differ from static file use?\nWhat’s confusing about working with API responses?\n\n\nPart B: HTML\n\nExtract state-specific data from a chosen Wikipedia page\nGuide participants through cleaning it (if time allows)\nAsk guiding questions:\n\nDid the table number match what you expected?\nWhat challenges did you face?\n(Example prompt written on the board: “Not every state’s table is the same.”)\n\n\n\n\n\n\nWhat did we learn?\nHow does this connect to the original Goals & Objectives of the session?\nHow do you see yourself using this in your classroom?\nWhat kinds of APIs or HTML sources would be most relevant for your students?\n\n\n\n\n\nSet expectations and workshop goals\nWhy data extraction matters: relevance to real-world education\nOverview of the layout / table of contents\nDiscuss libraries used (tidyverse, rvest, httr, etc.)\nBest practices (e.g., avoiding hardcoding, consistent comments)\nAdapting to changing APIs/websites\nAnecdote: Spotify example of lost API access\nExplain tidy data: snake_case column names, correct data types\nEmphasize code flexibility — developers can change APIs overnight\nActivity: Scaffolding + Code review using example(s)"
  },
  {
    "objectID": "sessions/session_1/01_Extraction_Introduction.html#session-1-introduction-to-data-extraction",
    "href": "sessions/session_1/01_Extraction_Introduction.html#session-1-introduction-to-data-extraction",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "",
    "text": "CourseKata\norganization of workshop of lecture and questions throughout\n\n\n\n\n\nUnderstand the importance of extracting dynamic data (via HTML and APIs) in modern data analysis and teaching\nLearn how to access and work with APIs to retrieve structured, real-time data\nLearn how to bring HTML-based data (e.g., web tables) into R using scraping tools\nApply these skills through hands-on coding practice and discussion of classroom integration\n\n\n\n(need to place)\n\n\n\n\n\nP1. My mentor, Allan, says ask good questions…\nP2. Statistical Question: Who had the most impactful first season in terms of points: Michael Jordan, LeBron James, or Kobe Bryant?\nP3. I recently submitted a manuscript on this exact dataset, so let’s use it as our starting point.\nP4. We’ll begin by working with a static Excel file that contains per-game stats for each player’s 15 seasons in the NBA.\nP5. Let’s Load the pertinent libraries\nT1. ____ Fill in the code (chatgpt)\n\n\nlibrary(readxl)      ## Data Extraction      --- E\nlibrary(dplyr)       ## Data Transformation  --- T\nlibrary(ggplot2)     ## Data Visualization   --- V\n\n\nP6. Load in the data\nT2. ____ Fill in the code (chatgpt)\n\n\nnba_df &lt;- read_xlsx(\"nba_data.xlsx\", sheet = \"modern_nba_legends_08302019\")\n\n\nP7. Let’s view the data …\nT3. Use the glimpse function to view the data\n\n\nglimpse(nba_df)\n\nRows: 3,400\nColumns: 38\n$ Name          &lt;chr&gt; \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"M…\n$ Season        &lt;chr&gt; \"season_1\", \"season_1\", \"season_1\", \"season_1\", \"season_…\n$ Game_Location &lt;chr&gt; \"Home\", \"Away\", \"Home\", \"Away\", \"Away\", \"Away\", \"Away\", …\n$ Game_Outcome  &lt;chr&gt; \"W\", \"L\", \"W\", \"W\", \"L\", \"W\", \"W\", \"W\", \"W\", \"L\", \"L\", \"…\n$ Point_Margin  &lt;dbl&gt; 16, -2, 6, 5, -16, 4, 15, 2, 3, -20, -9, -17, -10, 19, -…\n$ Rk            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ G             &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ Date          &lt;dttm&gt; 1984-10-26, 1984-10-27, 1984-10-29, 1984-10-30, 1984-11…\n$ Age           &lt;chr&gt; \"21-252\", \"21-253\", \"21-255\", \"21-256\", \"21-258\", \"21-26…\n$ Tm            &lt;chr&gt; \"CHI\", \"CHI\", \"CHI\", \"CHI\", \"CHI\", \"CHI\", \"CHI\", \"CHI\", …\n$ Opp           &lt;chr&gt; \"WSB\", \"MIL\", \"MIL\", \"KCK\", \"DEN\", \"DET\", \"NYK\", \"IND\", …\n$ GS            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ MP            &lt;dbl&gt; 40, 34, 34, 36, 33, 27, 33, 42, 43, 33, 44, 39, 42, 30, …\n$ FG            &lt;dbl&gt; 5, 8, 13, 8, 7, 9, 15, 9, 18, 12, 4, 11, 11, 9, 10, 6, 9…\n$ FGA           &lt;dbl&gt; 16, 13, 24, 21, 15, 19, 22, 22, 27, 24, 17, 26, 22, 13, …\n$ FG_Percent    &lt;dbl&gt; 0.313, 0.615, 0.542, 0.381, 0.467, 0.474, 0.682, 0.409, …\n$ `3P`          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ `3PA`         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 3, 0, 0, 1, 0, 1, 0, 0,…\n$ `3P_Percent`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 1, 0, NA, 0, NA, NA, 0, …\n$ FT            &lt;dbl&gt; 6, 5, 11, 9, 3, 7, 3, 9, 8, 3, 8, 12, 13, 5, 10, 1, 3, 2…\n$ FTA           &lt;dbl&gt; 7, 5, 13, 9, 4, 9, 4, 12, 11, 3, 8, 16, 14, 6, 10, 1, 4,…\n$ FT_Percent    &lt;dbl&gt; 0.857, 1.000, 0.846, 1.000, 0.750, 0.778, 0.750, 0.750, …\n$ ORB           &lt;dbl&gt; 1, 3, 2, 2, 3, 1, 4, 2, 2, 0, 0, 2, 4, 0, 3, 0, 1, 2, 2,…\n$ DRB           &lt;dbl&gt; 5, 2, 2, 2, 2, 3, 4, 7, 8, 2, 5, 3, 9, 4, 3, 2, 2, 3, 0,…\n$ TRB           &lt;dbl&gt; 6, 5, 4, 4, 5, 4, 8, 9, 10, 2, 5, 5, 13, 4, 6, 2, 3, 5, …\n$ AST           &lt;dbl&gt; 7, 5, 5, 5, 5, 3, 5, 4, 4, 2, 7, 2, 2, 3, 8, 3, 2, 5, 3,…\n$ STL           &lt;dbl&gt; 2, 2, 6, 3, 1, 3, 3, 2, 3, 2, 5, 2, 2, 4, 3, 3, 2, 3, 1,…\n$ BLK           &lt;dbl&gt; 4, 1, 2, 1, 1, 1, 2, 5, 2, 1, 2, 1, 2, 1, 1, 2, 0, 0, 1,…\n$ TOV           &lt;dbl&gt; 5, 3, 3, 6, 2, 5, 5, 3, 4, 1, 4, 3, 6, 4, 4, 4, 2, 4, 4,…\n$ PF            &lt;dbl&gt; 2, 4, 4, 5, 4, 5, 2, 4, 4, 4, 5, 3, 3, 4, 4, 1, 5, 4, 3,…\n$ PTS           &lt;dbl&gt; 16, 21, 37, 25, 17, 25, 33, 27, 45, 27, 16, 34, 35, 23, …\n$ GmSc          &lt;dbl&gt; 12.5, 19.4, 32.9, 14.7, 13.2, 14.9, 29.3, 21.2, 37.5, 17…\n$ number_game   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ DD            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ TD            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Age_Years     &lt;dbl&gt; 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, …\n$ Age_Days      &lt;dbl&gt; 252, 253, 255, 256, 258, 264, 265, 267, 270, 272, 274, 2…\n$ Last_Name     &lt;chr&gt; \"Jordan\", \"Jordan\", \"Jordan\", \"Jordan\", \"Jordan\", \"Jorda…\n\n\n\nQ1. Look through the data, does it look clean? Discuss amongst Your peers. (chatgpt)\nAns Q1: It is clean the numeric variables are supposed to be numeric and the characrets variables are treated as char (chatgpt)\nP8. Now let’s clean the focus on the data frame that we are after\nT4. ____ Fill in the code (chatgpt)\n\n\nseason_1_df &lt;- nba_df %&gt;% \n  filter(Season == \"season_1\")\n\n\nP9. Now lets look at a plot of their points (chatgpt)\n\nT5. ____ Fill in the code (chatgpt)\n\nseason_1_df %&gt;% \n  ggplot(aes(x = Name, y = PTS)) +\n  geom_boxplot() +\n  theme_bw() \n\nWarning: Removed 14 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nNote 1: We could have spruced it up but here we just wanted to answer the question, if you have the urge please do so.\nQ2. What conclusion could be made (chatgpt)\n\nP10. Now what about, Magic Johnson or Wilt Chamberlain Maybe Luka Dončić or Ja Morant If I wanted to add this data I need to go to the originnal source not an excel sheet to do this (chatgpt)\nNote 2: - Shift students from being passive data users to active data seekers - Move beyond the idea of “waiting for clean data” to learning how to access, validate, and clean it themselves - Teach both the skill to extract and the capacity to teach extraction\nNote 3: - Why this matters: We as instructors should not just rely on pre-built packages or static datasets. The digital world changes constantly — websites, APIs, and file structures evolve.\n\nOur responsibility: Teach students (and ourselves) how to adapt and access real-world data sources. Equip learners with skills to extract, not just consume pre-extracted content.\nDespite the growing importance of live data, most introductory courses still rely heavily on static, pre-cleaned datasets. This limits students’ exposure to the realities of modern data work.\nKey idea: Flat files can still be dynamic depending on how they’re maintained — but we use the term “dynamic” here to emphasize external, real-time data access through APIs and web scraping.\nThe availability of dynamic, frequently updated data — especially via web APIs — has grown exponentially in recent years. This shift demands new strategies in how we teach data access.\n\n\n\n\nP11.\nExamples: CSV, Excel files\nTypically unchanging unless manually edited\nOften pre-loaded into classroom activities\nMay still require cleaning (e.g., column names, missing data)\nNote 4: Messy data is not always a bad thing\n\n\n\n\n\nP12.\nDefinition: Data sources that update over time or are externally controlled (i.e., you don’t own the source)\nP13.\nTwo primary types:\n\nApplication Programming Interface APIs – Designed to serve structured data upon request (e.g., player stats, weather)\nHypertext Markup Language HTML/Web Pages – Seen as dynamic when content changes (especially sports, news, etc.)\n\nHTML pages are primarily designed for human readability, while APIs are designed for structured machine access. Both offer pathways to dynamic data, each with different advantages and challenges.\nBridging the gap between classroom exercises and real-world data practice requires that students learn not just how to analyze data — but how to find it, extract it, and prepare it themselves.\nNote 5: HTML can be treated as static or dynamic depending on how frequently the page updates. For this workshop, we treat HTML as dynamic, especially for sports data.\n\n\n\n\nNote 6:\n\nThere are many kinds of APIs, but in this workshop, we’ll focus specifically on web APIs — tools designed to let us request and retrieve data from online sources.\nIn R, we’ll act like a piece of software making those requests, allowing us to access live data programmatically.\n\n\n\n\nFlow of Data via API (vrogue.co)\n\n\n\nP14.\nAPI stand for Application Programming Interfaces\nIt is a way for software to communicate with one another\nOne way it work is that it allow programs to request data directly from external servers in a structured format (most often JSON).\n\n{\n  \"player\": \"LeBron James\",\n  \"points\": 27.1,\n  \"team\": \"Lakers\"\n}\n\nThe keys are players, pointsand team\nThe values for the corresponding keys are LeBron James, 27.1, Lakers\n\nNote 7:\n\nThere are a lot of acronyms\nJSON - Java Script Object Notation - javascript is web developing software (chatgpt)\n\nQ3. What does JSON? Answer: Java script object notation\nNote 8:\n\nDescribe Image Flow of Data via API (vrogue.co)\nA user sends a request via the internet → the API talks to the server → the server queries the database → the API responds with data, often as JSON.\nP15.\nLearning to work with web APIs teaches students more than just how to extract data — it gives them the tools to:\n\nLocate relevant APIs (e.g., weather, sports, music)\nConstruct and test their own API requests\nInterpret JSON responses (including nested structures)\nTransform the results into tidy formats ready for analysis\n\n\nP16\n\nAPIs aren’t just technical tools, they’re increasingly the primary way to access and query data stored in external databases.\nIn today’s fast-changing digital environment, students must be equipped to retrieve and work with information from live, external sources, not just rely on pre-cleaned datasets.\nNote 9:\n\nThis is what pushes students from passive observers of data to active agents in its collection, structure, and use. It aligns closely with what real-world data science jobs require, especially when you’re no longer just analyzing data, but acquiring it.\n\n\n\nVolume of Data Created (Statista)\n\n\nThe use of APIs requires keys, which are unique and secret codes that are used to authorize your request and identify your user and billing information. Consequently, keeping these codes secret is imperative. To do so, store API keys in environment files which reside on your computer, and not coded into variables or available in plain text on your working files.\nshow images creating an app (provide my youtube video) - get id and secret (blurr it out) (make highlighe secret and ID on image )\n[\nid: (eman post in some way) secret: (eman post on screen)\nNote 10: Note all APIs website work the same, getting experience with 1-2, you start to develop a philosophy on how to approach them (chatgpt)\nCode 2\nStep 1: Load the Required Packages\n\nlibrary(spotifyr)\nlibrary(dplyr)\n\n\nStep 2: Set Up Credentials\nBefore using the Spotify API, you need to create an app on the and retrieve your Client ID and Client Secret. (I have a video here to do this on your own but on the screen, you can see the id and secret - (chatgpt))How to make app\n\n# Replace these with your actual credentials (do NOT share them publicly)\nclient_id &lt;- 'your_spotify_client_id_here'\nclient_secret &lt;- 'your_spotify_client_secret_here'\n\n\nStep 3: Get an Access Token\nThe token allows Spotify to recognize both who you are and what you’re requesting.\n\naccess_token &lt;- get_spotify_access_token(\n  client_id = client_id,\n  client_secret = client_secret\n)\n\n\nStep 4: Request Artist Data\nNow that you’re authenticated, you can request track-level data from Spotify’s API.\n\nbeatles_df &lt;- get_artist_audio_features('the beatles')\n\n\nStep 5: Explore the Dataset\nUse glimpse() to quickly examine the structure of the data.\n\nglimpse(beatles_df)\n\n\nStep 6: Try Another Artist\nTry retrieving audio features for a different artist.\n\ntaylor_df &lt;- get_artist_audio_features('taylor swift')\nglimpse(taylor_df)\n\nNote 11: How databases and access can change in that the developers may change access or change the way you access information so you have to be aware of this. (chatgpt)\nData types are usually sotred correctly as chars or numerics so noy much cleaning required (chatgpt)\n\n\n\n\n\n\n\nProcess of HTML Scraping (sstm2.com)\n\n\n\nP17\n\nWebsites are designed using Hypertext Markup Language (HTML) to display information, (chatgpt: say better)\nInformation stored as tables are ideal within a HTML page (chatgpt: say more)\nNote 12: - we do not want to just copy and paste a table into a csv format it then clearn it up again in R, ideally we want to have access to and bring it into R (chatgpt: say better)\n\nP18:\n\nBelow is an image of code for html table and the actual table that it would produce\n{width=“200”, height = “1000”}\n\nNote 13:\n\nHighlight the following concepts:\n\nbeginning and the end of table\nthe column name\neach Row\nHow it translate into a human readable table\n\nEmphasize that we’re only focusing on &lt;table&gt; tags for this workshop\n\nP19.\n\nNow lets see one of the libraries that allows us to scrape in R\n\nlibrary(htmltab)\n\nNote 14: Lets go to the url via webbroswer\n\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n\nP20.\nThis function requires 2 args url and table number we can guess at it and may work\n\niowa_state_counties_df &lt;- htmltab(url,1)\n\n\niowa_state_counties_df &lt;- htmltab(url,2)\n\nNote 15:\n\nUnless you know html and want to look at the source code or you what exactly a table looks like you will have to guess sometimes\nWe can get the warning to go away by …\nP21.\n\nThis is what I would call static because the counties are note changeing but if we wanted baseball data at which currently everyday new data is displayed it is ideal that we have a more robust method of fgetting this data rather using htmltabs\n\nP22.\n\nCheck out article: Web Scraping in the Statistics and Data Science Curriculum: Challenges and Opportunities - Mine Dogucu & Mine Çetinkaya-Rundel\n\nP23.\n\nMuch like APIs, lots of relevant and useful information is available directly on webpages, which are readable by humans rather than APIs which are designed for machine access.\nBy learning this skill, students are able to:\n\nLocate relevant sources (e.g., sports data from Pro Football Reference)\nUnderstand how websites deliver and organize content\nTransform and clean data for analysis and visualization\n\nOften times, HTML tables contain unexpected structures or data types (images, links, etc) and can present a challenge that develops not only data cleaning skills, but intention, planning, and adaptability when handling and analyzing difficult data.\n\n\n\n\n\nSession 1: Introduction\nThis session serves as a discussion of the principles and reasoning behind learning these concepts and how they can benefit the classroom.\nSession 2: Getting Weather Data via OpenWeather API\nIn this session, we dive into OpenWeather API and learn to use packages like httr2 to execute API calls. We will also discuss URLs, queries, data structures, and more.\nSession 3: Scraping NFL Sports Data\nIn this session, we will use Pro-Football Reference to learn how to extract and clean HTML table data for use in statistical analysis and visualizations.\nSession 4: Putting it All Together (Project)\nIn this project, we will use HTML scraping joined with the OpenWeather API to create our own cloropleth map of Iowa.\n\n\n\nChloropleth Map (geoapify.com)\n\n\n\n\n\n\n\nGoal: Engage participants in applying both API and HTML extraction methods.\nPart A: API\n\nUse a new function from the spotifyr package\nCreate a simple plot using the data\nAsk comprehension questions:\n\nHow does this differ from static file use?\nWhat’s confusing about working with API responses?\n\n\nPart B: HTML\n\nExtract state-specific data from a chosen Wikipedia page\nGuide participants through cleaning it (if time allows)\nAsk guiding questions:\n\nDid the table number match what you expected?\nWhat challenges did you face?\n(Example prompt written on the board: “Not every state’s table is the same.”)\n\n\n\n\n\n\nWhat did we learn?\nHow does this connect to the original Goals & Objectives of the session?\nHow do you see yourself using this in your classroom?\nWhat kinds of APIs or HTML sources would be most relevant for your students?\n\n\n\n\n\nSet expectations and workshop goals\nWhy data extraction matters: relevance to real-world education\nOverview of the layout / table of contents\nDiscuss libraries used (tidyverse, rvest, httr, etc.)\nBest practices (e.g., avoiding hardcoding, consistent comments)\nAdapting to changing APIs/websites\nAnecdote: Spotify example of lost API access\nExplain tidy data: snake_case column names, correct data types\nEmphasize code flexibility — developers can change APIs overnight\nActivity: Scaffolding + Code review using example(s)"
  }
]