---
title: "Introduction To Extraction Workshop - HTML & APIs"
author: "Ciera Millard"
format: html
---

# Session 1: Introduction

# HTML and APIs in the Classroom

The availability of continuously changing data and the use of web APIs have grown exponentially in recent years. Dynamic data is published online in formats designed for both human readability (HTML) and machine access (APIs). However, this not always reflected in introductory statistics or data science classes. Many times, instruction relies on pre-cleaned, static datasets, which are not realistic to the current data landscape.

To bridge this gap, it is essential that we prepare students to meet real-world data challenges by equipping them with the skills to independently extract, process, and analyze data. While it is often sufficient to provide static datasets for demonstrations and exercises, careers in modern data science will demand the ability to find and retrieve data from live sources, not simply rely on whatâ€™s been prepackaged.

## APIs

![Flow of Data via API (vrogue.co)](images/clipboard-1951848651.jpeg)

APIs (Application Programming Interfaces) allow programs to request data directly from external servers in a structured format (most often JSON). Learning to work with web APIs teaches students not just to observe data, but to understand how to:

-   Locate relevant APIs (e.g., weather data from OpenWeatherMap)

-   Construct their own API requests (instead of relying on pre-built packages)

-   Interpret the response structure (e.g., nested JSON)

-   Transform the returned data into usable formats for analysis

This pushes students to explore data fully, recognize potential limitations, biases, or technical issues in the data structure, and move beyond canned examples.

Furthermore, as APIs often serve as the channel which to access and retrieve information from databases. In an evolving landscape where more and more data is created and captured with each passing day, being able to leverage the information stored in databases is more critical than ever.

[![Volume of Data Created (Statista)](images/clipboard-272118191.png)](https://www.statista.com/statistics/871513/worldwide-data-created/)

The use of APIs requires keys, which are unique and secret codes that are used to authorize your request and identify your user and billing information. Consequently, keeping these codes secret is imperative. To do so, store API keys in environment files which reside on your computer, and not coded into variables or available in plain text on your working files.

------------------------------------------------------------------------

## HTML Web Scraping

Much like APIs, lots of relevant and useful information is available directly on webpages, which are readable by humans rather than APIs which are designed for machine access. By learning this skill, students are able to:

-   Locate relevant sources (e.g., sports data from Pro Football Reference)

-   Understand how websites deliver and organize content

-   Transform and clean data for analysis and visualization

------------------------------------------------------------------------

# Our Focus: Extraction

While analyzing data is comprised of many steps and processes, one of the first and most foundational of these is *extracting the data*. Here are some common ways data can be extracted.

-   CSV (comma-seperated values) files

-   Excel files (.xlsx, .xlsb, .xlsm)

-   Pre-loaded data frames in R (mtcars)

-   URLs to GitHub repositories

-   **HTML (Web Extraction/Scraping)**

-   **API requests (Databases)**

# Workshop Agenda

## Session 1: Introduction

This session serves as a discussion of the principles and reasoning behind learning these concepts and how they can benefit the classroom.

## Session 2: Getting Weather Data via OpenWeather API

In this session, we dive into OpenWeather API and learn to use packages like httr2 to execute API calls. We will also discuss URLs, queries, data structures, and more.

## Session 3: Scraping NFL Sports Data

In this session, we will use Pro-Football Reference to learn how to extract and clean HTML table data for use in statistical analysis and visualizations.

## Session 4: Putting it All Together (Project)

In this project, we will use HTML scraping joined with the OpenWeather API to create our own cloropleth map of Iowa.

------------------------------------------------------------------------

# Introduction to Libraries

```{r}
#| output: false
#| eval: false
library(rvest)      # Web scraping
library(dplyr)      # Data manipulation
library(stringr)    # String cleaning
library(rlang)      # Advanced evaluation
library(purrr)      # Functional tools
library(ggplot2)    # Visualizations
library(httr2)      # Makes web requests
library(tibble)     # Easier and prettier data frames
library(lubridate)  # Handles dates
library(dotenv)     # Loads environment variables from .Renviron
library(glue)       # Easier string concatenation
library(tigris)     # U.S. shapefiles for mapping
```

Below is a list of R packages used in this project, along with descriptions of what each package is used for.

------------------------------------------------------------------------

### `rvest`

-   Used for web scraping: parses HTML/XML documents into a navigable format.
-   Extracts structured data using CSS selectors, XPath, or tag-based search (`html_table()`, `html_text()`, etc.).

------------------------------------------------------------------------

### `dplyr`

-   Core package for tidyverse-style data manipulation (`filter()`, `mutate()`, `select()`, etc.).
-   Supports chaining operations with `%>%` / \|`>`, making data workflows readable and efficient.

------------------------------------------------------------------------

### `stringr`

-   Provides a consistent set of functions for string manipulation.
-   Handles pattern matching, replacement, splitting, and formatting.

------------------------------------------------------------------------

### `rlang`

-   Supports advanced evaluation and programming with tidyverse tools.
-   Useful when writing custom functions that dynamically reference or modify variables.

------------------------------------------------------------------------

### `purrr`

-   Enables functional programming with mapping tools like `map()`, `map_df()`, `walk()`, etc.
-   Replaces (many) for-loops and supports clean iteration over lists and vectors.

------------------------------------------------------------------------

### `ggplot2`

-   Graphics package for building layered, flexible visualizations.
-   Supports various plot types, themes, scales, and faceting for data storytelling.

------------------------------------------------------------------------

### `httr2`

-   Modern HTTP client designed for tidyverse-like API interaction.
-   Replaces `httr` with a more intuitive and pipeable interface (`request() |> req_perform()`).

------------------------------------------------------------------------

### `tibble`

-   A modern rethinking of the `data.frame` that prints cleaner and behaves more predictably.
-   Default in tidyverse workflows; avoids surprises like string-to-factor conversion.

------------------------------------------------------------------------

### `lubridate`

-   Simplifies working with dates and times: parsing, formatting, and arithmetic.
-   Makes it easy to extract components (e.g., month, weekday) and perform date math.

------------------------------------------------------------------------

### `dotenv`

-   Loads environment variables from a `.env` or `.Renviron` file into R.
-   Keeps sensitive data like API keys out of your scripts and version control.

------------------------------------------------------------------------

### `glue`

-   Provides string interpolation (e.g., `glue("Hello {name}")`).
-   Cleaner and safer than `paste()` for building URLs, messages, or SQL queries.

------------------------------------------------------------------------

### `tigris`

-   Downloads shapefiles and geographic boundary data (e.g., counties, states) from the U.S. Census Bureau.
-   Returns spatial `sf` data frames, making it easy to map and visualize geographic data.
