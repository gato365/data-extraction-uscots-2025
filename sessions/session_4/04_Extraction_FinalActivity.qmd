---
title: "Session 4: Final Activity"
author: "Immanuel Williams & Ciera Millard"
date: "`r Sys.Date()`"
format: html
---

<!-- Delete this in Python Note Books -->

[Download starter .qmd file](04_Extraction_FinalActivity.qmd)

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE,echo = TRUE)
```

# Session 4: Final Project Activity

In this capstone session, we will combine our API and web scraping skills to complete two mini-projects. 

Each project will involve writing a custom function to extract data and then using that function to create a unique data visualization.


## Part 1. Goals & Objectives


1.  **Synthesize API and Scraping Skills:** Combine techniques learned throughout the workshop by seamlessly integrating data from both a web API (OpenWeather) and a scraped HTML table (Box Office Mojo) to complete a data project.
2.  **Develop Reusable Data Extraction Functions:** Write modular, single-purpose R functions that encapsulate the logic for calling an API or scraping a webpage, making code more readable, reusable, and easier to debug.


## Part 2: Conceptual Demonstration through projects

### **Project 1: Mapping U.S. Cities with the Geocoding API**

Our first goal is to take a list of U.S. cities, find their exact geographic coordinates using an API, and plot them on a map. For this, we'll use a new OpenWeatherMap endpoint: the **Geocoding API (v1.0)**.

#### **Step 1: Load Libraries**

We'll begin by loading all the libraries we need for API calls, data wrangling, and visualization.

**Filled Version (Answer)**

```{r}
#| eval: false
library(httr2)      # For making API requests
library(rvest)      # For web scraping
library(purrr)      # For functional programming like map()
library(glue)       # For building strings (URLs)
library(dplyr)      # For data manipulation
library(ggplot2)    # For plotting
```

**Fill-in-the-Blank Version**

```{r}
#| eval: false
# library(____)       # For making API requests
# library(____)       # For web scraping
# library(____)       # For functional programming like map()
# library(____)       # For building strings (URLs)
# library(____)       # For data manipulation
# library(____)       # For plotting
```

**Scratch Version**

```{r}
#| eval: false
# TODO: Load all necessary libraries for this project
# # For making API requests
# # For web scraping
# # For functional programming like map()
# # For building strings (URLs)
# # For data manipulation
# # For plotting
```

------------------------------------------------------------------------

#### **Step 2: Create a Geocoding Function**

Next, we'll write a function named `get_city_coords`. This function will take a city name as input, send a request to the OpenWeather Geocoding API, and return a clean data frame with the city's name, latitude, and longitude.

**Filled Version (Answer)**

```{r}
#| eval: false
get_city_coords <- function(city_name) {
  # Construct the request URL using the Geocoding v1.0 endpoint
  req_url <- glue("http://api.openweathermap.org/geo/1.0/direct?q={URLencode(city_name)}&limit=1&appid={Sys.getenv('OPENWEATHER_API_KEY')}")
  
  # Perform the request
  response <- request(req_url) %>% 
    req_perform()
  
  # Check for success and process the JSON response
  if (resp_status(response) == 200) {
    resp_body_json(response) %>% 
      pluck(1) %>% # Pluck the first result from the list
      as_tibble() %>% 
      select(name, lat, lon)
  } else {
    warning(paste("Could not find coordinates for:", city_name))
    return(NULL)
  }
}
```

**Fill-in-the-Blank Version**

```{r}
#| eval: false
# get_city_coords <- function(city_name) {
#   # Construct the request URL
#   req_url <- glue("http://api.openweathermap.org/geo/1.0/direct?q={____(city_name)}&limit=1&appid={Sys.getenv('OPENWEATHER_API_KEY')}")
#   
#   # Perform the request
#   response <- request(____) %>% 
#     req_perform()
#   
#   # Check for success (status code 200)
#   if (resp_status(____) == ____) {
#     resp_body_json(____) %>% 
#       pluck(____) %>% # Pluck the first result
#       as_tibble() %>% 
#       select(name, lat, lon)
#   } else {
#     warning(paste("Could not find coordinates for:", city_name))
#     return(NULL)
#   }
# }
```

**Scratch Version**

```{r}
#| eval: false
# TODO: Write a function named get_city_coords that takes a city_name,
# calls the Geocoding API, and returns a tibble with name, lat, and lon.
```

------------------------------------------------------------------------

#### **Step 3: Create a Plot of City Locations**

##### **Substep 3a: Get U.S. Map Data**

To create our map background, we use the `map_data()` function. This function retrieves a data frame containing the geographic polygon data—essentially a set of longitude and latitude coordinates—needed to draw the outlines of all U.S. states.

Code snippet

```{r}
#| eval: false
# Get U.S. map data
us_map <- map_data("state")
```

**How This Works & Which Library**

-   **Library**: The `map_data()` function comes from the **`ggplot2`** package. It is designed specifically to get map data in a format that works seamlessly with `ggplot()`.

-   **Process**:

    1.  When you call `map_data("state")`, `ggplot2` accesses a built-in dataset (from the **`maps`** package) that contains the outlines of the world, countries, and U.S. states.

    2.  It returns a data frame where each row represents a single point (vertex) on the border of a state.

    3.  Crucially, it includes a `group` column. This column tells `ggplot2` which points should be connected to form a single, continuous shape (a state polygon), preventing lines from being drawn incorrectly across the map.

Now, we'll define a list of cities and use our new function with `purrr::map_dfr` to get the coordinates for all of them. Finally, we'll use `ggplot2` to plot these locations on a map of the United States.

**Filled Version (Answer)**

```{r}
#| eval: false
# A vector of cities to map
cities_to_map <- c("San Luis Obispo", "Chicago", "New York", "Atlanta", "Houston", "Des Moines")

# Apply our function to each city and combine results into one data frame
city_locations_df <- map_dfr(cities_to_map, get_city_coords)



# Create the plot
us_map %>% 
  ggplot() +
  geom_polygon(aes(x = long, y = lat, group = group), fill = "gray90", color = "white") +
  geom_point(data = city_locations_df, aes(x = lon, y = lat), color = "red", size = 3) +
  labs(
    title = "Locations of Selected U.S. Cities",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal()
```

**Fill-in-the-Blank Version**

```{r}
#| eval: false
# A vector of cities to map
cities_to_map <- c("San Luis Obispo", "Chicago", "New York", "Atlanta", "Houston", "Des Moines")

# Apply our function to each city using map_dfr
city_locations_df <- map_dfr(____, ____)

# Get U.S. map data
us_map <- map_data("____")

# Create the plot
ggplot() +
  geom_polygon(data = ____, aes(x = long, y = lat, group = group), fill = "gray90", color = "white") +
  geom_point(data = ____, aes(x = lon, y = lat), color = "red", size = 3) +
  labs(
    title = "Locations of Selected U.S. Cities",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal()
```

**Scratch Version**

```{r}
#| eval: false
# TODO: Create a vector of city names.
# TODO: Use map_dfr() and your get_city_coords function to get their locations.
# TODO: Create a ggplot map of the US and add points for each city.
```

------------------------------------------------------------------------


::: {style="height: 300px; background-color: white;"}
:::

Of course. Here is the project completely revised to focus on the Box Office Mojo data, following the same structured, multi-version format.

-----

### **Project 2: Scraping and Plotting Box Office Data**

For this project, our goal is to scrape a list of the top-grossing films of all time directly from the **Box Office Mojo** website and create a visualization of the highest earners.

-----

#### **Step 1: Scrape Top-Grossing Movies**

First, we will scrape the Box Office Mojo website to get a clean table of the top lifetime grossing films.

**Filled Version (Answer)**

```{r}
#| eval: false
library(rvest)
library(janitor)
library(tibble)
library(purrr)

# Define the URL for the Box Office Mojo chart
mojo_url <- "https://www.boxofficemojo.com/chart/top_lifetime_gross/"

# Read the HTML, find the table, and clean it up
top_movies_df <- read_html(mojo_url) %>% 
  html_table() %>% 
  pluck(1) %>% 
  janitor::clean_names() %>% 
  as_tibble()

print(top_movies_df)
```

**Fill-in-the-Blank Version**

```{r}
#| eval: false
## Define the URL for the Box Office Mojo chart
# mojo_url <- "https://www.boxofficemojo.com/chart/top_lifetime_gross/"
# 
# # Read the HTML, find the table, and clean it up
# top_movies_df <- read_html(____) %>% 
#   html_table() %>% 
#   pluck(____) %>% # Which table on the page is it?
#   janitor::clean_names() %>% 
#   as_tibble()
# 
# print(top_movies_df)
```

**Scratch Version**

```{r}
#| eval: false
## TODO: Scrape the table of top-grossing films from the mojo_url.
## TODO: Clean the data using janitor::clean_names() and convert it to a tibble.
```

-----

#### **Step 2: Create a Function to Plot Top Movies**

The data we scraped has the gross earnings stored as text (e.g., "$2,923,706,026"). We need to write a function that takes this raw data frame, cleans the `lifetime_gross` column so it can be treated as a number, and then plots the top 10 movies in a bar chart.

**Filled Version (Answer)**

```{r}
#| eval: false
library(ggplot2)
library(dplyr)
library(readr)
library(stringr)

plot_top_grossing_movies <- function(data) {
  # Clean the data: convert currency text to numbers
  plot_data <- data %>%
    mutate(
      lifetime_gross_numeric = parse_number(lifetime_gross)
    ) %>% 
    # Get the top 10 movies by gross
    slice_max(order_by = lifetime_gross_numeric, n = 10)

  # Create the bar chart
  ggplot(plot_data, aes(x = lifetime_gross_numeric, y = reorder(title, lifetime_gross_numeric))) +
    geom_col(fill = "#e76f51") +
    scale_x_continuous(labels = scales::label_dollar(scale = 1/1e9, suffix = "B")) + # Format x-axis in billions
    labs(
      title = "Top 10 Lifetime Grossing Films",
      subtitle = "Source: Box Office Mojo",
      x = "Lifetime Gross (in Billions USD)",
      y = "Film Title"
    ) +
    theme_minimal()
}
```

**Fill-in-the-Blank Version**

```{r}
#| eval: false
library(ggplot2)
library(dplyr)
library(readr)
library(stringr)

# plot_top_grossing_movies <- function(data) {
#   # Clean the data
#   plot_data <- data %>%
#     mutate(
#       lifetime_gross_numeric = parse_number(____) # Convert the currency column
#     ) %>% 
#     slice_max(order_by = ____, n = 10) # Get top 10 movies
# 
#   # Create the bar chart
#   ggplot(plot_data, aes(x = ____, y = reorder(title, ____))) +
#     geom_col(fill = "#e76f51") +
#     scale_x_continuous(labels = scales::label_dollar(scale = 1/1e9, suffix = "B")) +
#     labs(
#       title = "Top 10 Lifetime Grossing Films",
#       subtitle = "Source: Box Office Mojo",
#       x = "Lifetime Gross (in Billions USD)",
#       y = "Film Title"
#     ) +
#     theme_minimal()
# }
```

**Scratch Version**

```{r}
#| eval: false
## TODO: Write a function called plot_top_grossing_movies that takes a data frame.
## TODO: Inside, use mutate() and parse_number() to clean the lifetime_gross column.
## TODO: Then, create a ggplot with geom_col() to visualize the top 10 movies.
```

-----

#### **Step 3: Plot the Scraped Data**

Finally, let's use our new function with the `top_movies_df` data frame we created in Step 1 to generate the plot.

**Filled Version (Answer)**

```{r}
#| eval: false
# Use the function to plot the scraped movie data
plot_top_grossing_movies(top_movies_df)
```

**Fill-in-the-Blank Version**

```{r}
#| eval: false
## Use the function to plot the scraped movie data
# plot_top_grossing_movies(____)
```

**Scratch Version**

```{r}
#| eval: false
## TODO: Call your plot_top_grossing_movies function with the top_movies_df data frame.
```



