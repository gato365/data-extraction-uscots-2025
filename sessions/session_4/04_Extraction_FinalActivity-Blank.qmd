---
title: "Session 4: Final Challenge and Reflection Fill-In (EMPTY)"
author: "Immanuel Williams & Ciera Millard"
date: "`r Sys.Date()`"
format: html
---






```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE,echo = TRUE)
```

# Session 4: Final Project Activity

In this capstone session, we will combine our API and web scraping skills to complete two mini-projects. 

Each project will involve writing a custom function to extract data and then using that function to create a unique data visualization.


## Part 1. Goals & Objectives


1. **Synthesize Skills:** Combine API requests and web scraping techniques in two separate, start-to-finish projects.
2. **Develop Reusable Functions:** Write modular R functions that handle the logic for extracting and cleaning data, making your code more readable and reusable.
3. **Master Data Cleaning:** Practice essential data cleaning skills required to transform raw, messy data from both APIs and HTML tables into a format ready for analysis.
4. **Create Meaningful Visualizations:** Use `ggplot2` to create two distinct data visualizations—a geographic map and a sorted bar chart—to effectively communicate the findings from your extracted data.



## Part 2: Conceptual Demonstration through projects

### **Project 1: Mapping U.S. Cities with the Geocoding API**

Our first goal is to take a list of U.S. cities, find their exact geographic coordinates using an API, and plot them on a map. For this, we'll use a new OpenWeatherMap endpoint: the **Geocoding API (v1.0)**.

#### **Step 1: Load Libraries**

We'll begin by loading all the libraries we need for API calls, data wrangling, and visualization.


**Fill-in-the-Blank Version**

```{r}
## EMPTY VERSION
# library(____)       # For making API requests
# library(____)       # For web scraping
# library(____)       # For functional programming like map()
# library(____)       # For building strings (URLs)
# library(____)       # For data manipulation
# library(____)       # For plotting
```


------------------------------------------------------------------------

#### **Step 2: Create a Geocoding Function**

Next, we'll write a function named `get_city_coords`. This function will take a city name as input, send a request to the OpenWeather Geocoding API, and return a clean data frame with the city's name, latitude, and longitude.


**Fill-in-the-Blank Version**

```{r}
## EMPTY VERSION
# get_city_coords <- function(city_name) {
#   # Construct the request URL
#   req_url <- glue("http://api.openweathermap.org/geo/1.0/direct?q={____(city_name)}&limit=1&appid={Sys.getenv('OPENWEATHER_API_KEY')}")
#   
#   # Perform the request
#   response <- request(____) %>% 
#     req_perform()
#   
#   # Check for success (status code 200)
#   if (resp_status(____) == ____) {
#     resp_body_json(____) %>% 
#       pluck(____) %>% # Pluck the first result
#       as_tibble() %>% 
#       select(name, lat, lon)
#   } else {
#     warning(paste("Could not find coordinates for:", city_name))
#     return(NULL)
#   }
# }
```


------------------------------------------------------------------------

#### **Step 3: Create a Plot of City Locations**

##### **Substep 3a: Get U.S. Map Data**

To create our map background, we use the `map_data()` function. This function retrieves a data frame containing the geographic polygon data—essentially a set of longitude and latitude coordinates—needed to draw the outlines of all U.S. states.

Code snippet

```{r}
# Get U.S. map data
us_map <- map_data("state")
```

**How This Works & Which Library**

-   **Library**: The `map_data()` function comes from the **`ggplot2`** package. It is designed specifically to get map data in a format that works seamlessly with `ggplot()`.

-   **Process**:

    1.  When you call `map_data("state")`, `ggplot2` accesses a built-in dataset (from the **`maps`** package) that contains the outlines of the world, countries, and U.S. states.

    2.  It returns a data frame where each row represents a single point (vertex) on the border of a state.

    3.  Crucially, it includes a `group` column. This column tells `ggplot2` which points should be connected to form a single, continuous shape (a state polygon), preventing lines from being drawn incorrectly across the map.

Now, we'll define a list of cities and use our new function with `purrr::map_dfr` to get the coordinates for all of them. Finally, we'll use `ggplot2` to plot these locations on a map of the United States.




```{r}
## EMPTY VERSION
## A vector of cities to map
# cities_to_map <- c("San Luis Obispo", "Chicago", "New York", "Atlanta", "Houston", "Des Moines")
# 
## Apply our function to each city using map_dfr
# city_locations_df <- map_dfr(____, ____)
# 
## Get U.S. map data
# us_map <- map_data("____")
# 
## Create the plot
# ggplot() +
#   geom_polygon(data = ____, aes(x = long, y = lat, group = group), fill = "gray90", color = "white") +
#   geom_point(data = ____, aes(x = lon, y = lat), color = "red", size = 3) +
#   labs(
#     title = "Locations of Selected U.S. Cities",
#     x = "Longitude",
#     y = "Latitude"
#   ) +
#   theme_minimal()
```


------------------------------------------------------------------------


::: {style="height: 300px; background-color: white;"}
:::

Of course. Here is the project completely revised to focus on the Box Office Mojo data, following the same structured, multi-version format.

-----

### **Project 2: Scraping and Plotting Box Office Data**

For this project, our goal is to scrape a list of the top-grossing films of all time directly from the **Box Office Mojo** website and create a visualization of the highest earners.

-----

#### **Step 1: Scrape Top-Grossing Movies**

First, we will scrape the Box Office Mojo website to get a clean table of the top lifetime grossing films.


**Fill-in-the-Blank Version**

```{r}
## EMPTY VERSION
# library(rvest)
# library(janitor)
# library(purrr)

## Define the URL for the Box Office Mojo chart
# mojo_url <- "https://www.boxofficemojo.com/chart/top_lifetime_gross/"
# 
## Read the HTML, find the table, and clean it up
# top_movies_df <- read_*(____) %>% 
#   html_*() %>% 
#   pluck(____) %>% # Which table on the page is it?
#   clean_names() 
# 
# print(top_movies_df)
```



#### **Step 2: Create a Function to Plot Top Movies**

The data we scraped has the gross earnings stored as text (e.g., "$2,923,706,026"). We need to write a function that takes this raw data frame, cleans the `lifetime_gross` column so it can be treated as a number, and then plots the top 10 movies in a bar chart.


**Fill-in-the-Blank Version**

```{r}
##EMPTY VERSION
# library(ggplot2)
# library(dplyr)
# library(readr)
# library(stringr)
# plot_top_grossing_movies <- function(data) {
#   # Clean the data
#   plot_data <- data %>%
#     mutate(
#       lifetime_gross_numeric = parse_number(____) # Convert the currency column
#     ) %>% 
#     slice_max(order_by = ____, n = 10) # Get top 10 movies
# 
#   # Create the bar chart
#   ggplot(plot_data, aes(x = ____, y = reorder(title, ____))) +
#     geom_col(fill = "#e76f51") +
#     scale_x_continuous(labels = scales::label_dollar(scale = 1/1e9, suffix = "B")) +
#     labs(
#       title = "Top 10 Lifetime Grossing Films",
#       subtitle = "Source: Box Office Mojo",
#       x = "Lifetime Gross (in Billions USD)",
#       y = "Film Title"
#     ) +
#     theme_minimal()
# }
```


-----

#### **Step 3: Plot the Scraped Data**

Finally, let's use our new function with the `top_movies_df` data frame we created in Step 1 to generate the plot.

**Filled Version (Answer)**

```{r}
# Use the function to plot the scraped movie data
plot_top_grossing_movies(top_movies_df)
```




## Part 3. Reflection

1.  Compare the data cleaning required for the API response in Project 1 (Geocoding) versus the scraped table in Project 2 (Box Office Mojo). Which required more work to prepare for plotting, and what does this tell you about the trade-offs between using APIs and web scraping?
2.  The Box Office Mojo scraper works today, but the website could be redesigned at any time. What specific part of our scraping code (`pluck(1)`) is most likely to break if the website's layout changes, and what would be your first step to debug it?
3.  Our `get_city_coords` function is useful, but it assumes the API key is stored in a specific way (`Sys.getenv('OPENWEATHER_API_KEY')`). What argument could you add to this function to make it more flexible and easier for another person to use with their own key?
4.  Think about the two datasets we created: one with city coordinates and one with movie revenues. What is a new, interesting question you could potentially answer by finding a way to **join** these two datasets (perhaps with a third dataset)? What would be the biggest challenge in that process?

