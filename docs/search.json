[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "",
    "text": "Empowering statistics educators with real-world data skills.\nThis site houses all the materials for our hands-on workshop at USCOTS 2025.\nWe‚Äôre learning how to extract, clean, and use live data from the web and APIs‚Äîbringing authentic data science into your classroom."
  },
  {
    "objectID": "index.html#workshop-goals",
    "href": "index.html#workshop-goals",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Workshop Goals",
    "text": "Workshop Goals\n\nUnderstand why extracting dynamic data is essential in modern statistics education.\nLearn how to read HTML tables and use web APIs to collect real-world data.\nUse tidyverse tools to transform and clean this data for analysis.\nBuild confidence integrating unstructured data into your courses."
  },
  {
    "objectID": "index.html#who-this-workshop-is-for",
    "href": "index.html#who-this-workshop-is-for",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Who This Workshop Is For",
    "text": "Who This Workshop Is For\n\nStatistics educators and data science instructors\nComfortable with R and tidyverse\nCurious about web scraping, APIs, and teaching modern data practices"
  },
  {
    "objectID": "index.html#whats-inside",
    "href": "index.html#whats-inside",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "What‚Äôs Inside",
    "text": "What‚Äôs Inside\n\nSession 1 ‚Äì Why and How We Extract Data\n\nÔ∏è Session 2 ‚Äì Working with APIs (Weather Data)\n\nSession 3 ‚Äì Scraping Sports Data from HTML\n\nSession 4 ‚Äì Final Challenge and Reflection\n\nExplore these under the Sessions tab above."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Getting Started",
    "text": "Getting Started\n\nHead over to Session 1 - Introduction to begin your learning journey.\nOr, check out the Organized Outline for a roadmap of the workshop.\n\n\n##Ô∏è Tools We‚Äôll Use\n\nrvest, httr, jsonlite, purrr, tidyverse\nJupyter via CourseKata CKHub\nRStudio / Quarto"
  },
  {
    "objectID": "index.html#quote-to-frame-the-work",
    "href": "index.html#quote-to-frame-the-work",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Quote to Frame the Work",
    "text": "Quote to Frame the Work\n\n‚ÄúThe world is one big dataset‚Äîif we learn how to extract it.‚Äù\n‚Äî Inspired by data educators like you"
  },
  {
    "objectID": "index.html#lets-get-started",
    "href": "index.html#lets-get-started",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Let‚Äôs Get Started!",
    "text": "Let‚Äôs Get Started!\nReady to transform how your students work with data?\nüìé Navigate using the menu above and join us in making data science real, relevant, and resilient."
  },
  {
    "objectID": "sessions/session_1/libraries_of_packages.html",
    "href": "sessions/session_1/libraries_of_packages.html",
    "title": "Library of Packages",
    "section": "",
    "text": "Chloropleth Map (geoapify.com)\n\n\n\n\nIntroduction to Libraries & Glossary\nThis project, when possible, utilizes tidyverse packages. tidyverse is a collection of open-source packages that are well-integrated to tackle problems of data extraction, manipulation, transformation, exploration, and visualization.\n\n\n\nCollection of Tidyverse packages.\n\n\n\nlibrary(rvest)      # Web scraping\nlibrary(dplyr)      # Data manipulation\nlibrary(stringr)    # String cleaning\nlibrary(rlang)      # Advanced evaluation\nlibrary(purrr)      # Functional tools\nlibrary(ggplot2)    # Visualizations\nlibrary(httr2)      # Makes web requests\nlibrary(tibble)     # Easier and prettier data frames\nlibrary(lubridate)  # Handles dates\nlibrary(dotenv)     # Loads environment variables from .Renviron\nlibrary(glue)       # Easier string concatenation\nlibrary(tigris)     # U.S. shapefiles for mapping\n\nThis is a list of R packages used in this project, accompanied by brief descriptions of what they do.\nrvest\n\nUsed for web scraping: parses HTML/XML documents into a navigable format.\nExtracts structured data using CSS selectors, XPath, or tag-based search (html_table(), html_text(), etc.).\n\n\ndplyr\n\nCore package for tidyverse-style data manipulation (filter(), mutate(), select(), etc.).\nSupports chaining operations with %&gt;% / |&gt;, making data workflows readable and efficient.\n\n\nstringr\n\nProvides a consistent set of functions for string manipulation.\nHandles pattern matching, replacement, splitting, and formatting.\n\n\nrlang\n\nSupports advanced evaluation and programming with tidyverse tools.\nUseful when writing custom functions that dynamically reference or modify variables.\n\n\npurrr\n\nEnables functional programming with mapping tools like map(), map_df(), walk(), etc.\nReplaces (many) for-loops and supports clean iteration over lists and vectors.\n\n\nggplot2\n\nGraphics package for building layered, flexible visualizations.\nSupports various plot types, themes, scales, and faceting for data storytelling.\n\n\nhttr2\n\nModern HTTP client designed for tidyverse-like API interaction.\nReplaces httr with a more intuitive and pipeable interface (request() |&gt; req_perform()).\n\n\ntibble\n\nA modern rethinking of the data.frame that prints cleaner and behaves more predictably.\nDefault in tidyverse workflows; avoids surprises like string-to-factor conversion.\n\n\nlubridate\n\nSimplifies working with dates and times: parsing, formatting, and arithmetic.\nMakes it easy to extract components (e.g., month, weekday) and perform date math.\n\n\ndotenv\n\nLoads environment variables from a .env or .Renviron file into R.\nKeeps sensitive data like API keys out of your scripts and version control.\n\n\nglue\n\nProvides string interpolation (e.g., glue(\"Hello {name}\")).\nCleaner and safer than paste() for building URLs, messages, or SQL queries.\n\n\ntigris\n\nDownloads shapefiles and geographic boundary data (e.g., counties, states) from the U.S. Census Bureau.\nReturns spatial sf data frames, making it easy to map and visualize geographic data."
  },
  {
    "objectID": "sessions/session_4/04_Extraction_FinalActivity.html",
    "href": "sessions/session_4/04_Extraction_FinalActivity.html",
    "title": "Session 4: Final Activity",
    "section": "",
    "text": "Now that we have worked with HTML elements and API calls, let‚Äôs put these skills together to create a weather map of Iowa counties based on the maximum temperature at each county‚Äôs administrative seat (county seat).\n\n\n\nChloropleth Map (geoapify.com)\n\n\n\n\n\nWe‚Äôll begin by loading the libraries necessary for data manipulation, API calls, and visualization.\n\n# Step 1: Load Libraries\n# Step 1a: General utilities\nlibrary(httr2)       # Makes web requests\nlibrary(tibble)      # Easier and prettier data frames\nlibrary(lubridate)   # Handles dates\nlibrary(ggplot2)     # Data visualization\nlibrary(dplyr)       # Data manipulation\nlibrary(dotenv)      # Loads environment variables from .Renviron\nlibrary(glue)        # Easier string concatenation\nlibrary(purrr)       # Functional programming tools\nlibrary(rvest)       # Web scraping\nlibrary(tigris)      # U.S. shapefiles for mapping\nlibrary(stringr)     # String manipulation and handling\n\n\n# Step 1b: Load API key from .Renviron.txt\ndotenv::load_dot_env(file = \".Renviron.txt\")\n\n\n\n\n\nHere we extract a table from Wikipedia listing all Iowa counties and their corresponding county seats.\n\n# Step 2: Scrape HTML Table from Wikipedia\n# Step 2a: Set URL\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n\n# Step 2b: Read HTML\nwebpage &lt;- url |&gt;  \n  rvest::read_html()\n\n# Step 2c: Extract all HTML tables from the page\nhtml_tables &lt;- webpage |&gt; \n  rvest::html_table()\n\n# Step 2d: Select the correct table (based on inspection)\ntable1 &lt;- html_tables |&gt; \n  purrr::pluck(2)\n\n# Step 2e: Clean and prepare county seat names\n#         Add ', IA, USA' to ensure geolocation works with OpenWeather API\n#         Use only the first County Seat city name when mulitple exist\ncounty_seats_df &lt;- table1 |&gt; \n  mutate(\n    `County seat[4]` = str_split(`County seat[4]`, \" and \") |&gt; sapply(`[`, 1),\n    city = paste0(`County seat[4]`, \", IA, USA\")\n  )\n\nDiscussion: Why would we need to append‚Äù, IA, USA‚Äù to the city name?\nWhat would happen if we do not?\nA: This helps geocoding APIs return accurate coordinates. (Just ‚ÄúAmes‚Äù returns a rural town of 600 in Northern France)\n\n\n\nNot Exactly Ames, IA, USA. Church of Ames, France (Wikipedia)\n\n\n\n\n\n\nWe now define a function to call the OpenWeather Geocoding and One Call APIs. Then we use it to retrieve temperature data for each city.\n\n# Step 3: Define function to get geocoded weather data from OpenWeather\nget_city_weather &lt;- function(city, date = Sys.Date()) {\n  # Step 3a: Get coordinates from geocoding API\n  geo_url &lt;- glue(\n    \"http://api.openweathermap.org/geo/1.0/direct?\",\n    \"q=\", URLencode(city),\n    \"&limit=1&appid=\", Sys.getenv(\"API_KEY\"))\n  geo_response &lt;- req_perform(request(geo_url))\n\n  if (resp_status(geo_response) == 200) {\n    geo_data &lt;- as.data.frame(resp_body_json(geo_response))\n    if (nrow(geo_data) == 0) return(NULL)\n\n    lat &lt;- geo_data$lat\n    lon &lt;- geo_data$lon\n\n    # Step 3b: Call the weather summary API\n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", format(date, \"%Y-%m-%d\"),\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\")\n    weather_response &lt;- req_perform(request(weather_url))\n\n    if (resp_status(weather_response) == 200) {\n      weather_data &lt;- resp_body_json(weather_response)\n      tibble(\n        city = city,\n        date = date,\n        lat = lat,\n        lon = lon,\n        temp_max = weather_data$temperature$max,\n        temp_min = weather_data$temperature$min\n      )\n    } else return(NULL)\n  } else return(NULL)\n}\n\n\n# Step 3c: Apply the function to all county seat cities\ncities &lt;- county_seats_df |&gt; \n  pull(12)\nweather_df &lt;- bind_rows(lapply(cities, get_city_weather))\n\nDiscussion: What happens if one of the cities fails to return a result?\nWe could add error handling or a fallback method, such as purrr::possibly()?\n\n\n\n\nNow we join our temperature data with spatial geometries (map pieces) from the tigris package so we can map it.\n\n# Step 4: Merge weather data into spatial map\n# Step 4a: Join temperature data with counties via county seat\ncounty_temp_df &lt;- county_seats_df |&gt; \n  left_join(weather_df, by = \"city\")  # joins on city (admin chair)\n\n# Step 4b: Load Iowa county shapes from tigris\niowa_counties &lt;- tigris::counties(state = \"IA\", cb = TRUE, class = \"sf\")\n\n# Step 4c: Join temperature data into spatial counties by NAME\n# (Be sure county names match exactly)\niowa_map_filled &lt;- iowa_counties |&gt; \n  left_join(county_temp_df, by = c(\"NAMELSAD\" = \"County\"))\n\nDiscussion: How could we verify that all counties successfully matched?\n\n\n\n\nFinally, we use ggplot2 and geom_sf() to create a choropleth map of Iowa counties filled by temperature.\n\n# Step 5: Plot the map\n# Step 5a: Use fill aesthetic to show temperature per county\n\nggplot(iowa_map_filled) +\n  geom_sf(aes(fill = temp_max), color = \"white\") +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Max Temp (¬∞F)\", na.value = \"grey90\") +\n  labs(\n    title = \"Iowa County Temperatures by County Seat\",\n    subtitle = paste(\"Date:\", unique(weather_df$date)),\n    caption = \"Source: OpenWeather API\"\n  ) +\n  theme_minimal(base_size = 14)\n\nIdeas to expand this visualization:\n- Add interactivity with plotly\n- Animate changes over multiple dates\n- Compare actual temps to historical averages\n- Add city point labels or icons"
  },
  {
    "objectID": "sessions/session_4/04_Extraction_FinalActivity.html#session-4-final-activity",
    "href": "sessions/session_4/04_Extraction_FinalActivity.html#session-4-final-activity",
    "title": "Session 4: Final Activity",
    "section": "",
    "text": "Now that we have worked with HTML elements and API calls, let‚Äôs put these skills together to create a weather map of Iowa counties based on the maximum temperature at each county‚Äôs administrative seat (county seat).\n\n\n\nChloropleth Map (geoapify.com)\n\n\n\n\n\nWe‚Äôll begin by loading the libraries necessary for data manipulation, API calls, and visualization.\n\n# Step 1: Load Libraries\n# Step 1a: General utilities\nlibrary(httr2)       # Makes web requests\nlibrary(tibble)      # Easier and prettier data frames\nlibrary(lubridate)   # Handles dates\nlibrary(ggplot2)     # Data visualization\nlibrary(dplyr)       # Data manipulation\nlibrary(dotenv)      # Loads environment variables from .Renviron\nlibrary(glue)        # Easier string concatenation\nlibrary(purrr)       # Functional programming tools\nlibrary(rvest)       # Web scraping\nlibrary(tigris)      # U.S. shapefiles for mapping\nlibrary(stringr)     # String manipulation and handling\n\n\n# Step 1b: Load API key from .Renviron.txt\ndotenv::load_dot_env(file = \".Renviron.txt\")\n\n\n\n\n\nHere we extract a table from Wikipedia listing all Iowa counties and their corresponding county seats.\n\n# Step 2: Scrape HTML Table from Wikipedia\n# Step 2a: Set URL\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n\n# Step 2b: Read HTML\nwebpage &lt;- url |&gt;  \n  rvest::read_html()\n\n# Step 2c: Extract all HTML tables from the page\nhtml_tables &lt;- webpage |&gt; \n  rvest::html_table()\n\n# Step 2d: Select the correct table (based on inspection)\ntable1 &lt;- html_tables |&gt; \n  purrr::pluck(2)\n\n# Step 2e: Clean and prepare county seat names\n#         Add ', IA, USA' to ensure geolocation works with OpenWeather API\n#         Use only the first County Seat city name when mulitple exist\ncounty_seats_df &lt;- table1 |&gt; \n  mutate(\n    `County seat[4]` = str_split(`County seat[4]`, \" and \") |&gt; sapply(`[`, 1),\n    city = paste0(`County seat[4]`, \", IA, USA\")\n  )\n\nDiscussion: Why would we need to append‚Äù, IA, USA‚Äù to the city name?\nWhat would happen if we do not?\nA: This helps geocoding APIs return accurate coordinates. (Just ‚ÄúAmes‚Äù returns a rural town of 600 in Northern France)\n\n\n\nNot Exactly Ames, IA, USA. Church of Ames, France (Wikipedia)\n\n\n\n\n\n\nWe now define a function to call the OpenWeather Geocoding and One Call APIs. Then we use it to retrieve temperature data for each city.\n\n# Step 3: Define function to get geocoded weather data from OpenWeather\nget_city_weather &lt;- function(city, date = Sys.Date()) {\n  # Step 3a: Get coordinates from geocoding API\n  geo_url &lt;- glue(\n    \"http://api.openweathermap.org/geo/1.0/direct?\",\n    \"q=\", URLencode(city),\n    \"&limit=1&appid=\", Sys.getenv(\"API_KEY\"))\n  geo_response &lt;- req_perform(request(geo_url))\n\n  if (resp_status(geo_response) == 200) {\n    geo_data &lt;- as.data.frame(resp_body_json(geo_response))\n    if (nrow(geo_data) == 0) return(NULL)\n\n    lat &lt;- geo_data$lat\n    lon &lt;- geo_data$lon\n\n    # Step 3b: Call the weather summary API\n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", format(date, \"%Y-%m-%d\"),\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\")\n    weather_response &lt;- req_perform(request(weather_url))\n\n    if (resp_status(weather_response) == 200) {\n      weather_data &lt;- resp_body_json(weather_response)\n      tibble(\n        city = city,\n        date = date,\n        lat = lat,\n        lon = lon,\n        temp_max = weather_data$temperature$max,\n        temp_min = weather_data$temperature$min\n      )\n    } else return(NULL)\n  } else return(NULL)\n}\n\n\n# Step 3c: Apply the function to all county seat cities\ncities &lt;- county_seats_df |&gt; \n  pull(12)\nweather_df &lt;- bind_rows(lapply(cities, get_city_weather))\n\nDiscussion: What happens if one of the cities fails to return a result?\nWe could add error handling or a fallback method, such as purrr::possibly()?\n\n\n\n\nNow we join our temperature data with spatial geometries (map pieces) from the tigris package so we can map it.\n\n# Step 4: Merge weather data into spatial map\n# Step 4a: Join temperature data with counties via county seat\ncounty_temp_df &lt;- county_seats_df |&gt; \n  left_join(weather_df, by = \"city\")  # joins on city (admin chair)\n\n# Step 4b: Load Iowa county shapes from tigris\niowa_counties &lt;- tigris::counties(state = \"IA\", cb = TRUE, class = \"sf\")\n\n# Step 4c: Join temperature data into spatial counties by NAME\n# (Be sure county names match exactly)\niowa_map_filled &lt;- iowa_counties |&gt; \n  left_join(county_temp_df, by = c(\"NAMELSAD\" = \"County\"))\n\nDiscussion: How could we verify that all counties successfully matched?\n\n\n\n\nFinally, we use ggplot2 and geom_sf() to create a choropleth map of Iowa counties filled by temperature.\n\n# Step 5: Plot the map\n# Step 5a: Use fill aesthetic to show temperature per county\n\nggplot(iowa_map_filled) +\n  geom_sf(aes(fill = temp_max), color = \"white\") +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Max Temp (¬∞F)\", na.value = \"grey90\") +\n  labs(\n    title = \"Iowa County Temperatures by County Seat\",\n    subtitle = paste(\"Date:\", unique(weather_df$date)),\n    caption = \"Source: OpenWeather API\"\n  ) +\n  theme_minimal(base_size = 14)\n\nIdeas to expand this visualization:\n- Add interactivity with plotly\n- Animate changes over multiple dates\n- Compare actual temps to historical averages\n- Add city point labels or icons"
  },
  {
    "objectID": "sessions/session_2/02_Extraction_Weather_Data_API.html",
    "href": "sessions/session_2/02_Extraction_Weather_Data_API.html",
    "title": "Session 2: Weather Data - OpenWeatherAPI",
    "section": "",
    "text": "Explain what an API is and how it supports data extraction Theoretical elements of API\nMake requests to a public API and interpret the JSON response\nUnderstand and apply HTTP status codes and API keys\nWrite clean, readable code to extract and parse API data\n\n(Change the based on concepte Foundation) (Mention Querying data base more as an action)\n\n\n\nPart A. Theoretical ideas of APIs\nNote 1:\n\nThis is not a webdeveloper nor a CS course but with a decent understanding of the logic, you and your students will appreciate the utilizartion of web scrapiing more\n\n\n\nWhat is an API? (Again)\nIt is the abiluty for software to communicate\n\n\n\nAPI Call (PhoenixNap.com)\n\n\n\nQ1: What is its utility of APIs? (multiple choice)\n\nNote 2:\n\nThis imagae is overly simipliefed in that a client left makes request through an api to a server/database then tthe server/database provides responses\nA client and server can exist on the same computer. This is often what‚Äôs happening in local development (e.g., querying a local database from R)\n\n\n\n\nLets go deepeer into understanding Define:\nClient (request) ‚Äì&gt; API ‚Äì&gt; Server ‚Äì&gt; Database\nClient &lt;‚Äì API &lt;‚Äì Server (response) &lt;‚Äì Database\n\n\n\nGATO365 API Request & Response\n\n\nQ2. Matching You might show this flow visually and say:\n‚ÄúThe [API] is the waiter.‚Äù\n‚ÄúThe [client] is the customer.‚Äù\n‚ÄúThe [server] is the kitchen.‚Äù\n‚ÄúThe [database] is the fridge or pantry.‚Äù\nNote 3:\n\nAction: Client makes a request\nAction: Server queries Database provides a response\n\n\n\n\nLets spend some more time on the request and response\nThe client sends a request asking for info (like taylor swift or today‚Äôs weather). This request includes:\n\nA URL (e.g., with parameters like ?q=San+Luis+Obispo)\nPossibly an API key\nA method (e.g., GET or POST)\n\nThe server then returns a response which contains:\n\ndata (temperature, artist name, forecast, etc.)\nmetadata (This is information about the response.)\nstatus code (Tells you whether the request was successful)\n\nThis information is traditionally provided in JSON Format.\n\n** Server Response  *************************\n\n\n\nLet‚Äôs focus on what the response is 1st (what we receive from the server):\nBelow is an example GIF of the information senr from the server in JSON format:\n\n\n\nGATO365 Anatomy JSON\n\n\nNote 4:\n\nWhen we send a request to an API, we get a response body, which includes the content ‚Äî typically JSON ‚Äî divided into data (what we wanted), metadata (info about the data), and a status_code telling us if the request worked.\n\n\n\n\nStatus codes tell you what happened with your request:\n-   `100s`: Info\n-   `200s`: Success (highlight: 200 OK)\n-   `300s`: Redirect\n-   `400s`: Client error\n-   `500s`: Server error\nNote 5:\n\nEmphasize: In most data APIs, your goal is to get a 200 response.\nUse examples like making up a nonexistent city or artist to show how an API might respond with a 400 or 404.\n\n\n** Client Request  *************************\n\n\n\nWhat type of client requests can we make?\nCRUD Framework (Create, Read, Update, Delete)\n\nThough APIs allow all four, Read (GET) is most common in data science.\nRESTful API mapping:\n\nCreate ‚Üí POST\nRead ‚Üí GET\nUpdate ‚Üí PUT/PATCH\nDelete ‚Üí DELETE\n\n\n(TODO: Create a GIF for each the above that is really illuminating, so GET and POST GIFs, its created, you have to actual turn it into a gif becuase it is actual a video)\n\n\n\nGATO365 Get Request\n\n\n\n\n\nGATO365 Get Request\n\n\nNote 4:\n\nWe‚Äôll focus mostly on GET, and occasionally show POST (e.g., retrieving personalized weather data).\nBriefly mention: Apps like Instagram or Facebook rely on all four CRUD operations‚Äîupdating posts, deleting comments, etc.\n\nP6. Setup API_Key [[Based on time do One of the three steps]]\n[[1. email attendees to go to the weather website and get API key or whatever information needed before the conference. Create a video that‚Äôs displaying how to do this]] [[1a. Discuss .Renviron.txt, how we use: to create and edit API key: usethis::edit_r_environ()]] [[1b. Use Sys.getenv(\"API_KEY\") to see API in console]] [[Note that you have to use the 1a to see the api key again]] Restart R\n[[2. have attendees get the key during the break session if they have not done so already]]\n[[3. use a common key, but tell them it is bad practice]]\n[[regardless of the decision made of the three options above have attendees store information in the environment file]]\nNote 5: there are many ways of doing this, but I‚Äôm going to stick with using tidyverse functions.I‚Äôm going to show you two ways to actually implement the query using the one way of one of the ways of doing this within a tiny verse using string glue\nP7: Requests, URLs & Queries P7. so what we‚Äôre going to first do is create our response and the most ideal way. - A request begins with a URL, which contains both:\n\nThe endpoint (base address of the API)\nThe query string (additional key-value pairs that modify the request)\nWe often need to glue strings together to build this full URL dynamically.\nA request is not ‚Äúautomatically‚Äù turned into JSON when sent ‚Äî it‚Äôs the response that‚Äôs usually formatted as JSON. The request is often URL-encoded if it‚Äôs a GET.\n\nP8: What Happens Under the Hood\n\nWhen we use a URL like ...?q=San+Luis+Obispo&appid=..., we‚Äôre constructing a query string, which is appended to the base URL.\nThink of this as ‚Äúasking the question‚Äù‚Äîthe query string shapes the request.\nThe server receives the request, processes it, and responds with structured data (typically JSON).\nWe‚Äôre not sending JSON in this case‚Äîwe‚Äôre sending a URL with parameters. JSON is returned to us as a response format.\n\n\n\n\n\n\n\nlibrary(httr2)       # Makes web requests\nlibrary(glue)        # Glue Strings\n\n\ncity_name &lt;- \"San Luis Obispo\"\n\n(TODO: place video into gif of URL anantomy)\n\ncurrent_weather_url &lt;- glue(\"https://api.openweathermap.org/data/2.5/weather?\",\n                            \"q=\", URLencode(city_name),\n                            \"&appid=\", Sys.getenv(\"API_KEY\"),\n                            \"&units=imperial\")\n\n\ncurrent_weather_url\n\nNote 6: explicily state what each element is\n\nreq &lt;- request(current_weather_url)\n\nBuilt Request Object\n\nreq\n\nNote 7:\n\nThis method shows the anatomy of the URL explicitly.\nGreat for emphasizing how query parameters are constructed using strings.\nHelps reinforce the idea of ‚Äúasking a question via the URL.‚Äù\n\nNote 8:\n\nWe are going to do it again in a diferent way but we are goint to process the response further here becuae\n\nI wanted you to undertstadn the anatomy of the URL\nHave multiple ways of doing the same thing\n\n\n\n\n\nStep 1: Build Request Object\n\nreq &lt;- request(\"https://api.openweathermap.org/data/2.5/weather\") |&gt; \n  req_url_query(\n    q = city_name,\n    appid = Sys.getenv(\"API_KEY\"),\n    units = \"imperial\"\n  )\n\n\nreq\n\nNote 9:\n\nThis method abstracts away the string building.\nIt‚Äôs cleaner and reduces chances of typos or formatting errors.\nTeaches students to treat query arguments like named inputs.\nYou can still inspect the built URL using req$url.\n\nStep 2: Make request\n\nresponse &lt;- req_perform(req)\n\n\nresponse\n\nNot a step: View content Type\n\ncontent_type &lt;- resp_content_type(response)\n\n\ncontent_type\n\n\nlibrary(dplyr)\n\n\n\n\n\n## IF the status code is 200 we are good\nif (resp_status(response) == 200) {\n  \n  # Parse JSON\n  result &lt;- resp_body_json(response)\n  \n  # Print Results as JSON\n  print(result)\n  \n  #---------------------------------------\n  \n  # Convert to Data Frame directly\n  current_weather_df &lt;- as.data.frame(result)\n  \n  # Print Results as Data Frame, using dplyr\n  print(select(current_weather_df, name, coord.lon, coord.lat, weather.main, main.temp))\n  \n  \n## ELSE state there is an Error\n} else {\n  cat(\"Failed. Status code:\", resp_status(response), \"\\n\")\n}\n\nNote 10: There are many other variable that is given but we focus on There is more information that is given but we are interested in the body of the request, hence we use resp_body_json that takes the body and that is what we are after as a json and we then convert it into a data frame\n\n\n\n\n\n(TODO: Remove certain element of functions that are needed to understand the function) (TODO: Break up functions 1 and 2, to be better compartmentalized)\n\n## Step 1: Define function \"get_city_coords\" that accepts the parameter \"city\"\nget_city_coords &lt;- function(city){\n  \n## Step 2: Create API request URL\n  \n  geo_url &lt;- glue(\n  \"http://api.openweathermap.org/geo/1.0/direct?\",\n  \"q=\", URLencode(city),\n  \"&limit=1&appid=\", Sys.getenv(\"API_KEY\")\n)\n ## Step 3: Use req_perform() and request() to call the API with the URL request \n\ngeo_response &lt;- req_perform(request(geo_url))\n  \n\n## Step 4: If the status code is 200 (OK), use resp_body_json() to parse our response and as.data.frame to coerce it to data.frame.\n\nif (resp_status(geo_response) == 200) {\n  geo_data_df &lt;- resp_body_json(geo_response) |&gt; \n    as.data.frame()\n  \n  \n  ## Step 5: Assess if the output has 0 length, meaning no result. If so, stop and display an error message.  \n  \n  if (length(geo_data_df) == 0) {\n    stop(\"City not found. Please check the city name.\")\n  }\n  \n  ## Step 6: Assign latitude and longitude to variables, and use round() to clip it down to 2 decimal places.\n  mod_1_geo_data_df &lt;- geo_data_df |&gt; \n    mutate(lat = round(lat,2),\n           lon = round(lon,2))\n\n  ## Step 7: Select Certain Columns (chaptgpt)\n  mod_2_geo_data_df &lt;- mod_1_geo_data_df |&gt; \n    select(country,name,lat,lon) |&gt; \n    rename(city = name)\n  \n  ## Step 8: Return data frame with the country, city name and latitude / longitude.  \n  return(mod_2_geo_data_df)\n  \n  \n   }\n}\n\nP. Lets try out this new function on the city\n\nget_city_coords(city_name)\n\nP. Lets Look at multtiple cities using the the map_df fucntion within the purrr package\n\nlibrary(purrr)\n\n# List of cities you want to geocode\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n\n# Use walk() from purrr to apply the function to each city\nmap_df(cities, get_city_coords)\n\n\n\n\n(TODO: Remove certain element of functions that are needed to understand the function) (highltight the function days in this)\n\nlibrary(lubridate)   # Time and date handling\n\nP. Emphasize the new endpoint (https://api.openweathermap.org/data/3.0/onecall/day_summary?) an how it is a paid subscription\n\nget_past_weather_by_city &lt;- function(city, days) {\n  # Step 1: Get city coordinates\n  coords_df &lt;- get_city_coords(city)\n  lat &lt;- coords_df$lat\n  lon &lt;- coords_df$lon\n  \n  cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n  \n  # Step 2: Create vector of past dates\n  date_range &lt;- as.character(today() - days(1:days))\n  \n  # Step 3: Define function for single date\n  fetch_day_summary &lt;- function(date) {\n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", date,\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\"\n    )\n    \n    response &lt;- req_perform(request(weather_url))\n    \n    if (resp_status(response) == 200) {\n      resp_body_json(response) |&gt; \n        as.data.frame() |&gt; \n        mutate(city = city, date = date)\n    } else {\n      warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(response)))\n      return(NULL)\n    }\n  }\n  \n  # Step 4: Map over date_range and bind into a single data frame\n  map_dfr(date_range, fetch_day_summary)\n}\n\n\nget_past_weather_by_city(city_name, 5)\n\nP. There are alot of ways of doing this, I decided not to use for loop in R\n\nnum_days &lt;- 5\n\n# Get historical weather data for each city using map_dfr\nall_weather_df &lt;- map_dfr(\n  cities,\n  ~ get_past_weather_by_city(.x, num_days)\n)\n\nNote: Emphasize error handling\n\n\n\n\nPurpose: Retrieves current weather conditions for a single city. Demonstrates basic GET usage and parsing a flat JSON structure.\n\nget_city_current_weather &lt;- function(city) {\n  url &lt;- glue::glue(\n    \"https://api.openweathermap.org/data/2.5/weather?\",\n    \"q=\", URLencode(city),\n    \"&appid=\", Sys.getenv(\"API_KEY\"),\n    \"&units=imperial\"\n  )\n  \n  response &lt;- request(url) |&gt; req_perform()\n  \n  if (resp_status(response) == 200) {\n    response |&gt; \n      resp_body_json() |&gt; \n      purrr::pluck(\"main\") |&gt; \n      tibble::as_tibble() |&gt; \n      dplyr::select(temp, humidity) |&gt; \n      dplyr::mutate(\n        city = city,\n        description = resp_body_json(response) |&gt; purrr::pluck(\"weather\", 1, \"description\")\n      ) |&gt; \n      dplyr::select(city, temp, humidity, description)\n  } else {\n    warning(\"Failed to retrieve current weather for \", city)\n    return(NULL)\n  }\n}\n\nP. Run for a Single City (e.g., ‚ÄúAtlanta‚Äù)\n\nget_city_current_weather(\"Atlanta\")\n\nP. Run for a Vector of Cities Using purrr::map_dfr()\n\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n\nweather_df &lt;- purrr::map_dfr(cities, get_city_current_weather)\n\n\n\n\n\nPurpose: Retrieves the next 5 days of forecast data (in 3-hour intervals). This introduces nested lists and flattening structures.\nNote: we are now getting times as well\n\nget_city_forecast_5day &lt;- function(city) {\n  url &lt;- glue::glue(\n    \"https://api.openweathermap.org/data/2.5/forecast?\",\n    \"q=\", URLencode(city),\n    \"&appid=\", Sys.getenv(\"API_KEY\"),\n    \"&units=imperial\"\n  )\n  \n  response &lt;- httr2::req_perform(httr2::request(url))\n  \n  if (httr2::resp_status(response) == 200) {\n  response |&gt; \n      resp_body_json() |&gt; \n      purrr::pluck(\"list\") |&gt; \n      purrr::map_dfr(\n        ~ tibble::tibble(\n            city = city,\n            timestamp = .x$dt_txt,\n            temp = .x$main$temp,\n            weather = .x$weather |&gt; purrr::pluck(1, \"description\")\n        )\n      )\n  } else {\n    warning(\"Failed to retrieve forecast for \", city)\n    return(NULL)\n  }\n}\n\nP. Try for city notice the number of rows and columns\n\nget_city_forecast_5day(\"Atlanta\")\n\nP. Lets Look at multiple cities\n\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n\nforecast_df &lt;- purrr::map_dfr(cities, get_city_forecast_5day)\n\n\n\n\n\nPurpose: Uses lat and lon to query current air quality. Demonstrates chaining of API requests (e.g., using get_city_coords() first), and different JSON structures.\n\nget_air_pollution_by_coords &lt;- function(lat, lon) {\n  url &lt;- glue::glue(\n    \"http://api.openweathermap.org/data/2.5/air_pollution?\",\n    \"lat=\", lat,\n    \"&lon=\", lon,\n    \"&appid=\", Sys.getenv(\"API_KEY\")\n  )\n  \n  response &lt;- request(url) |&gt; req_perform()\n  \n  if (resp_status(response) == 200) {\n    response |&gt;\n      resp_body_json() |&gt;\n      purrr::pluck(\"list\", 1) |&gt;\n      {\\(x) tibble::tibble(\n        aqi = x$main$aqi,\n        co = x$components$co,\n        pm2_5 = x$components$pm2_5,\n        pm10 = x$components$pm10\n      )}()\n  } else {\n    warning(\"Failed to retrieve air pollution data for lat = \", lat, \", lon = \", lon)\n    return(NULL)\n  }\n}\n\nP. Step-by-Step for Usage with a Data Frame, use get_city_coords to get the cities of lon and lat.\n\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\ncity_coords_df &lt;- map_dfr(cities, get_city_coords)\n\nP. Get Air Pollution Data for All Cities\n\nlibrary(tidyr)\n\n\npollution_df &lt;- city_coords_df |&gt; \n  mutate(\n    pollution = map2(lat, lon, get_air_pollution_by_coords)\n  ) |&gt; \n  unnest(pollution)"
  },
  {
    "objectID": "sessions/session_2/02_Extraction_Weather_Data_API.html#session-2-api-fundamentals",
    "href": "sessions/session_2/02_Extraction_Weather_Data_API.html#session-2-api-fundamentals",
    "title": "Session 2: Weather Data - OpenWeatherAPI",
    "section": "",
    "text": "Explain what an API is and how it supports data extraction Theoretical elements of API\nMake requests to a public API and interpret the JSON response\nUnderstand and apply HTTP status codes and API keys\nWrite clean, readable code to extract and parse API data\n\n(Change the based on concepte Foundation) (Mention Querying data base more as an action)\n\n\n\nPart A. Theoretical ideas of APIs\nNote 1:\n\nThis is not a webdeveloper nor a CS course but with a decent understanding of the logic, you and your students will appreciate the utilizartion of web scrapiing more\n\n\n\nWhat is an API? (Again)\nIt is the abiluty for software to communicate\n\n\n\nAPI Call (PhoenixNap.com)\n\n\n\nQ1: What is its utility of APIs? (multiple choice)\n\nNote 2:\n\nThis imagae is overly simipliefed in that a client left makes request through an api to a server/database then tthe server/database provides responses\nA client and server can exist on the same computer. This is often what‚Äôs happening in local development (e.g., querying a local database from R)\n\n\n\n\nLets go deepeer into understanding Define:\nClient (request) ‚Äì&gt; API ‚Äì&gt; Server ‚Äì&gt; Database\nClient &lt;‚Äì API &lt;‚Äì Server (response) &lt;‚Äì Database\n\n\n\nGATO365 API Request & Response\n\n\nQ2. Matching You might show this flow visually and say:\n‚ÄúThe [API] is the waiter.‚Äù\n‚ÄúThe [client] is the customer.‚Äù\n‚ÄúThe [server] is the kitchen.‚Äù\n‚ÄúThe [database] is the fridge or pantry.‚Äù\nNote 3:\n\nAction: Client makes a request\nAction: Server queries Database provides a response\n\n\n\n\nLets spend some more time on the request and response\nThe client sends a request asking for info (like taylor swift or today‚Äôs weather). This request includes:\n\nA URL (e.g., with parameters like ?q=San+Luis+Obispo)\nPossibly an API key\nA method (e.g., GET or POST)\n\nThe server then returns a response which contains:\n\ndata (temperature, artist name, forecast, etc.)\nmetadata (This is information about the response.)\nstatus code (Tells you whether the request was successful)\n\nThis information is traditionally provided in JSON Format.\n\n** Server Response  *************************\n\n\n\nLet‚Äôs focus on what the response is 1st (what we receive from the server):\nBelow is an example GIF of the information senr from the server in JSON format:\n\n\n\nGATO365 Anatomy JSON\n\n\nNote 4:\n\nWhen we send a request to an API, we get a response body, which includes the content ‚Äî typically JSON ‚Äî divided into data (what we wanted), metadata (info about the data), and a status_code telling us if the request worked.\n\n\n\n\nStatus codes tell you what happened with your request:\n-   `100s`: Info\n-   `200s`: Success (highlight: 200 OK)\n-   `300s`: Redirect\n-   `400s`: Client error\n-   `500s`: Server error\nNote 5:\n\nEmphasize: In most data APIs, your goal is to get a 200 response.\nUse examples like making up a nonexistent city or artist to show how an API might respond with a 400 or 404.\n\n\n** Client Request  *************************\n\n\n\nWhat type of client requests can we make?\nCRUD Framework (Create, Read, Update, Delete)\n\nThough APIs allow all four, Read (GET) is most common in data science.\nRESTful API mapping:\n\nCreate ‚Üí POST\nRead ‚Üí GET\nUpdate ‚Üí PUT/PATCH\nDelete ‚Üí DELETE\n\n\n(TODO: Create a GIF for each the above that is really illuminating, so GET and POST GIFs, its created, you have to actual turn it into a gif becuase it is actual a video)\n\n\n\nGATO365 Get Request\n\n\n\n\n\nGATO365 Get Request\n\n\nNote 4:\n\nWe‚Äôll focus mostly on GET, and occasionally show POST (e.g., retrieving personalized weather data).\nBriefly mention: Apps like Instagram or Facebook rely on all four CRUD operations‚Äîupdating posts, deleting comments, etc.\n\nP6. Setup API_Key [[Based on time do One of the three steps]]\n[[1. email attendees to go to the weather website and get API key or whatever information needed before the conference. Create a video that‚Äôs displaying how to do this]] [[1a. Discuss .Renviron.txt, how we use: to create and edit API key: usethis::edit_r_environ()]] [[1b. Use Sys.getenv(\"API_KEY\") to see API in console]] [[Note that you have to use the 1a to see the api key again]] Restart R\n[[2. have attendees get the key during the break session if they have not done so already]]\n[[3. use a common key, but tell them it is bad practice]]\n[[regardless of the decision made of the three options above have attendees store information in the environment file]]\nNote 5: there are many ways of doing this, but I‚Äôm going to stick with using tidyverse functions.I‚Äôm going to show you two ways to actually implement the query using the one way of one of the ways of doing this within a tiny verse using string glue\nP7: Requests, URLs & Queries P7. so what we‚Äôre going to first do is create our response and the most ideal way. - A request begins with a URL, which contains both:\n\nThe endpoint (base address of the API)\nThe query string (additional key-value pairs that modify the request)\nWe often need to glue strings together to build this full URL dynamically.\nA request is not ‚Äúautomatically‚Äù turned into JSON when sent ‚Äî it‚Äôs the response that‚Äôs usually formatted as JSON. The request is often URL-encoded if it‚Äôs a GET.\n\nP8: What Happens Under the Hood\n\nWhen we use a URL like ...?q=San+Luis+Obispo&appid=..., we‚Äôre constructing a query string, which is appended to the base URL.\nThink of this as ‚Äúasking the question‚Äù‚Äîthe query string shapes the request.\nThe server receives the request, processes it, and responds with structured data (typically JSON).\nWe‚Äôre not sending JSON in this case‚Äîwe‚Äôre sending a URL with parameters. JSON is returned to us as a response format.\n\n\n\n\n\n\n\nlibrary(httr2)       # Makes web requests\nlibrary(glue)        # Glue Strings\n\n\ncity_name &lt;- \"San Luis Obispo\"\n\n(TODO: place video into gif of URL anantomy)\n\ncurrent_weather_url &lt;- glue(\"https://api.openweathermap.org/data/2.5/weather?\",\n                            \"q=\", URLencode(city_name),\n                            \"&appid=\", Sys.getenv(\"API_KEY\"),\n                            \"&units=imperial\")\n\n\ncurrent_weather_url\n\nNote 6: explicily state what each element is\n\nreq &lt;- request(current_weather_url)\n\nBuilt Request Object\n\nreq\n\nNote 7:\n\nThis method shows the anatomy of the URL explicitly.\nGreat for emphasizing how query parameters are constructed using strings.\nHelps reinforce the idea of ‚Äúasking a question via the URL.‚Äù\n\nNote 8:\n\nWe are going to do it again in a diferent way but we are goint to process the response further here becuae\n\nI wanted you to undertstadn the anatomy of the URL\nHave multiple ways of doing the same thing\n\n\n\n\n\nStep 1: Build Request Object\n\nreq &lt;- request(\"https://api.openweathermap.org/data/2.5/weather\") |&gt; \n  req_url_query(\n    q = city_name,\n    appid = Sys.getenv(\"API_KEY\"),\n    units = \"imperial\"\n  )\n\n\nreq\n\nNote 9:\n\nThis method abstracts away the string building.\nIt‚Äôs cleaner and reduces chances of typos or formatting errors.\nTeaches students to treat query arguments like named inputs.\nYou can still inspect the built URL using req$url.\n\nStep 2: Make request\n\nresponse &lt;- req_perform(req)\n\n\nresponse\n\nNot a step: View content Type\n\ncontent_type &lt;- resp_content_type(response)\n\n\ncontent_type\n\n\nlibrary(dplyr)\n\n\n\n\n\n## IF the status code is 200 we are good\nif (resp_status(response) == 200) {\n  \n  # Parse JSON\n  result &lt;- resp_body_json(response)\n  \n  # Print Results as JSON\n  print(result)\n  \n  #---------------------------------------\n  \n  # Convert to Data Frame directly\n  current_weather_df &lt;- as.data.frame(result)\n  \n  # Print Results as Data Frame, using dplyr\n  print(select(current_weather_df, name, coord.lon, coord.lat, weather.main, main.temp))\n  \n  \n## ELSE state there is an Error\n} else {\n  cat(\"Failed. Status code:\", resp_status(response), \"\\n\")\n}\n\nNote 10: There are many other variable that is given but we focus on There is more information that is given but we are interested in the body of the request, hence we use resp_body_json that takes the body and that is what we are after as a json and we then convert it into a data frame\n\n\n\n\n\n(TODO: Remove certain element of functions that are needed to understand the function) (TODO: Break up functions 1 and 2, to be better compartmentalized)\n\n## Step 1: Define function \"get_city_coords\" that accepts the parameter \"city\"\nget_city_coords &lt;- function(city){\n  \n## Step 2: Create API request URL\n  \n  geo_url &lt;- glue(\n  \"http://api.openweathermap.org/geo/1.0/direct?\",\n  \"q=\", URLencode(city),\n  \"&limit=1&appid=\", Sys.getenv(\"API_KEY\")\n)\n ## Step 3: Use req_perform() and request() to call the API with the URL request \n\ngeo_response &lt;- req_perform(request(geo_url))\n  \n\n## Step 4: If the status code is 200 (OK), use resp_body_json() to parse our response and as.data.frame to coerce it to data.frame.\n\nif (resp_status(geo_response) == 200) {\n  geo_data_df &lt;- resp_body_json(geo_response) |&gt; \n    as.data.frame()\n  \n  \n  ## Step 5: Assess if the output has 0 length, meaning no result. If so, stop and display an error message.  \n  \n  if (length(geo_data_df) == 0) {\n    stop(\"City not found. Please check the city name.\")\n  }\n  \n  ## Step 6: Assign latitude and longitude to variables, and use round() to clip it down to 2 decimal places.\n  mod_1_geo_data_df &lt;- geo_data_df |&gt; \n    mutate(lat = round(lat,2),\n           lon = round(lon,2))\n\n  ## Step 7: Select Certain Columns (chaptgpt)\n  mod_2_geo_data_df &lt;- mod_1_geo_data_df |&gt; \n    select(country,name,lat,lon) |&gt; \n    rename(city = name)\n  \n  ## Step 8: Return data frame with the country, city name and latitude / longitude.  \n  return(mod_2_geo_data_df)\n  \n  \n   }\n}\n\nP. Lets try out this new function on the city\n\nget_city_coords(city_name)\n\nP. Lets Look at multtiple cities using the the map_df fucntion within the purrr package\n\nlibrary(purrr)\n\n# List of cities you want to geocode\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n\n# Use walk() from purrr to apply the function to each city\nmap_df(cities, get_city_coords)\n\n\n\n\n(TODO: Remove certain element of functions that are needed to understand the function) (highltight the function days in this)\n\nlibrary(lubridate)   # Time and date handling\n\nP. Emphasize the new endpoint (https://api.openweathermap.org/data/3.0/onecall/day_summary?) an how it is a paid subscription\n\nget_past_weather_by_city &lt;- function(city, days) {\n  # Step 1: Get city coordinates\n  coords_df &lt;- get_city_coords(city)\n  lat &lt;- coords_df$lat\n  lon &lt;- coords_df$lon\n  \n  cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n  \n  # Step 2: Create vector of past dates\n  date_range &lt;- as.character(today() - days(1:days))\n  \n  # Step 3: Define function for single date\n  fetch_day_summary &lt;- function(date) {\n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", date,\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\"\n    )\n    \n    response &lt;- req_perform(request(weather_url))\n    \n    if (resp_status(response) == 200) {\n      resp_body_json(response) |&gt; \n        as.data.frame() |&gt; \n        mutate(city = city, date = date)\n    } else {\n      warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(response)))\n      return(NULL)\n    }\n  }\n  \n  # Step 4: Map over date_range and bind into a single data frame\n  map_dfr(date_range, fetch_day_summary)\n}\n\n\nget_past_weather_by_city(city_name, 5)\n\nP. There are alot of ways of doing this, I decided not to use for loop in R\n\nnum_days &lt;- 5\n\n# Get historical weather data for each city using map_dfr\nall_weather_df &lt;- map_dfr(\n  cities,\n  ~ get_past_weather_by_city(.x, num_days)\n)\n\nNote: Emphasize error handling\n\n\n\n\nPurpose: Retrieves current weather conditions for a single city. Demonstrates basic GET usage and parsing a flat JSON structure.\n\nget_city_current_weather &lt;- function(city) {\n  url &lt;- glue::glue(\n    \"https://api.openweathermap.org/data/2.5/weather?\",\n    \"q=\", URLencode(city),\n    \"&appid=\", Sys.getenv(\"API_KEY\"),\n    \"&units=imperial\"\n  )\n  \n  response &lt;- request(url) |&gt; req_perform()\n  \n  if (resp_status(response) == 200) {\n    response |&gt; \n      resp_body_json() |&gt; \n      purrr::pluck(\"main\") |&gt; \n      tibble::as_tibble() |&gt; \n      dplyr::select(temp, humidity) |&gt; \n      dplyr::mutate(\n        city = city,\n        description = resp_body_json(response) |&gt; purrr::pluck(\"weather\", 1, \"description\")\n      ) |&gt; \n      dplyr::select(city, temp, humidity, description)\n  } else {\n    warning(\"Failed to retrieve current weather for \", city)\n    return(NULL)\n  }\n}\n\nP. Run for a Single City (e.g., ‚ÄúAtlanta‚Äù)\n\nget_city_current_weather(\"Atlanta\")\n\nP. Run for a Vector of Cities Using purrr::map_dfr()\n\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n\nweather_df &lt;- purrr::map_dfr(cities, get_city_current_weather)\n\n\n\n\n\nPurpose: Retrieves the next 5 days of forecast data (in 3-hour intervals). This introduces nested lists and flattening structures.\nNote: we are now getting times as well\n\nget_city_forecast_5day &lt;- function(city) {\n  url &lt;- glue::glue(\n    \"https://api.openweathermap.org/data/2.5/forecast?\",\n    \"q=\", URLencode(city),\n    \"&appid=\", Sys.getenv(\"API_KEY\"),\n    \"&units=imperial\"\n  )\n  \n  response &lt;- httr2::req_perform(httr2::request(url))\n  \n  if (httr2::resp_status(response) == 200) {\n  response |&gt; \n      resp_body_json() |&gt; \n      purrr::pluck(\"list\") |&gt; \n      purrr::map_dfr(\n        ~ tibble::tibble(\n            city = city,\n            timestamp = .x$dt_txt,\n            temp = .x$main$temp,\n            weather = .x$weather |&gt; purrr::pluck(1, \"description\")\n        )\n      )\n  } else {\n    warning(\"Failed to retrieve forecast for \", city)\n    return(NULL)\n  }\n}\n\nP. Try for city notice the number of rows and columns\n\nget_city_forecast_5day(\"Atlanta\")\n\nP. Lets Look at multiple cities\n\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n\nforecast_df &lt;- purrr::map_dfr(cities, get_city_forecast_5day)\n\n\n\n\n\nPurpose: Uses lat and lon to query current air quality. Demonstrates chaining of API requests (e.g., using get_city_coords() first), and different JSON structures.\n\nget_air_pollution_by_coords &lt;- function(lat, lon) {\n  url &lt;- glue::glue(\n    \"http://api.openweathermap.org/data/2.5/air_pollution?\",\n    \"lat=\", lat,\n    \"&lon=\", lon,\n    \"&appid=\", Sys.getenv(\"API_KEY\")\n  )\n  \n  response &lt;- request(url) |&gt; req_perform()\n  \n  if (resp_status(response) == 200) {\n    response |&gt;\n      resp_body_json() |&gt;\n      purrr::pluck(\"list\", 1) |&gt;\n      {\\(x) tibble::tibble(\n        aqi = x$main$aqi,\n        co = x$components$co,\n        pm2_5 = x$components$pm2_5,\n        pm10 = x$components$pm10\n      )}()\n  } else {\n    warning(\"Failed to retrieve air pollution data for lat = \", lat, \", lon = \", lon)\n    return(NULL)\n  }\n}\n\nP. Step-by-Step for Usage with a Data Frame, use get_city_coords to get the cities of lon and lat.\n\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\ncity_coords_df &lt;- map_dfr(cities, get_city_coords)\n\nP. Get Air Pollution Data for All Cities\n\nlibrary(tidyr)\n\n\npollution_df &lt;- city_coords_df |&gt; \n  mutate(\n    pollution = map2(lat, lon, get_air_pollution_by_coords)\n  ) |&gt; \n  unnest(pollution)"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html",
    "href": "outlines_and_organization/unorganized-outline.html",
    "title": "Unorganized Thoughts About Workshop",
    "section": "",
    "text": "Deliverables: R Quarto Document\n\n\n\n8:30 - 10:15 Session 1\n10:15 - 10:45 Snack 1\n10:45 - 12:30 - Session 2\n12:30 -1:30 Lunch\n1:30 - 2:30 - Session 3\n2:30 - 3:30 - Snack 2\n3:30 - 4:15 - Session 4"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html#general-timeline",
    "href": "outlines_and_organization/unorganized-outline.html#general-timeline",
    "title": "Unorganized Thoughts About Workshop",
    "section": "",
    "text": "8:30 - 10:15 Session 1\n10:15 - 10:45 Snack 1\n10:45 - 12:30 - Session 2\n12:30 -1:30 Lunch\n1:30 - 2:30 - Session 3\n2:30 - 3:30 - Snack 2\n3:30 - 4:15 - Session 4"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html#part-1-outcomes-after-workshop",
    "href": "outlines_and_organization/unorganized-outline.html#part-1-outcomes-after-workshop",
    "title": "Unorganized Thoughts About Workshop",
    "section": "Part 1: Outcomes after workshop",
    "text": "Part 1: Outcomes after workshop\n\nJoin Newsletter and become subsribers\nPatrons puchchase product\n\n\nSubscription model where they pay $25 a month to have access information\nSeminars where I teach how to teach the material\nA textbook/virtual book\nSeminars at conferences"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html#part-2-how-to-have-great-workshopnotes-for-great-scots-presentation",
    "href": "outlines_and_organization/unorganized-outline.html#part-2-how-to-have-great-workshopnotes-for-great-scots-presentation",
    "title": "Unorganized Thoughts About Workshop",
    "section": "Part 2: How to have great workshopNotes for great Scots presentation",
    "text": "Part 2: How to have great workshopNotes for great Scots presentation\n\nMake clear goals in the beginning**\nMake activities relate to goals\nInclude time for them to do actual work\nSchedule in break time for talking about\nI will have a list of where they are coming from\nSend them message about prep\nSend them information about lelve of content to do so before hand via email\nPrepare for No Shows"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html#part-3-misc",
    "href": "outlines_and_organization/unorganized-outline.html#part-3-misc",
    "title": "Unorganized Thoughts About Workshop",
    "section": "Part 3: Misc",
    "text": "Part 3: Misc\n\nGet API Access Token, use univerisal API, to make sure process goes by fast\nCost of API, need to make sure they know it possible to get more info but it costs\nStatus codes, teach them about the extent of status codes that will leave them with a foundation\nThe guts of the HTML complexity really is within the code to make the data clean whereas the guts of the API complexity is how the JSON is brought into R that requires deeper thought and intention around\nwebsite/coursekata will house everything, this github will be for me to organize everything\nProcess of viewers to client should be obvious and easy done\nMaterials: 4 Separate quarto files ‚Äì&gt; 4 separate jupyter notebook (potentiall 6, two additional notebooks for activity, split class to do API and others do html so they do not feel overwhelm, bring together and how them presnt their finding and how it can be imlplemnted in tjheir classes)\nHave questions, discussion questions, best practice within each quarto\nCombine strategies, i.e web scrape for ice cream sales, sports, other, and pull in weather API to join and look for patterns, Transformations and visualizations for each\nBring variety of candy\nHvae prizes for raffle\n\nHave this paradigm within each session - Goals & Objectives - Theory (I do not know what should be here for the introduction & conclusion) - Practice (what we do with API & HTML extraction) - (I do not know what to put here)"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#deliverable",
    "href": "outlines_and_organization/organized-outline.html#deliverable",
    "title": "Organized Workshop Outline",
    "section": "Deliverable",
    "text": "Deliverable\n\nFinal output: R Quarto Document(s) (4‚Äì6 files aligned with each session and additional activities)"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#general-timeline",
    "href": "outlines_and_organization/organized-outline.html#general-timeline",
    "title": "Organized Workshop Outline",
    "section": "General Timeline",
    "text": "General Timeline\n\n8:30 ‚Äì 10:15 Session 1: Introduction to Data Extraction\n10:15 ‚Äì 10:45 Snack Break 1\n10:45 ‚Äì 12:30 Session 2: APIs and Practice\n12:30 ‚Äì 1:30 Lunch\n1:30 ‚Äì 2:30 Session 3: Web Scraping with HTML\n2:30 ‚Äì 3:30 Snack Break 2\n3:30 ‚Äì 4:15 Session 4: Deep Dives into Complete Lessons"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#session-1-introduction-to-data-extraction",
    "href": "outlines_and_organization/organized-outline.html#session-1-introduction-to-data-extraction",
    "title": "Organized Workshop Outline",
    "section": "Session 1: Introduction to Data Extraction",
    "text": "Session 1: Introduction to Data Extraction\n\nSet expectations and workshop goals\nWhy data extraction matters: relevance to real-world education\nOverview of the layout / table of contents\nDiscuss libraries used (tidyverse, rvest, httr, etc.)\nBest practices (e.g., avoiding hardcoding, consistent comments)\nAdapting to changing APIs/websites\nAnecdote: Spotify example of lost API access\nExplain tidy data: snake_case column names, correct data types\nEmphasize code flexibility ‚Äî developers can change APIs overnight\nActivity: Scaffolding + Code review using example(s)\n\n\nGoals & Objectives\n\nIdentify the value of real-world data in statistics education\nDescribe the distinction between extraction, transformation, and visualization (ETv)\nRecognize challenges associated with pulling live data from the web\nApply tidy data principles to imported datasets\n\n\n\nConceptual Foundation\n\nWhy use live data?\nWhat is extraction and why it matters for teaching modern statistics\nETv framework: introduction to the first stage (Extraction)\nImportance of code flexibility and the fragility of external sources (e.g., Spotify anecdote)\nTidy data principles: naming conventions, structure, and data types\n\n\n\nHands-On Coding Activity\n\nExtracting from accessible sources such as:\n\nA static .csv hosted online (warm-up)\nA Wikipedia table using rvest and janitor\n\nIntroduce read_csv() and rvest::html_table()\nAdd cleaning steps to enforce tidy principles (snake_case, correct types)\n\n\n\nReflection\n\nHow can you introduce real-world messiness without overwhelming students?\nHow would you scaffold tidy principles at the intro-level?"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#session-2-api-fundamentals",
    "href": "outlines_and_organization/organized-outline.html#session-2-api-fundamentals",
    "title": "Organized Workshop Outline",
    "section": "Session 2: API Fundamentals",
    "text": "Session 2: API Fundamentals\n\nWhat is an API? Examples (Spotify, Weather, one bonus example)\nBasic API structure: request URLs, endpoints, tokens\nHTTP protocols and status codes\nCRUD operations (Create, Read, Update, Delete)\nAPI best practices (e.g., pagination, authentication, caching)\nTidyverse-friendly workflows (avoid deep nesting, use readable steps)\nActivity:\n\nModify API request (e.g., hometown weather)\nScaffolded practice (fill-in-the-blank)\nOptional take-home transformation/visualization\n\n\n\nGoals & Objectives\n\nExplain what an API is and how it supports data extraction\nMake requests to a public API and interpret the JSON response\nUnderstand and apply HTTP status codes and API keys\nWrite clean, readable code to extract and parse API data\n\n\n\nConceptual Foundation\n\nRESTful APIs: endpoints, parameters, keys\nAuthentication: tokens, secrets, and environment variables\nStatus codes and error handling (focus on 200, 401, 403, 404)\nJSON structure: nested data and tidy conversion\n\n\n\nHands-On Coding Activity\n\nWeather API (e.g., OpenWeatherMap):\n\nRetrieve current weather for participant‚Äôs hometown\nModify query parameters (e.g., units, location)\nParse and visualize simple results (e.g., temperature, humidity)\n\nScaffold activity: prewritten functions + one blank section\n\n\n\nReflection\n\nWhere could API data naturally integrate in your curriculum?\nWhat are the pitfalls (rate limits, authentication) students need to know?"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#session-3-html-scraping-in-r",
    "href": "outlines_and_organization/organized-outline.html#session-3-html-scraping-in-r",
    "title": "Organized Workshop Outline",
    "section": "Session 3: HTML Scraping in R",
    "text": "Session 3: HTML Scraping in R\n\nWhat is HTML and why it‚Äôs useful?\nExamples: Wikipedia, sports sites (NFL, Olympics)\nStructured vs unstructured web data\nReading the webpage source and locating tables/divs\nPractice: Going back and forth between R and browser to inspect structure\nTidy HTML scraping practices using rvest and janitor\nDifferent approaches:\n\nFull walkthrough\nPartial scaffold\n\nActivity:\n\nScrape 2 sources (in pairs), compare\nClean the data: name 3 needed transformations\nUse visualization and interpretation\nDiscuss hardcoding and fragile selectors\n\n\n\nGoals & Objectives\n\nIdentify basic HTML structure relevant for scraping\nScrape tables and text from structured web pages\nClean scraped data using tidyverse tools\nCompare different websites in terms of data accessibility\n\n\n\nConceptual Foundation\n\nHTML basics: tags, attributes, structure of web tables\nUsing rvest to read web pages and extract data\nThe importance of inspecting elements with browser tools\nStructured vs.¬†unstructured sites: Wikipedia vs.¬†ESPN\n\n\n\nHands-On Coding Activity\n\nScrape sports statistics from a reliable table:\n\nExample: Wikipedia table of Olympic medal counts or NBA season stats\nClean using janitor::clean_names()\nCompare scraped data from 2 sites (optional pair task)\n\n\n\n\nReflection\n\nHow could students use scraped data in a final project?\nWhat scaffolds would help students inspect and trust their source?"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#session-4-deep-dives-into-lessons",
    "href": "outlines_and_organization/organized-outline.html#session-4-deep-dives-into-lessons",
    "title": "Organized Workshop Outline",
    "section": "Session 4: Deep Dives into Lessons",
    "text": "Session 4: Deep Dives into Lessons\n\nWork through 2 complete lessons ‚Äî each with:\n\nAPI-based extraction and visualization\nHTML-based extraction and transformation\n\nSplit class into two groups: API vs HTML, then reconvene\nHighlight pedagogical framing: how this can be implemented in class\nBuild reflection and discussion time: What will you bring into your course?\n\n\nGoals & Objectives\n\nReview key takeaways from API and HTML extraction\nCollaborate with peers on a structured mini-project\nReflect on how to implement extraction in your own course\nShare classroom-ready ideas with other educators\n\n\n\nRecap (Conceptual Foundation)\n\nExtraction is not ‚Äújust tech‚Äù ‚Äî it‚Äôs pedagogy\nAPI vs.¬†HTML: strengths, limitations, educational value\nDesigning learning activities around messy data: student engagement, real-world relevance\n\n\n\nHands-On Coding Activity\n\nParticipants are randomly assigned:\n\nGroup A: Use an API (weather, Spotify, etc.)\nGroup B: Scrape HTML data (sports, Wikipedia, etc.)\n\nWork in small groups to clean, transform, and visualize\nPrepare a brief ‚Äúteaching demo‚Äù of how this could be used in class\n\n\n\nDiscussion & Reflection\n\nWhat worked in your group?\nWhat teaching goals does this type of project help support?\nHow would you modify it for your students‚Äô level and context?"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#supporting-infrastructure",
    "href": "outlines_and_organization/organized-outline.html#supporting-infrastructure",
    "title": "Organized Workshop Outline",
    "section": "Supporting Infrastructure",
    "text": "Supporting Infrastructure\n\nUse recent versions of all packages\nAll materials hosted on:\n\nCourseKata for workshop delivery\nGitHub repo for behind-the-scenes development organization\n\nEach session includes:\n\nCode chunk scaffolds (empty/fill-in versions)\nSolution files\nPedagogical notes and discussion questions\n\nOutput files:\n\n4‚Äì6 R Quarto files\nMatching Jupyter notebooks for hands-on use\n\nBonus activities:\n\nCombine scraping + API for multi-source projects (e.g., sports + weather + sales)"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#workshop-strategy-logistics",
    "href": "outlines_and_organization/organized-outline.html#workshop-strategy-logistics",
    "title": "Organized Workshop Outline",
    "section": "Workshop Strategy & Logistics",
    "text": "Workshop Strategy & Logistics\n\nOutcomes After Workshop\n\nInvite participants to join a newsletter and mailing list\nPromote:\n\nMonthly subscription model ($25/month) for materials\nTeaching-focused seminars\nCulturally relevant virtual textbook\nConference workshops\n\n\n\n\nPlanning Tips for Success\n\nMake goals explicit from the start\nAlign each activity with stated goals\nInclude participant work time and discussions\nSchedule breaks for casual conversation and social connection\nKnow your audience (collect pre-attendance data)\nSend prep info (software setup, expectations) in advance\nAnticipate and plan for no-shows\n\n\n\nAdditional Notes\n\nProvide API tokens ahead of time (Spotify key for all)\nExplain API rate limits and possible costs\nTeach foundational status codes\nClarify complexity differences:\n\nHTML: Cleaning/structure focus\nAPI: Parsing/logic focus (JSON)\n\nRaffle and candy: build fun and engagement\nEncourage pair programming and peer instruction\nAllow participants to present their work at the end"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#rationale-for-combinations",
    "href": "outlines_and_organization/organized-outline.html#rationale-for-combinations",
    "title": "Organized Workshop Outline",
    "section": "Rationale for Combinations",
    "text": "Rationale for Combinations\n\nSession 1 & General Thoughts: Merged all teaching philosophy related to data extraction fundamentals here.\nAPI Examples & API Best Practices: Combined into Session 2 for cohesion and clarity.\nHTML Examples & Cleaning Strategies: Combined into Session 3 for a unified focus on web scraping.\nSession 4 & Misc Deep Dives: Naturally fit as a concluding session to synthesize all techniques and apply in pedagogically rich lessons.\nOutcomes, Planning, and Misc Strategy: Organized into coherent post-workshop and infrastructure support categories.\n\nLet me know if you want me to turn this into a .qmd, Google Doc, or a GitHub README.md."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html",
    "href": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html",
    "title": "Session 3: Extraction of NFL Data - HTML Scraping",
    "section": "",
    "text": "Identify basic HTML structure relevant for scraping\nScrape tables and text from structured web pages\nClean scraped data using tidyverse tools\nCompare different websites in terms of data accessibility\n\n\n\n\n\nHTML basics: tags, attributes, structure of web tables\nUsing rvest to read web pages and extract data\nThe importance of inspecting elements with browser tools\nStructured vs.¬†unstructured sites: Wikipedia vs.¬†ESPN\n\n\n\n\nlibrary(rvest)      # Web scraping\nlibrary(dplyr)      # Data manipulation\nlibrary(stringr)    # String cleaning\nlibrary(rlang)      # Advanced evaluation\nlibrary(purrr)      # Functional tools\nlibrary(ggplot2)    # Visualizations\n\nDiscussion: Why are some of these packages (like purrr or rlang) useful for scraping tasks?\n\n\n\n\nWe start by creating the target URL for a given team and year.\n\n# Step 2: Define team and year\nteam_name &lt;- \"was\"\nyear &lt;- 2023\n\n# Step 2a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n\nDiscussion: How could we make this part of a function so it is reusable?\nSee Also: https://www.pro-football-reference.com/teams/was/2023.htm#games\n\n\n\n\n\n# Step 3: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n\nWhen you run read_html(url), it returns an HTML node pointer, not human-readable content.\nThis pointer references the structure of the web page in memory, but doesn‚Äôt display actual text or data.\nIf you try to print this object directly, you‚Äôll see something such as:\nwebpage[[1]] &lt;pointer: 0x00000225...&gt;\n\n\n\nR is showing memory addresses of HTML elements, not the content.\nThis is because the HTML content must still be parsed or extracted.\n\nUse rvest!\n\nhtml_table() : extracts data from &lt;table&gt; elements.\nhtml_text() : extracts plain text from HTML nodes.\nhtml_nodes() or html_elements() : selects multiple nodes using CSS or XPath.\nhtml_element() : selects a single node.\n\nDiscussion: Why do you think web scraping tools separate ‚Äústructure‚Äù from ‚Äúcontent‚Äù? What are the pros and cons of working with HTML nodes directly?\nFrom the webpage, grab the HTML tables using rvest::html_table().\n\n# Step 3a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\nThe result is a list containing HTML table elements.\nDiscussion: What does this data structure look like?\nSelect the desired table, the 2023 Regular Season Table, which is the second table on the webpage. Use purrr::pluck() to select the table.\n\n\n\nThis is the table we are after\n\n\n\n# Step 3b: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n\nDiscussion: Why might this index (2) break in the future? What alternatives could we use to select the correct table more reliably?\n\n\n\n\nOur first row contains more information regarding the columns than the header of the actual table. The merged cells in the header end up being repeated over the entire column group they represent, without providing useful information.\n\n# Step 4a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\nDiscussion: Why can‚Äôt we use dplyr slice()?\n\n# Step 4b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n\n# Step 4c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n\n# Step 4d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\nBecause these columns are neither labeled in the first row or the header, we must manually assign them names.\n\n\n\nNotice the unlabeled columns?\n\n\n\n# Step 4e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n\nDiscussion: What are the risks or tradeoffs in hardcoding columns like result and game_location? How could this break?\n\n\n\n\nHere we will use dplyr select and filter to drop columns that are not relevant, as well as the Bye Week where the team does not play a game.\n\n# Step 5: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n\n# Step 5a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n\n# Step 5b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n\n# Step 5c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\nDiscussion: Why convert categorical variables like score_rslt or game_location to factors? What impact could that have on modeling or plotting?\n\n\n\n\nBy putting it all together, we can input a year for the Washington Commanders and get an extracted and cleaned table out.\n\n# Step 6: Year-only function\nwas_year &lt;- function(year) {\n  # Step 1: Define team and year\nteam_name &lt;- \"was\"\n\n# Step 1a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n  \n # Step 2: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n# Step 2a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\n# Step 3: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n # Step 3a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\n# Step 3b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n# Step 3c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n# Step 3d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\n# Step 3e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n# Step 4: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n# Step 4a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n# Step 4b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n# Step 4c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\n  return(table_7)\n}\n\nTest Year Only Function\n\nhead(was_year(2022))\n\n\n\n\nNow we will do the same task but while supplying team_name as a parameter as well as year.\n\n# Step 7: Generalized function\nfn_team_year &lt;- function(team_name, year) {\n\n# Step 2a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n  \n # Step 3: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n# Step 3a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\n# Step 3b: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n # Step 4a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\n# Step 4b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n# Step 4c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n# Step 4d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\n# Step 4e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n# Step 5: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n# Step 5a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n# Step 5b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n# Step 5c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\n  return(table_7)\n}\n\nTest Function (Team + Year)\n\nhead(fn_team_year(\"sfo\", 2024))\n\n\n\n\n\nUse ggplot2 to create simple and insightful visualizations.\n\n# Step 8: Line plot of points scored by Week\nggplot(fn_team_year(\"sfo\", 2024), aes(x = week, y = tm)) +\n  geom_line(color = \"steelblue\", linewidth = 1.2) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Points Scored Over Time\",\n    x = \"Week\",\n    y = \"Points Scored\"\n  ) +\n  theme_minimal()\n\n\n# Step 8a: Compare performance by game location\nggplot(fn_team_year(\"sfo\", 2024), aes(x = game_location, y = tm, fill = game_location)) +\n  geom_boxplot() +\n  labs(\n    title = \"Points Scored: Home vs Away\",\n    x = \"Location\",\n    y = \"Points Scored\"\n  ) +\n  theme_minimal()\n\nDiscussion: How might you visualize win/loss trends over the season? Could you include opponent information or passing yards?\nNow that you‚Äôre familiar with HTML elements and scraping, this activity will walk through extracting, cleaning, and visualizing NFL team performance data.\n\n\n\n\n\n\nScrape sports statistics from a reliable table:\n\nExample: Wikipedia table of Olympic medal counts or NBA season stats\nClean using janitor::clean_names()\nCompare scraped data from 2 sites (optional pair task)\n\n\n\n\n\n\nHow could students use scraped data in a final project?\nWhat scaffolds would help students inspect and trust their source?\n\n\n\n\n\nWhat is HTML and why it‚Äôs useful?\nExamples: Wikipedia, sports sites (NFL, Olympics)\nStructured vs unstructured web data\nReading the webpage source and locating tables/divs\nPractice: Going back and forth between R and browser to inspect structure\nTidy HTML scraping practices using rvest and janitor\nDifferent approaches:\n\nFull walkthrough\nPartial scaffold\n\nActivity:\n\nScrape 2 sources (in pairs), compare\nClean the data: name 3 needed transformations\nUse visualization and interpretation\nDiscuss hardcoding and fragile selectors"
  },
  {
    "objectID": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html#session-3-html-web-scraping---nfl-data-extraction",
    "href": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html#session-3-html-web-scraping---nfl-data-extraction",
    "title": "Session 3: Extraction of NFL Data - HTML Scraping",
    "section": "",
    "text": "Identify basic HTML structure relevant for scraping\nScrape tables and text from structured web pages\nClean scraped data using tidyverse tools\nCompare different websites in terms of data accessibility\n\n\n\n\n\nHTML basics: tags, attributes, structure of web tables\nUsing rvest to read web pages and extract data\nThe importance of inspecting elements with browser tools\nStructured vs.¬†unstructured sites: Wikipedia vs.¬†ESPN\n\n\n\n\nlibrary(rvest)      # Web scraping\nlibrary(dplyr)      # Data manipulation\nlibrary(stringr)    # String cleaning\nlibrary(rlang)      # Advanced evaluation\nlibrary(purrr)      # Functional tools\nlibrary(ggplot2)    # Visualizations\n\nDiscussion: Why are some of these packages (like purrr or rlang) useful for scraping tasks?\n\n\n\n\nWe start by creating the target URL for a given team and year.\n\n# Step 2: Define team and year\nteam_name &lt;- \"was\"\nyear &lt;- 2023\n\n# Step 2a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n\nDiscussion: How could we make this part of a function so it is reusable?\nSee Also: https://www.pro-football-reference.com/teams/was/2023.htm#games\n\n\n\n\n\n# Step 3: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n\nWhen you run read_html(url), it returns an HTML node pointer, not human-readable content.\nThis pointer references the structure of the web page in memory, but doesn‚Äôt display actual text or data.\nIf you try to print this object directly, you‚Äôll see something such as:\nwebpage[[1]] &lt;pointer: 0x00000225...&gt;\n\n\n\nR is showing memory addresses of HTML elements, not the content.\nThis is because the HTML content must still be parsed or extracted.\n\nUse rvest!\n\nhtml_table() : extracts data from &lt;table&gt; elements.\nhtml_text() : extracts plain text from HTML nodes.\nhtml_nodes() or html_elements() : selects multiple nodes using CSS or XPath.\nhtml_element() : selects a single node.\n\nDiscussion: Why do you think web scraping tools separate ‚Äústructure‚Äù from ‚Äúcontent‚Äù? What are the pros and cons of working with HTML nodes directly?\nFrom the webpage, grab the HTML tables using rvest::html_table().\n\n# Step 3a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\nThe result is a list containing HTML table elements.\nDiscussion: What does this data structure look like?\nSelect the desired table, the 2023 Regular Season Table, which is the second table on the webpage. Use purrr::pluck() to select the table.\n\n\n\nThis is the table we are after\n\n\n\n# Step 3b: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n\nDiscussion: Why might this index (2) break in the future? What alternatives could we use to select the correct table more reliably?\n\n\n\n\nOur first row contains more information regarding the columns than the header of the actual table. The merged cells in the header end up being repeated over the entire column group they represent, without providing useful information.\n\n# Step 4a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\nDiscussion: Why can‚Äôt we use dplyr slice()?\n\n# Step 4b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n\n# Step 4c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n\n# Step 4d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\nBecause these columns are neither labeled in the first row or the header, we must manually assign them names.\n\n\n\nNotice the unlabeled columns?\n\n\n\n# Step 4e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n\nDiscussion: What are the risks or tradeoffs in hardcoding columns like result and game_location? How could this break?\n\n\n\n\nHere we will use dplyr select and filter to drop columns that are not relevant, as well as the Bye Week where the team does not play a game.\n\n# Step 5: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n\n# Step 5a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n\n# Step 5b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n\n# Step 5c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\nDiscussion: Why convert categorical variables like score_rslt or game_location to factors? What impact could that have on modeling or plotting?\n\n\n\n\nBy putting it all together, we can input a year for the Washington Commanders and get an extracted and cleaned table out.\n\n# Step 6: Year-only function\nwas_year &lt;- function(year) {\n  # Step 1: Define team and year\nteam_name &lt;- \"was\"\n\n# Step 1a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n  \n # Step 2: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n# Step 2a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\n# Step 3: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n # Step 3a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\n# Step 3b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n# Step 3c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n# Step 3d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\n# Step 3e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n# Step 4: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n# Step 4a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n# Step 4b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n# Step 4c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\n  return(table_7)\n}\n\nTest Year Only Function\n\nhead(was_year(2022))\n\n\n\n\nNow we will do the same task but while supplying team_name as a parameter as well as year.\n\n# Step 7: Generalized function\nfn_team_year &lt;- function(team_name, year) {\n\n# Step 2a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n  \n # Step 3: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n# Step 3a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\n# Step 3b: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n # Step 4a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\n# Step 4b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n# Step 4c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n# Step 4d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\n# Step 4e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n# Step 5: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n# Step 5a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n# Step 5b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n# Step 5c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\n  return(table_7)\n}\n\nTest Function (Team + Year)\n\nhead(fn_team_year(\"sfo\", 2024))\n\n\n\n\n\nUse ggplot2 to create simple and insightful visualizations.\n\n# Step 8: Line plot of points scored by Week\nggplot(fn_team_year(\"sfo\", 2024), aes(x = week, y = tm)) +\n  geom_line(color = \"steelblue\", linewidth = 1.2) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Points Scored Over Time\",\n    x = \"Week\",\n    y = \"Points Scored\"\n  ) +\n  theme_minimal()\n\n\n# Step 8a: Compare performance by game location\nggplot(fn_team_year(\"sfo\", 2024), aes(x = game_location, y = tm, fill = game_location)) +\n  geom_boxplot() +\n  labs(\n    title = \"Points Scored: Home vs Away\",\n    x = \"Location\",\n    y = \"Points Scored\"\n  ) +\n  theme_minimal()\n\nDiscussion: How might you visualize win/loss trends over the season? Could you include opponent information or passing yards?\nNow that you‚Äôre familiar with HTML elements and scraping, this activity will walk through extracting, cleaning, and visualizing NFL team performance data.\n\n\n\n\n\n\nScrape sports statistics from a reliable table:\n\nExample: Wikipedia table of Olympic medal counts or NBA season stats\nClean using janitor::clean_names()\nCompare scraped data from 2 sites (optional pair task)\n\n\n\n\n\n\nHow could students use scraped data in a final project?\nWhat scaffolds would help students inspect and trust their source?\n\n\n\n\n\nWhat is HTML and why it‚Äôs useful?\nExamples: Wikipedia, sports sites (NFL, Olympics)\nStructured vs unstructured web data\nReading the webpage source and locating tables/divs\nPractice: Going back and forth between R and browser to inspect structure\nTidy HTML scraping practices using rvest and janitor\nDifferent approaches:\n\nFull walkthrough\nPartial scaffold\n\nActivity:\n\nScrape 2 sources (in pairs), compare\nClean the data: name 3 needed transformations\nUse visualization and interpretation\nDiscuss hardcoding and fragile selectors"
  },
  {
    "objectID": "sessions/session_1/01_Extraction_Introduction.html",
    "href": "sessions/session_1/01_Extraction_Introduction.html",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "",
    "text": "CourseKata\norganization of workshop of lecture and questions throughout\n\n\n\n\n\nUnderstand the importance of extracting dynamic data (via HTML and APIs) in modern data analysis and teaching\nLearn how to access and work with APIs to retrieve structured, real-time data\nLearn how to bring HTML-based data (e.g., web tables) into R using scraping tools\nApply these skills through hands-on coding practice and discussion of classroom integration\n\n\n\n(need to place) (Mention Querying data base more as an action) ### 2. Conceptual Foundation\n\nP1. My mentor, Allan, says ask good questions‚Ä¶\nP2. Statistical Question: Who had the most impactful first season in terms of points: Michael Jordan, LeBron James, or Kobe Bryant?\nP3. I recently submitted a manuscript on this exact dataset, so let‚Äôs use it as our starting point.\nP4. We‚Äôll begin by working with a static Excel file that contains per-game stats for each player‚Äôs 15 seasons in the NBA.\nP5. Let‚Äôs Load the pertinent libraries\nT1. ____ Fill in the code (chatgpt)\n\n\nlibrary(readxl)      ## Data Extraction      --- E\nlibrary(dplyr)       ## Data Transformation  --- T\nlibrary(ggplot2)     ## Data Visualization   --- V\n\n\nP6. Load in the data\nT2. ____ Fill in the code (chatgpt)\n\n\nnba_df &lt;- read_xlsx(\"nba_data.xlsx\", sheet = \"modern_nba_legends_08302019\")\n\n\nP7. Let‚Äôs view the data ‚Ä¶\nT3. Use the glimpse function to view the data\n\n\nglimpse(nba_df)\n\nRows: 3,400\nColumns: 38\n$ Name          &lt;chr&gt; \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"M‚Ä¶\n$ Season        &lt;chr&gt; \"season_1\", \"season_1\", \"season_1\", \"season_1\", \"season_‚Ä¶\n$ Game_Location &lt;chr&gt; \"Home\", \"Away\", \"Home\", \"Away\", \"Away\", \"Away\", \"Away\", ‚Ä¶\n$ Game_Outcome  &lt;chr&gt; \"W\", \"L\", \"W\", \"W\", \"L\", \"W\", \"W\", \"W\", \"W\", \"L\", \"L\", \"‚Ä¶\n$ Point_Margin  &lt;dbl&gt; 16, -2, 6, 5, -16, 4, 15, 2, 3, -20, -9, -17, -10, 19, -‚Ä¶\n$ Rk            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1‚Ä¶\n$ G             &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1‚Ä¶\n$ Date          &lt;dttm&gt; 1984-10-26, 1984-10-27, 1984-10-29, 1984-10-30, 1984-11‚Ä¶\n$ Age           &lt;chr&gt; \"21-252\", \"21-253\", \"21-255\", \"21-256\", \"21-258\", \"21-26‚Ä¶\n$ Tm            &lt;chr&gt; \"CHI\", \"CHI\", \"CHI\", \"CHI\", \"CHI\", \"CHI\", \"CHI\", \"CHI\", ‚Ä¶\n$ Opp           &lt;chr&gt; \"WSB\", \"MIL\", \"MIL\", \"KCK\", \"DEN\", \"DET\", \"NYK\", \"IND\", ‚Ä¶\n$ GS            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ MP            &lt;dbl&gt; 40, 34, 34, 36, 33, 27, 33, 42, 43, 33, 44, 39, 42, 30, ‚Ä¶\n$ FG            &lt;dbl&gt; 5, 8, 13, 8, 7, 9, 15, 9, 18, 12, 4, 11, 11, 9, 10, 6, 9‚Ä¶\n$ FGA           &lt;dbl&gt; 16, 13, 24, 21, 15, 19, 22, 22, 27, 24, 17, 26, 22, 13, ‚Ä¶\n$ FG_Percent    &lt;dbl&gt; 0.313, 0.615, 0.542, 0.381, 0.467, 0.474, 0.682, 0.409, ‚Ä¶\n$ `3P`          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,‚Ä¶\n$ `3PA`         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 3, 0, 0, 1, 0, 1, 0, 0,‚Ä¶\n$ `3P_Percent`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 1, 0, NA, 0, NA, NA, 0, ‚Ä¶\n$ FT            &lt;dbl&gt; 6, 5, 11, 9, 3, 7, 3, 9, 8, 3, 8, 12, 13, 5, 10, 1, 3, 2‚Ä¶\n$ FTA           &lt;dbl&gt; 7, 5, 13, 9, 4, 9, 4, 12, 11, 3, 8, 16, 14, 6, 10, 1, 4,‚Ä¶\n$ FT_Percent    &lt;dbl&gt; 0.857, 1.000, 0.846, 1.000, 0.750, 0.778, 0.750, 0.750, ‚Ä¶\n$ ORB           &lt;dbl&gt; 1, 3, 2, 2, 3, 1, 4, 2, 2, 0, 0, 2, 4, 0, 3, 0, 1, 2, 2,‚Ä¶\n$ DRB           &lt;dbl&gt; 5, 2, 2, 2, 2, 3, 4, 7, 8, 2, 5, 3, 9, 4, 3, 2, 2, 3, 0,‚Ä¶\n$ TRB           &lt;dbl&gt; 6, 5, 4, 4, 5, 4, 8, 9, 10, 2, 5, 5, 13, 4, 6, 2, 3, 5, ‚Ä¶\n$ AST           &lt;dbl&gt; 7, 5, 5, 5, 5, 3, 5, 4, 4, 2, 7, 2, 2, 3, 8, 3, 2, 5, 3,‚Ä¶\n$ STL           &lt;dbl&gt; 2, 2, 6, 3, 1, 3, 3, 2, 3, 2, 5, 2, 2, 4, 3, 3, 2, 3, 1,‚Ä¶\n$ BLK           &lt;dbl&gt; 4, 1, 2, 1, 1, 1, 2, 5, 2, 1, 2, 1, 2, 1, 1, 2, 0, 0, 1,‚Ä¶\n$ TOV           &lt;dbl&gt; 5, 3, 3, 6, 2, 5, 5, 3, 4, 1, 4, 3, 6, 4, 4, 4, 2, 4, 4,‚Ä¶\n$ PF            &lt;dbl&gt; 2, 4, 4, 5, 4, 5, 2, 4, 4, 4, 5, 3, 3, 4, 4, 1, 5, 4, 3,‚Ä¶\n$ PTS           &lt;dbl&gt; 16, 21, 37, 25, 17, 25, 33, 27, 45, 27, 16, 34, 35, 23, ‚Ä¶\n$ GmSc          &lt;dbl&gt; 12.5, 19.4, 32.9, 14.7, 13.2, 14.9, 29.3, 21.2, 37.5, 17‚Ä¶\n$ number_game   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1‚Ä¶\n$ DD            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ TD            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ Age_Years     &lt;dbl&gt; 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, ‚Ä¶\n$ Age_Days      &lt;dbl&gt; 252, 253, 255, 256, 258, 264, 265, 267, 270, 272, 274, 2‚Ä¶\n$ Last_Name     &lt;chr&gt; \"Jordan\", \"Jordan\", \"Jordan\", \"Jordan\", \"Jordan\", \"Jorda‚Ä¶\n\n\n\nQ1. Look through the data, does it look clean? Discuss amongst Your peers. (chatgpt)\nAns Q1: It is clean the numeric variables are supposed to be numeric and the characrets variables are treated as char (chatgpt)\nP8. Now let‚Äôs clean the focus on the data frame that we are after\nT4. ____ Fill in the code (chatgpt)\n\n\nseason_1_df &lt;- nba_df %&gt;% \n  filter(Season == \"season_1\")\n\n\nP9. Now lets look at a plot of their points (chatgpt)\n\nT5. ____ Fill in the code (chatgpt)\n\nseason_1_df %&gt;% \n  ggplot(aes(x = Name, y = PTS)) +\n  geom_boxplot() +\n  theme_bw() \n\nWarning: Removed 14 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nNote 1: We could have spruced it up but here we just wanted to answer the question, if you have the urge please do so.\nQ2. What conclusion could be made (chatgpt)\n\nP10. Now what about, Magic Johnson or Wilt Chamberlain Maybe Luka Donƒçiƒá or Ja Morant If I wanted to add this data I need to go to the originnal source not an excel sheet to do this (chatgpt)\nNote 2: - Shift students from being passive data users to active data seekers - Move beyond the idea of ‚Äúwaiting for clean data‚Äù to learning how to access, validate, and clean it themselves - Teach both the skill to extract and the capacity to teach extraction\nNote 3: - Why this matters: We as instructors should not just rely on pre-built packages or static datasets. The digital world changes constantly ‚Äî websites, APIs, and file structures evolve.\n\nOur responsibility: Teach students (and ourselves) how to adapt and access real-world data sources. Equip learners with skills to extract, not just consume pre-extracted content.\nDespite the growing importance of live data, most introductory courses still rely heavily on static, pre-cleaned datasets. This limits students‚Äô exposure to the realities of modern data work.\nKey idea: Flat files can still be dynamic depending on how they‚Äôre maintained ‚Äî but we use the term ‚Äúdynamic‚Äù here to emphasize external, real-time data access through APIs and web scraping.\nThe availability of dynamic, frequently updated data ‚Äî especially via web APIs ‚Äî has grown exponentially in recent years. This shift demands new strategies in how we teach data access.\n\n\n\n\n\nP11.\nExamples: CSV, Excel files\nTypically unchanging unless manually edited\nOften pre-loaded into classroom activities\nMay still require cleaning (e.g., column names, missing data)\nNote 4: Messy data is not always a bad thing\n\n\n\n\n\nP12.\nDefinition: Data sources that update over time or are externally controlled (i.e., you don‚Äôt own the source)\nP13.\nTwo primary types:\n\nApplication Programming Interface APIs ‚Äì Designed to serve structured data upon request (e.g., player stats, weather)\nHypertext Markup Language HTML/Web Pages ‚Äì Seen as dynamic when content changes (especially sports, news, etc.)\n\nHTML pages are primarily designed for human readability, while APIs are designed for structured machine access. Both offer pathways to dynamic data, each with different advantages and challenges.\nBridging the gap between classroom exercises and real-world data practice requires that students learn not just how to analyze data ‚Äî but how to find it, extract it, and prepare it themselves.\nNote 5: HTML can be treated as static or dynamic depending on how frequently the page updates. For this workshop, we treat HTML as dynamic, especially for sports data.\n\n\n\n\nNote 6:\n\nThere are many kinds of APIs, but in this workshop, we‚Äôll focus specifically on web APIs ‚Äî tools designed to let us request and retrieve data from online sources.\nIn R, we‚Äôll act like a piece of software making those requests, allowing us to access live data programmatically.\n\n(Mention Querying data base more as an action)\n\n\n\nFlow of Data via API (vrogue.co)\n\n\n\nP14.\nAPI stand for Application Programming Interfaces\nIt is a way for software to communicate with one another\nOne way it work is that it allow programs to request data directly from external servers in a structured format (most often JSON).\n\n(Mention Querying data base more as an action)\n{\n  \"player\": \"LeBron James\",\n  \"points\": 27.1,\n  \"team\": \"Lakers\"\n}\n\nThe keys are players, pointsand team\nThe values for the corresponding keys are LeBron James, 27.1, Lakers\n\nNote 7:\n\nThere are a lot of acronyms\nJSON - Java Script Object Notation - javascript is web developing software (chatgpt)\n\n(Mention Querying data base more as an action)\nQ3. What does JSON? Answer: Java script object notation\nNote 8:\n\nDescribe Image Flow of Data via API (vrogue.co)\nA user sends a request via the internet ‚Üí the API talks to the server ‚Üí the server queries the database ‚Üí the API responds with data, often as JSON. (Mention Querying data base more as an action)\nP15.\nLearning to work with web APIs teaches students more than just how to extract data ‚Äî it gives them the tools to:\n\nLocate relevant APIs (e.g., weather, sports, music)\nConstruct and test their own API requests\nInterpret JSON responses (including nested structures)\nTransform the results into tidy formats ready for analysis\n\n\n(Mention Querying data base more as an action)\n\nP16\nAPIs aren‚Äôt just technical tools, they‚Äôre increasingly the primary way to access and query data stored in external databases.\nIn today‚Äôs fast-changing digital environment, students must be equipped to retrieve and work with information from live, external sources, not just rely on pre-cleaned datasets.\nNote 9:\n\nThis is what pushes students from passive observers of data to active agents in its collection, structure, and use. It aligns closely with what real-world data science jobs require, especially when you‚Äôre no longer just analyzing data, but acquiring it.\n\n\n\nVolume of Data Created (Statista)\n\n\nThe use of APIs requires keys, which are unique and secret codes that are used to authorize your request and identify your user and billing information. Consequently, keeping these codes secret is imperative. To do so, store API keys in environment files which reside on your computer, and not coded into variables or available in plain text on your working files.\nshow images creating an app (provide my youtube video) - get id and secret (blurr it out) (make highlighe secret and ID on image )\n[\nid: (eman post in some way) secret: (eman post on screen)\nNote 10: Note all APIs website work the same, getting experience with 1-2, you start to develop a philosophy on how to approach them (chatgpt)\nCode 2\nStep 1: Load the Required Packages\n\nlibrary(spotifyr)\nlibrary(dplyr)\n\n\nStep 2: Set Up Credentials\nBefore using the Spotify API, you need to create an app on the and retrieve your Client ID and Client Secret. (I have a video here to do this on your own but on the screen, you can see the id and secret - (chatgpt))How to make app\n\n# Replace these with your actual credentials (do NOT share them publicly)\nclient_id &lt;- 'your_spotify_client_id_here'\nclient_secret &lt;- 'your_spotify_client_secret_here'\n\n\nStep 3: Get an Access Token\nThe token allows Spotify to recognize both who you are and what you‚Äôre requesting.\n\naccess_token &lt;- get_spotify_access_token(\n  client_id = client_id,\n  client_secret = client_secret\n)\n\n\nStep 4: Request Artist Data\nNow that you‚Äôre authenticated, you can request track-level data from Spotify‚Äôs API.\n\nbeatles_df &lt;- get_artist_audio_features('the beatles')\n\n\nStep 5: Explore the Dataset\nUse glimpse() to quickly examine the structure of the data.\n\nglimpse(beatles_df)\n\n\nStep 6: Try Another Artist\nTry retrieving audio features for a different artist.\n\ntaylor_df &lt;- get_artist_audio_features('taylor swift')\nglimpse(taylor_df)\n\nNote 11: How databases and access can change in that the developers may change access or change the way you access information so you have to be aware of this. (chatgpt)\nData types are usually sotred correctly as chars or numerics so noy much cleaning required (chatgpt)\n\n\n\n\n\n\n\nProcess of HTML Scraping (sstm2.com)\n\n\n\nP17\n\nWebsites are designed using Hypertext Markup Language (HTML) to display information, (chatgpt: say better)\nInformation stored as tables are ideal within a HTML page (chatgpt: say more)\nNote 12: - we do not want to just copy and paste a table into a csv format it then clearn it up again in R, ideally we want to have access to and bring it into R (chatgpt: say better)\n\nP18:\n\nBelow is an image of code for html table and the actual table that it would produce\n{width=‚Äú200‚Äù, height = ‚Äú1000‚Äù}\n\nNote 13:\n\nHighlight the following concepts:\n\nbeginning and the end of table\nthe column name\neach Row\nHow it translate into a human readable table\n\nEmphasize that we‚Äôre only focusing on &lt;table&gt; tags for this workshop\n\nP19.\n\nNow lets see one of the libraries that allows us to scrape in R\n\nlibrary(htmltab)\n\nNote 14: Lets go to the url via webbroswer\n\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n\nP20.\nThis function requires 2 args url and table number we can guess at it and may work\n\niowa_state_counties_df &lt;- htmltab(url,1)\n\n\niowa_state_counties_df &lt;- htmltab(url,2)\n\nNote 15:\n\nUnless you know html and want to look at the source code or you what exactly a table looks like you will have to guess sometimes\nWe can get the warning to go away by ‚Ä¶\nP21.\n\nThis is what I would call static because the counties are note changeing but if we wanted baseball data at which currently everyday new data is displayed it is ideal that we have a more robust method of fgetting this data rather using htmltabs\n\nP22.\n\nCheck out article: Web Scraping in the Statistics and Data Science Curriculum: Challenges and Opportunities - Mine Dogucu & Mine √áetinkaya-Rundel\n\nP23.\n\nMuch like APIs, lots of relevant and useful information is available directly on webpages, which are readable by humans rather than APIs which are designed for machine access.\nBy learning this skill, students are able to:\n\nLocate relevant sources (e.g., sports data from Pro Football Reference)\nUnderstand how websites deliver and organize content\nTransform and clean data for analysis and visualization\n\nOften times, HTML tables contain unexpected structures or data types (images, links, etc) and can present a challenge that develops not only data cleaning skills, but intention, planning, and adaptability when handling and analyzing difficult data.\n\n\n\n\n\nSession 1: Introduction\nThis session serves as a discussion of the principles and reasoning behind learning these concepts and how they can benefit the classroom.\nSession 2: Getting Weather Data via OpenWeather API\nIn this session, we dive into OpenWeather API and learn to use packages like httr2 to execute API calls. We will also discuss URLs, queries, data structures, and more.\nSession 3: Scraping NFL Sports Data\nIn this session, we will use Pro-Football Reference to learn how to extract and clean HTML table data for use in statistical analysis and visualizations.\nSession 4: Putting it All Together (Project)\nIn this project, we will use HTML scraping joined with the OpenWeather API to create our own cloropleth map of Iowa.\n\n\n\nChloropleth Map (geoapify.com)\n\n\n\n\n\n\n\nGoal: Engage participants in applying both API and HTML extraction methods.\nPart A: API\n\nUse a new function from the spotifyr package\nCreate a simple plot using the data\nAsk comprehension questions:\n\nHow does this differ from static file use?\nWhat‚Äôs confusing about working with API responses?\n\n\nPart B: HTML\n\nExtract state-specific data from a chosen Wikipedia page\nGuide participants through cleaning it (if time allows)\nAsk guiding questions:\n\nDid the table number match what you expected?\nWhat challenges did you face?\n(Example prompt written on the board: ‚ÄúNot every state‚Äôs table is the same.‚Äù)\n\n\n\n\n\n\nWhat did we learn?\nHow does this connect to the original Goals & Objectives of the session?\nHow do you see yourself using this in your classroom?\nWhat kinds of APIs or HTML sources would be most relevant for your students?\n\n\n\n\n\nSet expectations and workshop goals\nWhy data extraction matters: relevance to real-world education\nOverview of the layout / table of contents\nDiscuss libraries used (tidyverse, rvest, httr, etc.)\nBest practices (e.g., avoiding hardcoding, consistent comments)\nAdapting to changing APIs/websites\nAnecdote: Spotify example of lost API access\nExplain tidy data: snake_case column names, correct data types\nEmphasize code flexibility ‚Äî developers can change APIs overnight\nActivity: Scaffolding + Code review using example(s)"
  },
  {
    "objectID": "sessions/session_1/01_Extraction_Introduction.html#session-1-introduction-to-data-extraction",
    "href": "sessions/session_1/01_Extraction_Introduction.html#session-1-introduction-to-data-extraction",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "",
    "text": "CourseKata\norganization of workshop of lecture and questions throughout\n\n\n\n\n\nUnderstand the importance of extracting dynamic data (via HTML and APIs) in modern data analysis and teaching\nLearn how to access and work with APIs to retrieve structured, real-time data\nLearn how to bring HTML-based data (e.g., web tables) into R using scraping tools\nApply these skills through hands-on coding practice and discussion of classroom integration\n\n\n\n(need to place) (Mention Querying data base more as an action) ### 2. Conceptual Foundation\n\nP1. My mentor, Allan, says ask good questions‚Ä¶\nP2. Statistical Question: Who had the most impactful first season in terms of points: Michael Jordan, LeBron James, or Kobe Bryant?\nP3. I recently submitted a manuscript on this exact dataset, so let‚Äôs use it as our starting point.\nP4. We‚Äôll begin by working with a static Excel file that contains per-game stats for each player‚Äôs 15 seasons in the NBA.\nP5. Let‚Äôs Load the pertinent libraries\nT1. ____ Fill in the code (chatgpt)\n\n\nlibrary(readxl)      ## Data Extraction      --- E\nlibrary(dplyr)       ## Data Transformation  --- T\nlibrary(ggplot2)     ## Data Visualization   --- V\n\n\nP6. Load in the data\nT2. ____ Fill in the code (chatgpt)\n\n\nnba_df &lt;- read_xlsx(\"nba_data.xlsx\", sheet = \"modern_nba_legends_08302019\")\n\n\nP7. Let‚Äôs view the data ‚Ä¶\nT3. Use the glimpse function to view the data\n\n\nglimpse(nba_df)\n\nRows: 3,400\nColumns: 38\n$ Name          &lt;chr&gt; \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"MJ\", \"M‚Ä¶\n$ Season        &lt;chr&gt; \"season_1\", \"season_1\", \"season_1\", \"season_1\", \"season_‚Ä¶\n$ Game_Location &lt;chr&gt; \"Home\", \"Away\", \"Home\", \"Away\", \"Away\", \"Away\", \"Away\", ‚Ä¶\n$ Game_Outcome  &lt;chr&gt; \"W\", \"L\", \"W\", \"W\", \"L\", \"W\", \"W\", \"W\", \"W\", \"L\", \"L\", \"‚Ä¶\n$ Point_Margin  &lt;dbl&gt; 16, -2, 6, 5, -16, 4, 15, 2, 3, -20, -9, -17, -10, 19, -‚Ä¶\n$ Rk            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1‚Ä¶\n$ G             &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1‚Ä¶\n$ Date          &lt;dttm&gt; 1984-10-26, 1984-10-27, 1984-10-29, 1984-10-30, 1984-11‚Ä¶\n$ Age           &lt;chr&gt; \"21-252\", \"21-253\", \"21-255\", \"21-256\", \"21-258\", \"21-26‚Ä¶\n$ Tm            &lt;chr&gt; \"CHI\", \"CHI\", \"CHI\", \"CHI\", \"CHI\", \"CHI\", \"CHI\", \"CHI\", ‚Ä¶\n$ Opp           &lt;chr&gt; \"WSB\", \"MIL\", \"MIL\", \"KCK\", \"DEN\", \"DET\", \"NYK\", \"IND\", ‚Ä¶\n$ GS            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ MP            &lt;dbl&gt; 40, 34, 34, 36, 33, 27, 33, 42, 43, 33, 44, 39, 42, 30, ‚Ä¶\n$ FG            &lt;dbl&gt; 5, 8, 13, 8, 7, 9, 15, 9, 18, 12, 4, 11, 11, 9, 10, 6, 9‚Ä¶\n$ FGA           &lt;dbl&gt; 16, 13, 24, 21, 15, 19, 22, 22, 27, 24, 17, 26, 22, 13, ‚Ä¶\n$ FG_Percent    &lt;dbl&gt; 0.313, 0.615, 0.542, 0.381, 0.467, 0.474, 0.682, 0.409, ‚Ä¶\n$ `3P`          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,‚Ä¶\n$ `3PA`         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 3, 0, 0, 1, 0, 1, 0, 0,‚Ä¶\n$ `3P_Percent`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 1, 0, NA, 0, NA, NA, 0, ‚Ä¶\n$ FT            &lt;dbl&gt; 6, 5, 11, 9, 3, 7, 3, 9, 8, 3, 8, 12, 13, 5, 10, 1, 3, 2‚Ä¶\n$ FTA           &lt;dbl&gt; 7, 5, 13, 9, 4, 9, 4, 12, 11, 3, 8, 16, 14, 6, 10, 1, 4,‚Ä¶\n$ FT_Percent    &lt;dbl&gt; 0.857, 1.000, 0.846, 1.000, 0.750, 0.778, 0.750, 0.750, ‚Ä¶\n$ ORB           &lt;dbl&gt; 1, 3, 2, 2, 3, 1, 4, 2, 2, 0, 0, 2, 4, 0, 3, 0, 1, 2, 2,‚Ä¶\n$ DRB           &lt;dbl&gt; 5, 2, 2, 2, 2, 3, 4, 7, 8, 2, 5, 3, 9, 4, 3, 2, 2, 3, 0,‚Ä¶\n$ TRB           &lt;dbl&gt; 6, 5, 4, 4, 5, 4, 8, 9, 10, 2, 5, 5, 13, 4, 6, 2, 3, 5, ‚Ä¶\n$ AST           &lt;dbl&gt; 7, 5, 5, 5, 5, 3, 5, 4, 4, 2, 7, 2, 2, 3, 8, 3, 2, 5, 3,‚Ä¶\n$ STL           &lt;dbl&gt; 2, 2, 6, 3, 1, 3, 3, 2, 3, 2, 5, 2, 2, 4, 3, 3, 2, 3, 1,‚Ä¶\n$ BLK           &lt;dbl&gt; 4, 1, 2, 1, 1, 1, 2, 5, 2, 1, 2, 1, 2, 1, 1, 2, 0, 0, 1,‚Ä¶\n$ TOV           &lt;dbl&gt; 5, 3, 3, 6, 2, 5, 5, 3, 4, 1, 4, 3, 6, 4, 4, 4, 2, 4, 4,‚Ä¶\n$ PF            &lt;dbl&gt; 2, 4, 4, 5, 4, 5, 2, 4, 4, 4, 5, 3, 3, 4, 4, 1, 5, 4, 3,‚Ä¶\n$ PTS           &lt;dbl&gt; 16, 21, 37, 25, 17, 25, 33, 27, 45, 27, 16, 34, 35, 23, ‚Ä¶\n$ GmSc          &lt;dbl&gt; 12.5, 19.4, 32.9, 14.7, 13.2, 14.9, 29.3, 21.2, 37.5, 17‚Ä¶\n$ number_game   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1‚Ä¶\n$ DD            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ TD            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ Age_Years     &lt;dbl&gt; 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, ‚Ä¶\n$ Age_Days      &lt;dbl&gt; 252, 253, 255, 256, 258, 264, 265, 267, 270, 272, 274, 2‚Ä¶\n$ Last_Name     &lt;chr&gt; \"Jordan\", \"Jordan\", \"Jordan\", \"Jordan\", \"Jordan\", \"Jorda‚Ä¶\n\n\n\nQ1. Look through the data, does it look clean? Discuss amongst Your peers. (chatgpt)\nAns Q1: It is clean the numeric variables are supposed to be numeric and the characrets variables are treated as char (chatgpt)\nP8. Now let‚Äôs clean the focus on the data frame that we are after\nT4. ____ Fill in the code (chatgpt)\n\n\nseason_1_df &lt;- nba_df %&gt;% \n  filter(Season == \"season_1\")\n\n\nP9. Now lets look at a plot of their points (chatgpt)\n\nT5. ____ Fill in the code (chatgpt)\n\nseason_1_df %&gt;% \n  ggplot(aes(x = Name, y = PTS)) +\n  geom_boxplot() +\n  theme_bw() \n\nWarning: Removed 14 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nNote 1: We could have spruced it up but here we just wanted to answer the question, if you have the urge please do so.\nQ2. What conclusion could be made (chatgpt)\n\nP10. Now what about, Magic Johnson or Wilt Chamberlain Maybe Luka Donƒçiƒá or Ja Morant If I wanted to add this data I need to go to the originnal source not an excel sheet to do this (chatgpt)\nNote 2: - Shift students from being passive data users to active data seekers - Move beyond the idea of ‚Äúwaiting for clean data‚Äù to learning how to access, validate, and clean it themselves - Teach both the skill to extract and the capacity to teach extraction\nNote 3: - Why this matters: We as instructors should not just rely on pre-built packages or static datasets. The digital world changes constantly ‚Äî websites, APIs, and file structures evolve.\n\nOur responsibility: Teach students (and ourselves) how to adapt and access real-world data sources. Equip learners with skills to extract, not just consume pre-extracted content.\nDespite the growing importance of live data, most introductory courses still rely heavily on static, pre-cleaned datasets. This limits students‚Äô exposure to the realities of modern data work.\nKey idea: Flat files can still be dynamic depending on how they‚Äôre maintained ‚Äî but we use the term ‚Äúdynamic‚Äù here to emphasize external, real-time data access through APIs and web scraping.\nThe availability of dynamic, frequently updated data ‚Äî especially via web APIs ‚Äî has grown exponentially in recent years. This shift demands new strategies in how we teach data access.\n\n\n\n\n\nP11.\nExamples: CSV, Excel files\nTypically unchanging unless manually edited\nOften pre-loaded into classroom activities\nMay still require cleaning (e.g., column names, missing data)\nNote 4: Messy data is not always a bad thing\n\n\n\n\n\nP12.\nDefinition: Data sources that update over time or are externally controlled (i.e., you don‚Äôt own the source)\nP13.\nTwo primary types:\n\nApplication Programming Interface APIs ‚Äì Designed to serve structured data upon request (e.g., player stats, weather)\nHypertext Markup Language HTML/Web Pages ‚Äì Seen as dynamic when content changes (especially sports, news, etc.)\n\nHTML pages are primarily designed for human readability, while APIs are designed for structured machine access. Both offer pathways to dynamic data, each with different advantages and challenges.\nBridging the gap between classroom exercises and real-world data practice requires that students learn not just how to analyze data ‚Äî but how to find it, extract it, and prepare it themselves.\nNote 5: HTML can be treated as static or dynamic depending on how frequently the page updates. For this workshop, we treat HTML as dynamic, especially for sports data.\n\n\n\n\nNote 6:\n\nThere are many kinds of APIs, but in this workshop, we‚Äôll focus specifically on web APIs ‚Äî tools designed to let us request and retrieve data from online sources.\nIn R, we‚Äôll act like a piece of software making those requests, allowing us to access live data programmatically.\n\n(Mention Querying data base more as an action)\n\n\n\nFlow of Data via API (vrogue.co)\n\n\n\nP14.\nAPI stand for Application Programming Interfaces\nIt is a way for software to communicate with one another\nOne way it work is that it allow programs to request data directly from external servers in a structured format (most often JSON).\n\n(Mention Querying data base more as an action)\n{\n  \"player\": \"LeBron James\",\n  \"points\": 27.1,\n  \"team\": \"Lakers\"\n}\n\nThe keys are players, pointsand team\nThe values for the corresponding keys are LeBron James, 27.1, Lakers\n\nNote 7:\n\nThere are a lot of acronyms\nJSON - Java Script Object Notation - javascript is web developing software (chatgpt)\n\n(Mention Querying data base more as an action)\nQ3. What does JSON? Answer: Java script object notation\nNote 8:\n\nDescribe Image Flow of Data via API (vrogue.co)\nA user sends a request via the internet ‚Üí the API talks to the server ‚Üí the server queries the database ‚Üí the API responds with data, often as JSON. (Mention Querying data base more as an action)\nP15.\nLearning to work with web APIs teaches students more than just how to extract data ‚Äî it gives them the tools to:\n\nLocate relevant APIs (e.g., weather, sports, music)\nConstruct and test their own API requests\nInterpret JSON responses (including nested structures)\nTransform the results into tidy formats ready for analysis\n\n\n(Mention Querying data base more as an action)\n\nP16\nAPIs aren‚Äôt just technical tools, they‚Äôre increasingly the primary way to access and query data stored in external databases.\nIn today‚Äôs fast-changing digital environment, students must be equipped to retrieve and work with information from live, external sources, not just rely on pre-cleaned datasets.\nNote 9:\n\nThis is what pushes students from passive observers of data to active agents in its collection, structure, and use. It aligns closely with what real-world data science jobs require, especially when you‚Äôre no longer just analyzing data, but acquiring it.\n\n\n\nVolume of Data Created (Statista)\n\n\nThe use of APIs requires keys, which are unique and secret codes that are used to authorize your request and identify your user and billing information. Consequently, keeping these codes secret is imperative. To do so, store API keys in environment files which reside on your computer, and not coded into variables or available in plain text on your working files.\nshow images creating an app (provide my youtube video) - get id and secret (blurr it out) (make highlighe secret and ID on image )\n[\nid: (eman post in some way) secret: (eman post on screen)\nNote 10: Note all APIs website work the same, getting experience with 1-2, you start to develop a philosophy on how to approach them (chatgpt)\nCode 2\nStep 1: Load the Required Packages\n\nlibrary(spotifyr)\nlibrary(dplyr)\n\n\nStep 2: Set Up Credentials\nBefore using the Spotify API, you need to create an app on the and retrieve your Client ID and Client Secret. (I have a video here to do this on your own but on the screen, you can see the id and secret - (chatgpt))How to make app\n\n# Replace these with your actual credentials (do NOT share them publicly)\nclient_id &lt;- 'your_spotify_client_id_here'\nclient_secret &lt;- 'your_spotify_client_secret_here'\n\n\nStep 3: Get an Access Token\nThe token allows Spotify to recognize both who you are and what you‚Äôre requesting.\n\naccess_token &lt;- get_spotify_access_token(\n  client_id = client_id,\n  client_secret = client_secret\n)\n\n\nStep 4: Request Artist Data\nNow that you‚Äôre authenticated, you can request track-level data from Spotify‚Äôs API.\n\nbeatles_df &lt;- get_artist_audio_features('the beatles')\n\n\nStep 5: Explore the Dataset\nUse glimpse() to quickly examine the structure of the data.\n\nglimpse(beatles_df)\n\n\nStep 6: Try Another Artist\nTry retrieving audio features for a different artist.\n\ntaylor_df &lt;- get_artist_audio_features('taylor swift')\nglimpse(taylor_df)\n\nNote 11: How databases and access can change in that the developers may change access or change the way you access information so you have to be aware of this. (chatgpt)\nData types are usually sotred correctly as chars or numerics so noy much cleaning required (chatgpt)\n\n\n\n\n\n\n\nProcess of HTML Scraping (sstm2.com)\n\n\n\nP17\n\nWebsites are designed using Hypertext Markup Language (HTML) to display information, (chatgpt: say better)\nInformation stored as tables are ideal within a HTML page (chatgpt: say more)\nNote 12: - we do not want to just copy and paste a table into a csv format it then clearn it up again in R, ideally we want to have access to and bring it into R (chatgpt: say better)\n\nP18:\n\nBelow is an image of code for html table and the actual table that it would produce\n{width=‚Äú200‚Äù, height = ‚Äú1000‚Äù}\n\nNote 13:\n\nHighlight the following concepts:\n\nbeginning and the end of table\nthe column name\neach Row\nHow it translate into a human readable table\n\nEmphasize that we‚Äôre only focusing on &lt;table&gt; tags for this workshop\n\nP19.\n\nNow lets see one of the libraries that allows us to scrape in R\n\nlibrary(htmltab)\n\nNote 14: Lets go to the url via webbroswer\n\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n\nP20.\nThis function requires 2 args url and table number we can guess at it and may work\n\niowa_state_counties_df &lt;- htmltab(url,1)\n\n\niowa_state_counties_df &lt;- htmltab(url,2)\n\nNote 15:\n\nUnless you know html and want to look at the source code or you what exactly a table looks like you will have to guess sometimes\nWe can get the warning to go away by ‚Ä¶\nP21.\n\nThis is what I would call static because the counties are note changeing but if we wanted baseball data at which currently everyday new data is displayed it is ideal that we have a more robust method of fgetting this data rather using htmltabs\n\nP22.\n\nCheck out article: Web Scraping in the Statistics and Data Science Curriculum: Challenges and Opportunities - Mine Dogucu & Mine √áetinkaya-Rundel\n\nP23.\n\nMuch like APIs, lots of relevant and useful information is available directly on webpages, which are readable by humans rather than APIs which are designed for machine access.\nBy learning this skill, students are able to:\n\nLocate relevant sources (e.g., sports data from Pro Football Reference)\nUnderstand how websites deliver and organize content\nTransform and clean data for analysis and visualization\n\nOften times, HTML tables contain unexpected structures or data types (images, links, etc) and can present a challenge that develops not only data cleaning skills, but intention, planning, and adaptability when handling and analyzing difficult data.\n\n\n\n\n\nSession 1: Introduction\nThis session serves as a discussion of the principles and reasoning behind learning these concepts and how they can benefit the classroom.\nSession 2: Getting Weather Data via OpenWeather API\nIn this session, we dive into OpenWeather API and learn to use packages like httr2 to execute API calls. We will also discuss URLs, queries, data structures, and more.\nSession 3: Scraping NFL Sports Data\nIn this session, we will use Pro-Football Reference to learn how to extract and clean HTML table data for use in statistical analysis and visualizations.\nSession 4: Putting it All Together (Project)\nIn this project, we will use HTML scraping joined with the OpenWeather API to create our own cloropleth map of Iowa.\n\n\n\nChloropleth Map (geoapify.com)\n\n\n\n\n\n\n\nGoal: Engage participants in applying both API and HTML extraction methods.\nPart A: API\n\nUse a new function from the spotifyr package\nCreate a simple plot using the data\nAsk comprehension questions:\n\nHow does this differ from static file use?\nWhat‚Äôs confusing about working with API responses?\n\n\nPart B: HTML\n\nExtract state-specific data from a chosen Wikipedia page\nGuide participants through cleaning it (if time allows)\nAsk guiding questions:\n\nDid the table number match what you expected?\nWhat challenges did you face?\n(Example prompt written on the board: ‚ÄúNot every state‚Äôs table is the same.‚Äù)\n\n\n\n\n\n\nWhat did we learn?\nHow does this connect to the original Goals & Objectives of the session?\nHow do you see yourself using this in your classroom?\nWhat kinds of APIs or HTML sources would be most relevant for your students?\n\n\n\n\n\nSet expectations and workshop goals\nWhy data extraction matters: relevance to real-world education\nOverview of the layout / table of contents\nDiscuss libraries used (tidyverse, rvest, httr, etc.)\nBest practices (e.g., avoiding hardcoding, consistent comments)\nAdapting to changing APIs/websites\nAnecdote: Spotify example of lost API access\nExplain tidy data: snake_case column names, correct data types\nEmphasize code flexibility ‚Äî developers can change APIs overnight\nActivity: Scaffolding + Code review using example(s)"
  },
  {
    "objectID": "Tasks_Lists.html#session-1-tasks",
    "href": "Tasks_Lists.html#session-1-tasks",
    "title": "Tasks List",
    "section": "Session 1 Tasks",
    "text": "Session 1 Tasks"
  },
  {
    "objectID": "Tasks_Lists.html#session-2-tasks",
    "href": "Tasks_Lists.html#session-2-tasks",
    "title": "Tasks List",
    "section": "Session 2 Tasks",
    "text": "Session 2 Tasks\n\nPurchase API Key"
  },
  {
    "objectID": "Tasks_Lists.html#session-3-tasks",
    "href": "Tasks_Lists.html#session-3-tasks",
    "title": "Tasks List",
    "section": "Session 3 Tasks",
    "text": "Session 3 Tasks"
  },
  {
    "objectID": "Tasks_Lists.html#session-4-tasks",
    "href": "Tasks_Lists.html#session-4-tasks",
    "title": "Tasks List",
    "section": "Session 4 Tasks",
    "text": "Session 4 Tasks"
  },
  {
    "objectID": "Tasks_Lists.html#end-of-workshop-tasks",
    "href": "Tasks_Lists.html#end-of-workshop-tasks",
    "title": "Tasks List",
    "section": "End of Workshop Tasks",
    "text": "End of Workshop Tasks"
  }
]