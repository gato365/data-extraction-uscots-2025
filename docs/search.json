[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "",
    "text": "Empowering statistics educators with real-world data skills.\nThis site houses all the materials for our hands-on workshop at USCOTS 2025.\nWe‚Äôre learning how to extract, clean, and use live data from the web and APIs‚Äîbringing authentic data science into your classroom."
  },
  {
    "objectID": "index.html#workshop-goals",
    "href": "index.html#workshop-goals",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Workshop Goals",
    "text": "Workshop Goals\n\nUnderstand why extracting dynamic data is essential in modern statistics education.\nLearn how to read HTML tables and use web APIs to collect real-world data.\nUse tidyverse tools to transform and clean this data for analysis.\nBuild confidence integrating unstructured data into your courses."
  },
  {
    "objectID": "index.html#who-this-workshop-is-for",
    "href": "index.html#who-this-workshop-is-for",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Who This Workshop Is For",
    "text": "Who This Workshop Is For\n\nStatistics educators and data science instructors\nComfortable with R and tidyverse\nCurious about web scraping, APIs, and teaching modern data practices"
  },
  {
    "objectID": "index.html#whats-inside",
    "href": "index.html#whats-inside",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "What‚Äôs Inside",
    "text": "What‚Äôs Inside\n\nSession 1 ‚Äì Why and How We Extract Data\n\nÔ∏è Session 2 ‚Äì Working with APIs (Weather Data)\n\nSession 3 ‚Äì Scraping Sports Data from HTML\n\nSession 4 ‚Äì Final Challenge and Reflection\n\nExplore these under the Sessions tab above."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Getting Started",
    "text": "Getting Started\n\nHead over to Session 1 - Introduction to begin your learning journey.\nOr, check out the Organized Outline for a roadmap of the workshop.\n\n\n##Ô∏è Tools We‚Äôll Use\n\nrvest, httr, jsonlite, purrr, tidyverse\nJupyter via CourseKata CKHub\nRStudio / Quarto"
  },
  {
    "objectID": "index.html#quote-to-frame-the-work",
    "href": "index.html#quote-to-frame-the-work",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Quote to Frame the Work",
    "text": "Quote to Frame the Work\n\n‚ÄúThe world is one big dataset‚Äîif we learn how to extract it.‚Äù\n‚Äî Inspired by data educators like you"
  },
  {
    "objectID": "index.html#lets-get-started",
    "href": "index.html#lets-get-started",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Let‚Äôs Get Started!",
    "text": "Let‚Äôs Get Started!\nReady to transform how your students work with data?\nüìé Navigate using the menu above and join us in making data science real, relevant, and resilient."
  },
  {
    "objectID": "sessions/session_1/libraries_of_packages.html",
    "href": "sessions/session_1/libraries_of_packages.html",
    "title": "Library of Packages",
    "section": "",
    "text": "Chloropleth Map (geoapify.com)\n\n\n\n\nIntroduction to Libraries & Glossary\nThis project, when possible, utilizes tidyverse packages. tidyverse is a collection of open-source packages that are well-integrated to tackle problems of data extraction, manipulation, transformation, exploration, and visualization.\n\n\n\nCollection of Tidyverse packages.\n\n\n\nlibrary(rvest)      # Web scraping\nlibrary(dplyr)      # Data manipulation\nlibrary(stringr)    # String cleaning\nlibrary(rlang)      # Advanced evaluation\nlibrary(purrr)      # Functional tools\nlibrary(ggplot2)    # Visualizations\nlibrary(httr2)      # Makes web requests\nlibrary(tibble)     # Easier and prettier data frames\nlibrary(lubridate)  # Handles dates\nlibrary(dotenv)     # Loads environment variables from .Renviron\nlibrary(glue)       # Easier string concatenation\nlibrary(tigris)     # U.S. shapefiles for mapping\n\nThis is a list of R packages used in this project, accompanied by brief descriptions of what they do.\nrvest\n\nUsed for web scraping: parses HTML/XML documents into a navigable format.\nExtracts structured data using CSS selectors, XPath, or tag-based search (html_table(), html_text(), etc.).\n\n\ndplyr\n\nCore package for tidyverse-style data manipulation (filter(), mutate(), select(), etc.).\nSupports chaining operations with %&gt;% / |&gt;, making data workflows readable and efficient.\n\n\nstringr\n\nProvides a consistent set of functions for string manipulation.\nHandles pattern matching, replacement, splitting, and formatting.\n\n\nrlang\n\nSupports advanced evaluation and programming with tidyverse tools.\nUseful when writing custom functions that dynamically reference or modify variables.\n\n\npurrr\n\nEnables functional programming with mapping tools like map(), map_df(), walk(), etc.\nReplaces (many) for-loops and supports clean iteration over lists and vectors.\n\n\nggplot2\n\nGraphics package for building layered, flexible visualizations.\nSupports various plot types, themes, scales, and faceting for data storytelling.\n\n\nhttr2\n\nModern HTTP client designed for tidyverse-like API interaction.\nReplaces httr with a more intuitive and pipeable interface (request() |&gt; req_perform()).\n\n\ntibble\n\nA modern rethinking of the data.frame that prints cleaner and behaves more predictably.\nDefault in tidyverse workflows; avoids surprises like string-to-factor conversion.\n\n\nlubridate\n\nSimplifies working with dates and times: parsing, formatting, and arithmetic.\nMakes it easy to extract components (e.g., month, weekday) and perform date math.\n\n\ndotenv\n\nLoads environment variables from a .env or .Renviron file into R.\nKeeps sensitive data like API keys out of your scripts and version control.\n\n\nglue\n\nProvides string interpolation (e.g., glue(\"Hello {name}\")).\nCleaner and safer than paste() for building URLs, messages, or SQL queries.\n\n\ntigris\n\nDownloads shapefiles and geographic boundary data (e.g., counties, states) from the U.S. Census Bureau.\nReturns spatial sf data frames, making it easy to map and visualize geographic data."
  },
  {
    "objectID": "sessions/session_4/04_Extraction_FinalActivity.html",
    "href": "sessions/session_4/04_Extraction_FinalActivity.html",
    "title": "Session 4: Final Activity",
    "section": "",
    "text": "Now that we have worked with HTML elements and API calls, let‚Äôs put these skills together to create a weather map of Iowa counties based on the maximum temperature at each county‚Äôs administrative seat (county seat).\n\n\n\nChloropleth Map (geoapify.com)\n\n\n\n\n\nWe‚Äôll begin by loading the libraries necessary for data manipulation, API calls, and visualization.\n\n# Step 1: Load Libraries\n# Step 1a: General utilities\nlibrary(httr2)       # Makes web requests\nlibrary(tibble)      # Easier and prettier data frames\nlibrary(lubridate)   # Handles dates\nlibrary(ggplot2)     # Data visualization\nlibrary(dplyr)       # Data manipulation\nlibrary(dotenv)      # Loads environment variables from .Renviron\nlibrary(glue)        # Easier string concatenation\nlibrary(purrr)       # Functional programming tools\nlibrary(rvest)       # Web scraping\nlibrary(tigris)      # U.S. shapefiles for mapping\nlibrary(stringr)     # String manipulation and handling\n\n\n# Step 1b: Load API key from .Renviron.txt\ndotenv::load_dot_env(file = \".Renviron.txt\")\n\n\n\n\n\nHere we extract a table from Wikipedia listing all Iowa counties and their corresponding county seats.\n\n# Step 2: Scrape HTML Table from Wikipedia\n# Step 2a: Set URL\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n\n# Step 2b: Read HTML\nwebpage &lt;- url |&gt;  \n  rvest::read_html()\n\n# Step 2c: Extract all HTML tables from the page\nhtml_tables &lt;- webpage |&gt; \n  rvest::html_table()\n\n# Step 2d: Select the correct table (based on inspection)\ntable1 &lt;- html_tables |&gt; \n  purrr::pluck(2)\n\n# Step 2e: Clean and prepare county seat names\n#         Add ', IA, USA' to ensure geolocation works with OpenWeather API\n#         Use only the first County Seat city name when mulitple exist\ncounty_seats_df &lt;- table1 |&gt; \n  mutate(\n    `County seat[4]` = str_split(`County seat[4]`, \" and \") |&gt; sapply(`[`, 1),\n    city = paste0(`County seat[4]`, \", IA, USA\")\n  )\n\nDiscussion: Why would we need to append‚Äù, IA, USA‚Äù to the city name?\nWhat would happen if we do not?\nA: This helps geocoding APIs return accurate coordinates. (Just ‚ÄúAmes‚Äù returns a rural town of 600 in Northern France)\n\n\n\nNot Exactly Ames, IA, USA. Church of Ames, France (Wikipedia)\n\n\n\n\n\n\nWe now define a function to call the OpenWeather Geocoding and One Call APIs. Then we use it to retrieve temperature data for each city.\n\n# Step 3: Define function to get geocoded weather data from OpenWeather\nget_city_weather &lt;- function(city, date = Sys.Date()) {\n  # Step 3a: Get coordinates from geocoding API\n  geo_url &lt;- glue(\n    \"http://api.openweathermap.org/geo/1.0/direct?\",\n    \"q=\", URLencode(city),\n    \"&limit=1&appid=\", Sys.getenv(\"API_KEY\"))\n  geo_response &lt;- req_perform(request(geo_url))\n\n  if (resp_status(geo_response) == 200) {\n    geo_data &lt;- as.data.frame(resp_body_json(geo_response))\n    if (nrow(geo_data) == 0) return(NULL)\n\n    lat &lt;- geo_data$lat\n    lon &lt;- geo_data$lon\n\n    # Step 3b: Call the weather summary API\n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", format(date, \"%Y-%m-%d\"),\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\")\n    weather_response &lt;- req_perform(request(weather_url))\n\n    if (resp_status(weather_response) == 200) {\n      weather_data &lt;- resp_body_json(weather_response)\n      tibble(\n        city = city,\n        date = date,\n        lat = lat,\n        lon = lon,\n        temp_max = weather_data$temperature$max,\n        temp_min = weather_data$temperature$min\n      )\n    } else return(NULL)\n  } else return(NULL)\n}\n\n\n# Step 3c: Apply the function to all county seat cities\ncities &lt;- county_seats_df |&gt; \n  pull(12)\nweather_df &lt;- bind_rows(lapply(cities, get_city_weather))\n\nDiscussion: What happens if one of the cities fails to return a result?\nWe could add error handling or a fallback method, such as purrr::possibly()?\n\n\n\n\nNow we join our temperature data with spatial geometries (map pieces) from the tigris package so we can map it.\n\n# Step 4: Merge weather data into spatial map\n# Step 4a: Join temperature data with counties via county seat\ncounty_temp_df &lt;- county_seats_df |&gt; \n  left_join(weather_df, by = \"city\")  # joins on city (admin chair)\n\n# Step 4b: Load Iowa county shapes from tigris\niowa_counties &lt;- tigris::counties(state = \"IA\", cb = TRUE, class = \"sf\")\n\n# Step 4c: Join temperature data into spatial counties by NAME\n# (Be sure county names match exactly)\niowa_map_filled &lt;- iowa_counties |&gt; \n  left_join(county_temp_df, by = c(\"NAMELSAD\" = \"County\"))\n\nDiscussion: How could we verify that all counties successfully matched?\n\n\n\n\nFinally, we use ggplot2 and geom_sf() to create a choropleth map of Iowa counties filled by temperature.\n\n# Step 5: Plot the map\n# Step 5a: Use fill aesthetic to show temperature per county\n\nggplot(iowa_map_filled) +\n  geom_sf(aes(fill = temp_max), color = \"white\") +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Max Temp (¬∞F)\", na.value = \"grey90\") +\n  labs(\n    title = \"Iowa County Temperatures by County Seat\",\n    subtitle = paste(\"Date:\", unique(weather_df$date)),\n    caption = \"Source: OpenWeather API\"\n  ) +\n  theme_minimal(base_size = 14)\n\nIdeas to expand this visualization:\n- Add interactivity with plotly\n- Animate changes over multiple dates\n- Compare actual temps to historical averages\n- Add city point labels or icons"
  },
  {
    "objectID": "sessions/session_4/04_Extraction_FinalActivity.html#session-4-final-activity",
    "href": "sessions/session_4/04_Extraction_FinalActivity.html#session-4-final-activity",
    "title": "Session 4: Final Activity",
    "section": "",
    "text": "Now that we have worked with HTML elements and API calls, let‚Äôs put these skills together to create a weather map of Iowa counties based on the maximum temperature at each county‚Äôs administrative seat (county seat).\n\n\n\nChloropleth Map (geoapify.com)\n\n\n\n\n\nWe‚Äôll begin by loading the libraries necessary for data manipulation, API calls, and visualization.\n\n# Step 1: Load Libraries\n# Step 1a: General utilities\nlibrary(httr2)       # Makes web requests\nlibrary(tibble)      # Easier and prettier data frames\nlibrary(lubridate)   # Handles dates\nlibrary(ggplot2)     # Data visualization\nlibrary(dplyr)       # Data manipulation\nlibrary(dotenv)      # Loads environment variables from .Renviron\nlibrary(glue)        # Easier string concatenation\nlibrary(purrr)       # Functional programming tools\nlibrary(rvest)       # Web scraping\nlibrary(tigris)      # U.S. shapefiles for mapping\nlibrary(stringr)     # String manipulation and handling\n\n\n# Step 1b: Load API key from .Renviron.txt\ndotenv::load_dot_env(file = \".Renviron.txt\")\n\n\n\n\n\nHere we extract a table from Wikipedia listing all Iowa counties and their corresponding county seats.\n\n# Step 2: Scrape HTML Table from Wikipedia\n# Step 2a: Set URL\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n\n# Step 2b: Read HTML\nwebpage &lt;- url |&gt;  \n  rvest::read_html()\n\n# Step 2c: Extract all HTML tables from the page\nhtml_tables &lt;- webpage |&gt; \n  rvest::html_table()\n\n# Step 2d: Select the correct table (based on inspection)\ntable1 &lt;- html_tables |&gt; \n  purrr::pluck(2)\n\n# Step 2e: Clean and prepare county seat names\n#         Add ', IA, USA' to ensure geolocation works with OpenWeather API\n#         Use only the first County Seat city name when mulitple exist\ncounty_seats_df &lt;- table1 |&gt; \n  mutate(\n    `County seat[4]` = str_split(`County seat[4]`, \" and \") |&gt; sapply(`[`, 1),\n    city = paste0(`County seat[4]`, \", IA, USA\")\n  )\n\nDiscussion: Why would we need to append‚Äù, IA, USA‚Äù to the city name?\nWhat would happen if we do not?\nA: This helps geocoding APIs return accurate coordinates. (Just ‚ÄúAmes‚Äù returns a rural town of 600 in Northern France)\n\n\n\nNot Exactly Ames, IA, USA. Church of Ames, France (Wikipedia)\n\n\n\n\n\n\nWe now define a function to call the OpenWeather Geocoding and One Call APIs. Then we use it to retrieve temperature data for each city.\n\n# Step 3: Define function to get geocoded weather data from OpenWeather\nget_city_weather &lt;- function(city, date = Sys.Date()) {\n  # Step 3a: Get coordinates from geocoding API\n  geo_url &lt;- glue(\n    \"http://api.openweathermap.org/geo/1.0/direct?\",\n    \"q=\", URLencode(city),\n    \"&limit=1&appid=\", Sys.getenv(\"API_KEY\"))\n  geo_response &lt;- req_perform(request(geo_url))\n\n  if (resp_status(geo_response) == 200) {\n    geo_data &lt;- as.data.frame(resp_body_json(geo_response))\n    if (nrow(geo_data) == 0) return(NULL)\n\n    lat &lt;- geo_data$lat\n    lon &lt;- geo_data$lon\n\n    # Step 3b: Call the weather summary API\n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", format(date, \"%Y-%m-%d\"),\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\")\n    weather_response &lt;- req_perform(request(weather_url))\n\n    if (resp_status(weather_response) == 200) {\n      weather_data &lt;- resp_body_json(weather_response)\n      tibble(\n        city = city,\n        date = date,\n        lat = lat,\n        lon = lon,\n        temp_max = weather_data$temperature$max,\n        temp_min = weather_data$temperature$min\n      )\n    } else return(NULL)\n  } else return(NULL)\n}\n\n\n# Step 3c: Apply the function to all county seat cities\ncities &lt;- county_seats_df |&gt; \n  pull(12)\nweather_df &lt;- bind_rows(lapply(cities, get_city_weather))\n\nDiscussion: What happens if one of the cities fails to return a result?\nWe could add error handling or a fallback method, such as purrr::possibly()?\n\n\n\n\nNow we join our temperature data with spatial geometries (map pieces) from the tigris package so we can map it.\n\n# Step 4: Merge weather data into spatial map\n# Step 4a: Join temperature data with counties via county seat\ncounty_temp_df &lt;- county_seats_df |&gt; \n  left_join(weather_df, by = \"city\")  # joins on city (admin chair)\n\n# Step 4b: Load Iowa county shapes from tigris\niowa_counties &lt;- tigris::counties(state = \"IA\", cb = TRUE, class = \"sf\")\n\n# Step 4c: Join temperature data into spatial counties by NAME\n# (Be sure county names match exactly)\niowa_map_filled &lt;- iowa_counties |&gt; \n  left_join(county_temp_df, by = c(\"NAMELSAD\" = \"County\"))\n\nDiscussion: How could we verify that all counties successfully matched?\n\n\n\n\nFinally, we use ggplot2 and geom_sf() to create a choropleth map of Iowa counties filled by temperature.\n\n# Step 5: Plot the map\n# Step 5a: Use fill aesthetic to show temperature per county\n\nggplot(iowa_map_filled) +\n  geom_sf(aes(fill = temp_max), color = \"white\") +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Max Temp (¬∞F)\", na.value = \"grey90\") +\n  labs(\n    title = \"Iowa County Temperatures by County Seat\",\n    subtitle = paste(\"Date:\", unique(weather_df$date)),\n    caption = \"Source: OpenWeather API\"\n  ) +\n  theme_minimal(base_size = 14)\n\nIdeas to expand this visualization:\n- Add interactivity with plotly\n- Animate changes over multiple dates\n- Compare actual temps to historical averages\n- Add city point labels or icons"
  },
  {
    "objectID": "sessions/session_2/02_Extraction_Weather_Data_API.html",
    "href": "sessions/session_2/02_Extraction_Weather_Data_API.html",
    "title": "Session 2: Weather Data - OpenWeatherAPI",
    "section": "",
    "text": "Explain what an API is and how it supports data extraction Theoretical elements of API\nMake requests to a public API and interpret the JSON response\nUnderstand and apply HTTP status codes and API keys\nWrite clean, readable code to extract and parse API data\n\n(TODO: Change the based on concept Foundation) (Mention Querying data base more as an action)\n\n\n\nPart A. Theoretical ideas of APIs\nNote 1:\n\nThis is not a webdeveloper nor a CS course but with a decent understanding of the logic, you and your students will appreciate the utilizartion of web scrapiing more\n\n\n\nWhat is an API? (Again)\nIt is the abiluty for software to communicate\n\n\n\nAPI Call (PhoenixNap.com)\n\n\n\nQ1: What is its utility of APIs? (multiple choice)\n\nNote 2:\n\nThis imagae is overly simipliefed in that a client left makes request through an api to a server/database then tthe server/database provides responses\nA client and server can exist on the same computer. This is often what‚Äôs happening in local development (e.g., querying a local database from R)\n\n\n\n\nLets go deepeer into understanding Define:\nClient (request) ‚Äì&gt; API ‚Äì&gt; Server ‚Äì&gt; Database\nClient &lt;‚Äì API &lt;‚Äì Server (response) &lt;‚Äì Database\n\n\n\nGATO365 API Request & Response\n\n\nQ2. Matching You might show this flow visually and say:\n‚ÄúThe [API] is the waiter.‚Äù\n‚ÄúThe [client] is the customer.‚Äù\n‚ÄúThe [server] is the kitchen.‚Äù\n‚ÄúThe [database] is the fridge or pantry.‚Äù\nNote 3:\n\nAction: Client makes a request\nAction: Server queries Database provides a response\n\n\n\n\nLets spend some more time on the request and response\nThe client sends a request asking for info (like taylor swift or today‚Äôs weather). This request includes:\n\nA URL (e.g., with parameters like ?q=San+Luis+Obispo)\nPossibly an API key\nA method (e.g., GET or POST)\n\nThe server then returns a response which contains:\n\ndata (temperature, artist name, forecast, etc.)\nmetadata (This is information about the response.)\nstatus code (Tells you whether the request was successful)\n\nThis information is traditionally provided in JSON Format.\n\n** Server Response  *************************\n\n\n\nLet‚Äôs focus on what the response is 1st (what we receive from the server):\nBelow is an example GIF of the information senr from the server in JSON format:\n\n\n\nGATO365 Anatomy JSON\n\n\nNote 4:\n\nWhen we send a request to an API, we get a response body, which includes the content ‚Äî typically JSON ‚Äî divided into data (what we wanted), metadata (info about the data), and a status_code telling us if the request worked.\n\n\n\n\nStatus codes tell you what happened with your request:\n-   `100s`: Info\n-   `200s`: Success (highlight: 200 OK)\n-   `300s`: Redirect\n-   `400s`: Client error\n-   `500s`: Server error\nNote 5:\n\nEmphasize: In most data APIs, your goal is to get a 200 response.\nUse examples like making up a nonexistent city or artist to show how an API might respond with a 400 or 404.\n\n\n** Client Request  *************************\n\n\n\nWhat type of client requests can we make?\nCRUD Framework (Create, Read, Update, Delete)\n\nThough APIs allow all four, Read (GET) is most common in data science.\nRESTful API mapping:\n\nCreate ‚Üí POST\nRead ‚Üí GET\nUpdate ‚Üí PUT/PATCH\nDelete ‚Üí DELETE\n\n\n(TODO: Create a GIF for each the above that is really illuminating, so GET and POST GIFs, its created, you have to actual turn it into a gif becuase it is actual a video)\n\n\n\nGATO365 Get Request\n\n\nNote 6: (what is happeningn in GIF)\n\n\n\nGATO365 Post Request\n\n\nNote 7:\n\nWe‚Äôll focus mostly on GET, and occasionally show POST (e.g., retrieving personalized weather data).\nBriefly mention: Apps like Instagram or Facebook rely on all four CRUD operations‚Äîupdating posts, deleting comments, etc.\n\n\n\n\n\n[[Based on time do One of the three steps]]\n[[1. email attendees to go to the weather website and get API key or whatever information needed before the conference. Create a video that‚Äôs displaying how to do this]]\n[[1a. Discuss .Renviron.txt, how we use: to create and edit API key: usethis::edit_r_environ()]]\n[[1b. Use Sys.getenv(\"API_KEY\") to see API in console]]\n[[Note that you have to use the 1a to see the api key again]] Restart R\n[[2. have attendees get the key during the break session if they have not done so already]]\n[[3. use a common key, but tell them it is bad practice]]\n[[regardless of the decision made of the three options above have attendees store information in the environment file]]\n\nNote 8:\nThere are many ways of doing this, but I‚Äôm going to stick with using tidyverse functions.I‚Äôm going to show you two ways to actually implement the query using the one way of one of the ways of doing this within a tiny verse using string glue\n\n\n\n\nso what we‚Äôre going to first do is create our response and the most ideal way.\nA request begins with a URL, which contains both:\n\nThe endpoint (base address of the API)\nThe query string (additional key-value pairs that modify the request)\n\nWe often need to glue strings together to build this full URL dynamically.\nA request is not ‚Äúautomatically‚Äù turned into JSON when sent ‚Äî it‚Äôs the response that‚Äôs usually formatted as JSON. The request is often URL-encoded if it‚Äôs a GET.\n\n\n\n\n\nWhen we use a URL like ...?q=San+Luis+Obispo&appid=..., we‚Äôre constructing a query string, which is appended to the base URL.\nThink of this as ‚Äúasking the question‚Äù‚Äîthe query string shapes the request.\nThe server receives the request, processes it, and responds with structured data (typically JSON).\nWe‚Äôre not sending JSON in this case‚Äîwe‚Äôre sending a URL with parameters. JSON is returned to us as a response format.\n\n Note 9: (WHat is going on in GIF)\n\n\n\n\n\n\n\nlibrary(httr2)       # Makes web requests\nlibrary(glue)        # Glue Strings\n\n\ncity_name &lt;- \"San Luis Obispo\"\n\nNote 10: Packages Needed, Lets focus on San Luis Obispo\n\ncurrent_weather_url &lt;- glue(\"https://api.openweathermap.org/data/2.5/weather?\",\n                            \"q=\", URLencode(city_name),\n                            \"&appid=\", Sys.getenv(\"API_KEY\"),\n                            \"&units=imperial\")\n\nNote 11: explicily state what each element is\n\ncurrent_weather_url\n\nNote 12: See what is glued together\n\nreq &lt;- request(current_weather_url)\n\nBuilt Request Object\n\nreq\n\nNote 13: Look at the request\nSelf Question make sure request is a URL\n\nThis method shows the anatomy of the URL explicitly.\nGreat for emphasizing how query parameters are constructed using strings.\nHelps reinforce the idea of ‚Äúasking a question via the URL.‚Äù\n\nNote 14:\n\nWe are going to do it again in a different way but we are goint to process the response further here becasue\n\nI wanted you to understand the anatomy of the URL\nHave multiple ways of doing the same thing\n\n\n\n\n\nStep 1: Build Request Object\n\nreq &lt;- request(\"https://api.openweathermap.org/data/2.5/weather\") |&gt; \n  req_url_query(\n    q = city_name,\n    appid = Sys.getenv(\"API_KEY\"),\n    units = \"imperial\"\n  )\n\n\nreq\n\nNote 15:\n\nThis method abstracts away the string building.\nIt‚Äôs cleaner and reduces chances of typos or formatting errors.\nTeaches students to treat query arguments like named inputs.\nYou can still inspect the built URL using req$url.\n\nStep 2: Make request\n\nresponse &lt;- req_perform(req)\n\n\nresponse\n\nNote 16: (explain the response)\nNot a step: View content Type\n\ncontent_type &lt;- resp_content_type(response)\n\n\ncontent_type\n\nNote 17: (Just observing the type of information given)\n\nlibrary(dplyr)\n\nNote 18: (We nee to select variable so we load the dplyr library)\nStep 3: Process the Response \n\n## IF the status code is 200 we are good\nif (resp_status(response) == 200) {\n  \n  # Parse JSON\n  result &lt;- resp_body_json(response)\n  \n  # Print Results as JSON\n  print(result)\n  \n  #---------------------------------------\n  \n  # Convert to Data Frame directly\n  current_weather_df &lt;- as.data.frame(result)\n  \n  # Print Results as Data Frame, using dplyr\n  print(select(current_weather_df, name, coord.lon, coord.lat, weather.main, main.temp))\n  \n  \n## ELSE state there is an Error\n} else {\n  cat(\"Failed. Status code:\", resp_status(response), \"\\n\")\n}\n\nNote 19:\n\nThere are many other variable that is given but we focus on\nThere is more information that is given but we are interested in the body of the request, hence we use resp_body_json that takes the body and that is what we are after as a json and we then convert it into a data frame\nTalk about error handling wiith the else statment above\n\n\n\n\n\n\n\n\n(TODO: Remove certain element of functions that are needed to understand the function) (TODO: Break up functions 1 and 2, to be better compartmentalized)\n\n## Step 1: Define function \"get_city_coords\" that accepts the parameter \"city\"\nget_city_coords &lt;- function(city){\n  \n## Step 2: Create API request URL\n  \n  geo_url &lt;- glue(\n  \"http://api.openweathermap.org/geo/1.0/direct?\",\n  \"q=\", URLencode(city),\n  \"&limit=1&appid=\", Sys.getenv(\"API_KEY\")\n)\n ## Step 3: Use req_perform() and request() to call the API with the URL request \n\ngeo_response &lt;- req_perform(request(geo_url))\n  \n\n## Step 4: If the status code is 200 (OK), use resp_body_json() to parse our response and as.data.frame to coerce it to data.frame.\n\nif (resp_status(geo_response) == 200) {\n  geo_data_df &lt;- resp_body_json(geo_response) |&gt; \n    as.data.frame()\n  \n  \n  ## Step 5: Assess if the output has 0 length, meaning no result. If so, stop and display an error message.  \n  \n  if (length(geo_data_df) == 0) {\n    stop(\"City not found. Please check the city name.\")\n  }\n  \n  ## Step 6: Assign latitude and longitude to variables, and use round() to clip it down to 2 decimal places.\n  mod_1_geo_data_df &lt;- geo_data_df |&gt; \n    mutate(lat = round(lat,2),\n           lon = round(lon,2))\n\n  ## Step 7: Select Certain Columns (chaptgpt)\n  mod_2_geo_data_df &lt;- mod_1_geo_data_df |&gt; \n    select(country,name,lat,lon) |&gt; \n    rename(city = name)\n  \n  ## Step 8: Return data frame with the country, city name and latitude / longitude.  \n  return(mod_2_geo_data_df)\n  \n  \n   }\n}\n\nNote 20:\nExplain the function and fill in blanks Emphasize error handling\n\n\n\nget_city_coords(city_name)\n\nNote 21: Explain output\n\n\n\n\nlibrary(purrr)\n\n# List of cities you want to geocode\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n\n# Use walk() from purrr to apply the function to each city\nmap_df(cities, get_city_coords)\n\nNote 22: Exaplin Task Explain map_df Explain output\n\n\n\n\n\n\n\n(TODO: Remove certain element of functions that are needed to understand the function) (highltight the function days in this)\n\nlibrary(lubridate)   # Time and date handling\n\nNote 23: Explain need for lubridate Emphasize the new endpoint (https://api.openweathermap.org/data/3.0/onecall/day_summary?) an how it is a paid subscription\n\nget_past_weather_by_city &lt;- function(city, days) {\n  # Step 1: Get city coordinates\n  coords_df &lt;- get_city_coords(city)\n  lat &lt;- coords_df$lat\n  lon &lt;- coords_df$lon\n  \n  cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n  \n  # Step 2: Create vector of past dates\n  date_range &lt;- as.character(today() - days(1:days))\n  \n  # Step 3: Define function for single date\n  fetch_day_summary &lt;- function(date) {\n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", date,\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\"\n    )\n    \n    response &lt;- req_perform(request(weather_url))\n    \n    if (resp_status(response) == 200) {\n      resp_body_json(response) |&gt; \n        as.data.frame() |&gt; \n        mutate(city = city, date = date)\n    } else {\n      warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(response)))\n      return(NULL)\n    }\n  }\n  \n  # Step 4: Map over date_range and bind into a single data frame\n  map_dfr(date_range, fetch_day_summary)\n}\n\nNote 24: (Explanation of filling out code and what code does)\n\nget_past_weather_by_city(city_name, 5)\n\nNote 25: Explain code and output\n\n\n\nnum_days &lt;- 5\n\n# Get historical weather data for each city using map_dfr\nall_weather_df &lt;- map_dfr(\n  cities,\n  ~ get_past_weather_by_city(.x, num_days)\n)\n\nNote 26: (Explanation of map_dfr and each element of input)\n\n\n\n\n\n\n\nPurpose: Retrieves current weather conditions for a single city. Demonstrates basic GET usage and parsing a flat JSON structure.\n(TODO: Highlight ENdpoint: https://api.openweathermap.org/data/2.5/weather?)\n\nget_city_current_weather &lt;- function(city) {\n  url &lt;- glue::glue(\n    \"https://api.openweathermap.org/data/2.5/weather?\",\n    \"q=\", URLencode(city),\n    \"&appid=\", Sys.getenv(\"API_KEY\"),\n    \"&units=imperial\"\n  )\n  \n  response &lt;- request(url) |&gt; req_perform()\n  \n  if (resp_status(response) == 200) {\n    response |&gt; \n      resp_body_json() |&gt; \n      purrr::pluck(\"main\") |&gt; \n      tibble::as_tibble() |&gt; \n      dplyr::select(temp, humidity) |&gt; \n      dplyr::mutate(\n        city = city,\n        description = resp_body_json(response) |&gt; purrr::pluck(\"weather\", 1, \"description\")\n      ) |&gt; \n      dplyr::select(city, temp, humidity, description)\n  } else {\n    warning(\"Failed to retrieve current weather for \", city)\n    return(NULL)\n  }\n}\n\n\n\n\n\nget_city_current_weather(\"Atlanta\")\n\n\n\n\n(TODO: MAke it different ciites)\n\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n\nweather_df &lt;- purrr::map_dfr(cities, get_city_current_weather)\n\n\n\n\n\n\n\n\nPurpose: Retrieves the next 5 days of forecast data (in 3-hour intervals). This introduces nested lists and flattening structures.\nNote: we are now getting times as well (TODO: Highlight ENdpoint: https://api.openweathermap.org/data/2.5/forecast?)\n\nget_city_forecast_5day &lt;- function(city) {\n  url &lt;- glue::glue(\n    \"https://api.openweathermap.org/data/2.5/forecast?\",\n    \"q=\", URLencode(city),\n    \"&appid=\", Sys.getenv(\"API_KEY\"),\n    \"&units=imperial\"\n  )\n  \n  response &lt;- httr2::req_perform(httr2::request(url))\n  \n  if (httr2::resp_status(response) == 200) {\n  response |&gt; \n      resp_body_json() |&gt; \n      purrr::pluck(\"list\") |&gt; \n      purrr::map_dfr(\n        ~ tibble::tibble(\n            city = city,\n            timestamp = .x$dt_txt,\n            temp = .x$main$temp,\n            weather = .x$weather |&gt; purrr::pluck(1, \"description\")\n        )\n      )\n  } else {\n    warning(\"Failed to retrieve forecast for \", city)\n    return(NULL)\n  }\n}\n\n\n\n\n\nget_city_forecast_5day(\"Atlanta\")\n\n\n\n\n(TODO: MAke it different ciites)\n\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n\nforecast_df &lt;- purrr::map_dfr(cities, get_city_forecast_5day)\n\n\n\n\n\n\n\n\nPurpose: Uses lat and lon to query current air quality. Demonstrates chaining of API requests (e.g., using get_city_coords() first), and different JSON structures.\n(TODO: Highlight ENdpoint: http://api.openweathermap.org/data/2.5/air_pollution?)\n\nget_air_pollution_by_coords &lt;- function(lat, lon) {\n  url &lt;- glue::glue(\n    \"http://api.openweathermap.org/data/2.5/air_pollution?\",\n    \"lat=\", lat,\n    \"&lon=\", lon,\n    \"&appid=\", Sys.getenv(\"API_KEY\")\n  )\n  \n  response &lt;- request(url) |&gt; req_perform()\n  \n  if (resp_status(response) == 200) {\n    response |&gt;\n      resp_body_json() |&gt;\n      purrr::pluck(\"list\", 1) |&gt;\n      {\\(x) tibble::tibble(\n        aqi = x$main$aqi,\n        co = x$components$co,\n        pm2_5 = x$components$pm2_5,\n        pm10 = x$components$pm10\n      )}()\n  } else {\n    warning(\"Failed to retrieve air pollution data for lat = \", lat, \", lon = \", lon)\n    return(NULL)\n  }\n}\n\n\n\n\nStep-by-Step for Usage with a Data Frame, use get_city_coords to get the cities of lon and lat.\n(TODO: MAke it different ciites)\n\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\ncity_coords_df &lt;- map_dfr(cities, get_city_coords)\n\n\n\n\nGet Air Pollution Data for All Cities\n\nlibrary(tidyr)\n\n\npollution_df &lt;- city_coords_df |&gt; \n  mutate(\n    pollution = map2(lat, lon, get_air_pollution_by_coords)\n  ) |&gt; \n  unnest(pollution)"
  },
  {
    "objectID": "sessions/session_2/02_Extraction_Weather_Data_API.html#session-2-api-fundamentals",
    "href": "sessions/session_2/02_Extraction_Weather_Data_API.html#session-2-api-fundamentals",
    "title": "Session 2: Weather Data - OpenWeatherAPI",
    "section": "",
    "text": "Explain what an API is and how it supports data extraction Theoretical elements of API\nMake requests to a public API and interpret the JSON response\nUnderstand and apply HTTP status codes and API keys\nWrite clean, readable code to extract and parse API data\n\n(TODO: Change the based on concept Foundation) (Mention Querying data base more as an action)\n\n\n\nPart A. Theoretical ideas of APIs\nNote 1:\n\nThis is not a webdeveloper nor a CS course but with a decent understanding of the logic, you and your students will appreciate the utilizartion of web scrapiing more\n\n\n\nWhat is an API? (Again)\nIt is the abiluty for software to communicate\n\n\n\nAPI Call (PhoenixNap.com)\n\n\n\nQ1: What is its utility of APIs? (multiple choice)\n\nNote 2:\n\nThis imagae is overly simipliefed in that a client left makes request through an api to a server/database then tthe server/database provides responses\nA client and server can exist on the same computer. This is often what‚Äôs happening in local development (e.g., querying a local database from R)\n\n\n\n\nLets go deepeer into understanding Define:\nClient (request) ‚Äì&gt; API ‚Äì&gt; Server ‚Äì&gt; Database\nClient &lt;‚Äì API &lt;‚Äì Server (response) &lt;‚Äì Database\n\n\n\nGATO365 API Request & Response\n\n\nQ2. Matching You might show this flow visually and say:\n‚ÄúThe [API] is the waiter.‚Äù\n‚ÄúThe [client] is the customer.‚Äù\n‚ÄúThe [server] is the kitchen.‚Äù\n‚ÄúThe [database] is the fridge or pantry.‚Äù\nNote 3:\n\nAction: Client makes a request\nAction: Server queries Database provides a response\n\n\n\n\nLets spend some more time on the request and response\nThe client sends a request asking for info (like taylor swift or today‚Äôs weather). This request includes:\n\nA URL (e.g., with parameters like ?q=San+Luis+Obispo)\nPossibly an API key\nA method (e.g., GET or POST)\n\nThe server then returns a response which contains:\n\ndata (temperature, artist name, forecast, etc.)\nmetadata (This is information about the response.)\nstatus code (Tells you whether the request was successful)\n\nThis information is traditionally provided in JSON Format.\n\n** Server Response  *************************\n\n\n\nLet‚Äôs focus on what the response is 1st (what we receive from the server):\nBelow is an example GIF of the information senr from the server in JSON format:\n\n\n\nGATO365 Anatomy JSON\n\n\nNote 4:\n\nWhen we send a request to an API, we get a response body, which includes the content ‚Äî typically JSON ‚Äî divided into data (what we wanted), metadata (info about the data), and a status_code telling us if the request worked.\n\n\n\n\nStatus codes tell you what happened with your request:\n-   `100s`: Info\n-   `200s`: Success (highlight: 200 OK)\n-   `300s`: Redirect\n-   `400s`: Client error\n-   `500s`: Server error\nNote 5:\n\nEmphasize: In most data APIs, your goal is to get a 200 response.\nUse examples like making up a nonexistent city or artist to show how an API might respond with a 400 or 404.\n\n\n** Client Request  *************************\n\n\n\nWhat type of client requests can we make?\nCRUD Framework (Create, Read, Update, Delete)\n\nThough APIs allow all four, Read (GET) is most common in data science.\nRESTful API mapping:\n\nCreate ‚Üí POST\nRead ‚Üí GET\nUpdate ‚Üí PUT/PATCH\nDelete ‚Üí DELETE\n\n\n(TODO: Create a GIF for each the above that is really illuminating, so GET and POST GIFs, its created, you have to actual turn it into a gif becuase it is actual a video)\n\n\n\nGATO365 Get Request\n\n\nNote 6: (what is happeningn in GIF)\n\n\n\nGATO365 Post Request\n\n\nNote 7:\n\nWe‚Äôll focus mostly on GET, and occasionally show POST (e.g., retrieving personalized weather data).\nBriefly mention: Apps like Instagram or Facebook rely on all four CRUD operations‚Äîupdating posts, deleting comments, etc.\n\n\n\n\n\n[[Based on time do One of the three steps]]\n[[1. email attendees to go to the weather website and get API key or whatever information needed before the conference. Create a video that‚Äôs displaying how to do this]]\n[[1a. Discuss .Renviron.txt, how we use: to create and edit API key: usethis::edit_r_environ()]]\n[[1b. Use Sys.getenv(\"API_KEY\") to see API in console]]\n[[Note that you have to use the 1a to see the api key again]] Restart R\n[[2. have attendees get the key during the break session if they have not done so already]]\n[[3. use a common key, but tell them it is bad practice]]\n[[regardless of the decision made of the three options above have attendees store information in the environment file]]\n\nNote 8:\nThere are many ways of doing this, but I‚Äôm going to stick with using tidyverse functions.I‚Äôm going to show you two ways to actually implement the query using the one way of one of the ways of doing this within a tiny verse using string glue\n\n\n\n\nso what we‚Äôre going to first do is create our response and the most ideal way.\nA request begins with a URL, which contains both:\n\nThe endpoint (base address of the API)\nThe query string (additional key-value pairs that modify the request)\n\nWe often need to glue strings together to build this full URL dynamically.\nA request is not ‚Äúautomatically‚Äù turned into JSON when sent ‚Äî it‚Äôs the response that‚Äôs usually formatted as JSON. The request is often URL-encoded if it‚Äôs a GET.\n\n\n\n\n\nWhen we use a URL like ...?q=San+Luis+Obispo&appid=..., we‚Äôre constructing a query string, which is appended to the base URL.\nThink of this as ‚Äúasking the question‚Äù‚Äîthe query string shapes the request.\nThe server receives the request, processes it, and responds with structured data (typically JSON).\nWe‚Äôre not sending JSON in this case‚Äîwe‚Äôre sending a URL with parameters. JSON is returned to us as a response format.\n\n Note 9: (WHat is going on in GIF)\n\n\n\n\n\n\n\nlibrary(httr2)       # Makes web requests\nlibrary(glue)        # Glue Strings\n\n\ncity_name &lt;- \"San Luis Obispo\"\n\nNote 10: Packages Needed, Lets focus on San Luis Obispo\n\ncurrent_weather_url &lt;- glue(\"https://api.openweathermap.org/data/2.5/weather?\",\n                            \"q=\", URLencode(city_name),\n                            \"&appid=\", Sys.getenv(\"API_KEY\"),\n                            \"&units=imperial\")\n\nNote 11: explicily state what each element is\n\ncurrent_weather_url\n\nNote 12: See what is glued together\n\nreq &lt;- request(current_weather_url)\n\nBuilt Request Object\n\nreq\n\nNote 13: Look at the request\nSelf Question make sure request is a URL\n\nThis method shows the anatomy of the URL explicitly.\nGreat for emphasizing how query parameters are constructed using strings.\nHelps reinforce the idea of ‚Äúasking a question via the URL.‚Äù\n\nNote 14:\n\nWe are going to do it again in a different way but we are goint to process the response further here becasue\n\nI wanted you to understand the anatomy of the URL\nHave multiple ways of doing the same thing\n\n\n\n\n\nStep 1: Build Request Object\n\nreq &lt;- request(\"https://api.openweathermap.org/data/2.5/weather\") |&gt; \n  req_url_query(\n    q = city_name,\n    appid = Sys.getenv(\"API_KEY\"),\n    units = \"imperial\"\n  )\n\n\nreq\n\nNote 15:\n\nThis method abstracts away the string building.\nIt‚Äôs cleaner and reduces chances of typos or formatting errors.\nTeaches students to treat query arguments like named inputs.\nYou can still inspect the built URL using req$url.\n\nStep 2: Make request\n\nresponse &lt;- req_perform(req)\n\n\nresponse\n\nNote 16: (explain the response)\nNot a step: View content Type\n\ncontent_type &lt;- resp_content_type(response)\n\n\ncontent_type\n\nNote 17: (Just observing the type of information given)\n\nlibrary(dplyr)\n\nNote 18: (We nee to select variable so we load the dplyr library)\nStep 3: Process the Response \n\n## IF the status code is 200 we are good\nif (resp_status(response) == 200) {\n  \n  # Parse JSON\n  result &lt;- resp_body_json(response)\n  \n  # Print Results as JSON\n  print(result)\n  \n  #---------------------------------------\n  \n  # Convert to Data Frame directly\n  current_weather_df &lt;- as.data.frame(result)\n  \n  # Print Results as Data Frame, using dplyr\n  print(select(current_weather_df, name, coord.lon, coord.lat, weather.main, main.temp))\n  \n  \n## ELSE state there is an Error\n} else {\n  cat(\"Failed. Status code:\", resp_status(response), \"\\n\")\n}\n\nNote 19:\n\nThere are many other variable that is given but we focus on\nThere is more information that is given but we are interested in the body of the request, hence we use resp_body_json that takes the body and that is what we are after as a json and we then convert it into a data frame\nTalk about error handling wiith the else statment above\n\n\n\n\n\n\n\n\n(TODO: Remove certain element of functions that are needed to understand the function) (TODO: Break up functions 1 and 2, to be better compartmentalized)\n\n## Step 1: Define function \"get_city_coords\" that accepts the parameter \"city\"\nget_city_coords &lt;- function(city){\n  \n## Step 2: Create API request URL\n  \n  geo_url &lt;- glue(\n  \"http://api.openweathermap.org/geo/1.0/direct?\",\n  \"q=\", URLencode(city),\n  \"&limit=1&appid=\", Sys.getenv(\"API_KEY\")\n)\n ## Step 3: Use req_perform() and request() to call the API with the URL request \n\ngeo_response &lt;- req_perform(request(geo_url))\n  \n\n## Step 4: If the status code is 200 (OK), use resp_body_json() to parse our response and as.data.frame to coerce it to data.frame.\n\nif (resp_status(geo_response) == 200) {\n  geo_data_df &lt;- resp_body_json(geo_response) |&gt; \n    as.data.frame()\n  \n  \n  ## Step 5: Assess if the output has 0 length, meaning no result. If so, stop and display an error message.  \n  \n  if (length(geo_data_df) == 0) {\n    stop(\"City not found. Please check the city name.\")\n  }\n  \n  ## Step 6: Assign latitude and longitude to variables, and use round() to clip it down to 2 decimal places.\n  mod_1_geo_data_df &lt;- geo_data_df |&gt; \n    mutate(lat = round(lat,2),\n           lon = round(lon,2))\n\n  ## Step 7: Select Certain Columns (chaptgpt)\n  mod_2_geo_data_df &lt;- mod_1_geo_data_df |&gt; \n    select(country,name,lat,lon) |&gt; \n    rename(city = name)\n  \n  ## Step 8: Return data frame with the country, city name and latitude / longitude.  \n  return(mod_2_geo_data_df)\n  \n  \n   }\n}\n\nNote 20:\nExplain the function and fill in blanks Emphasize error handling\n\n\n\nget_city_coords(city_name)\n\nNote 21: Explain output\n\n\n\n\nlibrary(purrr)\n\n# List of cities you want to geocode\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n\n# Use walk() from purrr to apply the function to each city\nmap_df(cities, get_city_coords)\n\nNote 22: Exaplin Task Explain map_df Explain output\n\n\n\n\n\n\n\n(TODO: Remove certain element of functions that are needed to understand the function) (highltight the function days in this)\n\nlibrary(lubridate)   # Time and date handling\n\nNote 23: Explain need for lubridate Emphasize the new endpoint (https://api.openweathermap.org/data/3.0/onecall/day_summary?) an how it is a paid subscription\n\nget_past_weather_by_city &lt;- function(city, days) {\n  # Step 1: Get city coordinates\n  coords_df &lt;- get_city_coords(city)\n  lat &lt;- coords_df$lat\n  lon &lt;- coords_df$lon\n  \n  cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n  \n  # Step 2: Create vector of past dates\n  date_range &lt;- as.character(today() - days(1:days))\n  \n  # Step 3: Define function for single date\n  fetch_day_summary &lt;- function(date) {\n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", date,\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\"\n    )\n    \n    response &lt;- req_perform(request(weather_url))\n    \n    if (resp_status(response) == 200) {\n      resp_body_json(response) |&gt; \n        as.data.frame() |&gt; \n        mutate(city = city, date = date)\n    } else {\n      warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(response)))\n      return(NULL)\n    }\n  }\n  \n  # Step 4: Map over date_range and bind into a single data frame\n  map_dfr(date_range, fetch_day_summary)\n}\n\nNote 24: (Explanation of filling out code and what code does)\n\nget_past_weather_by_city(city_name, 5)\n\nNote 25: Explain code and output\n\n\n\nnum_days &lt;- 5\n\n# Get historical weather data for each city using map_dfr\nall_weather_df &lt;- map_dfr(\n  cities,\n  ~ get_past_weather_by_city(.x, num_days)\n)\n\nNote 26: (Explanation of map_dfr and each element of input)\n\n\n\n\n\n\n\nPurpose: Retrieves current weather conditions for a single city. Demonstrates basic GET usage and parsing a flat JSON structure.\n(TODO: Highlight ENdpoint: https://api.openweathermap.org/data/2.5/weather?)\n\nget_city_current_weather &lt;- function(city) {\n  url &lt;- glue::glue(\n    \"https://api.openweathermap.org/data/2.5/weather?\",\n    \"q=\", URLencode(city),\n    \"&appid=\", Sys.getenv(\"API_KEY\"),\n    \"&units=imperial\"\n  )\n  \n  response &lt;- request(url) |&gt; req_perform()\n  \n  if (resp_status(response) == 200) {\n    response |&gt; \n      resp_body_json() |&gt; \n      purrr::pluck(\"main\") |&gt; \n      tibble::as_tibble() |&gt; \n      dplyr::select(temp, humidity) |&gt; \n      dplyr::mutate(\n        city = city,\n        description = resp_body_json(response) |&gt; purrr::pluck(\"weather\", 1, \"description\")\n      ) |&gt; \n      dplyr::select(city, temp, humidity, description)\n  } else {\n    warning(\"Failed to retrieve current weather for \", city)\n    return(NULL)\n  }\n}\n\n\n\n\n\nget_city_current_weather(\"Atlanta\")\n\n\n\n\n(TODO: MAke it different ciites)\n\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n\nweather_df &lt;- purrr::map_dfr(cities, get_city_current_weather)\n\n\n\n\n\n\n\n\nPurpose: Retrieves the next 5 days of forecast data (in 3-hour intervals). This introduces nested lists and flattening structures.\nNote: we are now getting times as well (TODO: Highlight ENdpoint: https://api.openweathermap.org/data/2.5/forecast?)\n\nget_city_forecast_5day &lt;- function(city) {\n  url &lt;- glue::glue(\n    \"https://api.openweathermap.org/data/2.5/forecast?\",\n    \"q=\", URLencode(city),\n    \"&appid=\", Sys.getenv(\"API_KEY\"),\n    \"&units=imperial\"\n  )\n  \n  response &lt;- httr2::req_perform(httr2::request(url))\n  \n  if (httr2::resp_status(response) == 200) {\n  response |&gt; \n      resp_body_json() |&gt; \n      purrr::pluck(\"list\") |&gt; \n      purrr::map_dfr(\n        ~ tibble::tibble(\n            city = city,\n            timestamp = .x$dt_txt,\n            temp = .x$main$temp,\n            weather = .x$weather |&gt; purrr::pluck(1, \"description\")\n        )\n      )\n  } else {\n    warning(\"Failed to retrieve forecast for \", city)\n    return(NULL)\n  }\n}\n\n\n\n\n\nget_city_forecast_5day(\"Atlanta\")\n\n\n\n\n(TODO: MAke it different ciites)\n\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n\nforecast_df &lt;- purrr::map_dfr(cities, get_city_forecast_5day)\n\n\n\n\n\n\n\n\nPurpose: Uses lat and lon to query current air quality. Demonstrates chaining of API requests (e.g., using get_city_coords() first), and different JSON structures.\n(TODO: Highlight ENdpoint: http://api.openweathermap.org/data/2.5/air_pollution?)\n\nget_air_pollution_by_coords &lt;- function(lat, lon) {\n  url &lt;- glue::glue(\n    \"http://api.openweathermap.org/data/2.5/air_pollution?\",\n    \"lat=\", lat,\n    \"&lon=\", lon,\n    \"&appid=\", Sys.getenv(\"API_KEY\")\n  )\n  \n  response &lt;- request(url) |&gt; req_perform()\n  \n  if (resp_status(response) == 200) {\n    response |&gt;\n      resp_body_json() |&gt;\n      purrr::pluck(\"list\", 1) |&gt;\n      {\\(x) tibble::tibble(\n        aqi = x$main$aqi,\n        co = x$components$co,\n        pm2_5 = x$components$pm2_5,\n        pm10 = x$components$pm10\n      )}()\n  } else {\n    warning(\"Failed to retrieve air pollution data for lat = \", lat, \", lon = \", lon)\n    return(NULL)\n  }\n}\n\n\n\n\nStep-by-Step for Usage with a Data Frame, use get_city_coords to get the cities of lon and lat.\n(TODO: MAke it different ciites)\n\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\ncity_coords_df &lt;- map_dfr(cities, get_city_coords)\n\n\n\n\nGet Air Pollution Data for All Cities\n\nlibrary(tidyr)\n\n\npollution_df &lt;- city_coords_df |&gt; \n  mutate(\n    pollution = map2(lat, lon, get_air_pollution_by_coords)\n  ) |&gt; \n  unnest(pollution)"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html",
    "href": "outlines_and_organization/unorganized-outline.html",
    "title": "Unorganized Thoughts About Workshop",
    "section": "",
    "text": "Deliverables: R Quarto Document\n\n\n\n8:30 - 10:15 Session 1\n10:15 - 10:45 Snack 1\n10:45 - 12:30 - Session 2\n12:30 -1:30 Lunch\n1:30 - 2:30 - Session 3\n2:30 - 3:30 - Snack 2\n3:30 - 4:15 - Session 4"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html#general-timeline",
    "href": "outlines_and_organization/unorganized-outline.html#general-timeline",
    "title": "Unorganized Thoughts About Workshop",
    "section": "",
    "text": "8:30 - 10:15 Session 1\n10:15 - 10:45 Snack 1\n10:45 - 12:30 - Session 2\n12:30 -1:30 Lunch\n1:30 - 2:30 - Session 3\n2:30 - 3:30 - Snack 2\n3:30 - 4:15 - Session 4"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html#part-1-outcomes-after-workshop",
    "href": "outlines_and_organization/unorganized-outline.html#part-1-outcomes-after-workshop",
    "title": "Unorganized Thoughts About Workshop",
    "section": "Part 1: Outcomes after workshop",
    "text": "Part 1: Outcomes after workshop\n\nJoin Newsletter and become subsribers\nPatrons puchchase product\n\n\nSubscription model where they pay $25 a month to have access information\nSeminars where I teach how to teach the material\nA textbook/virtual book\nSeminars at conferences"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html#part-2-how-to-have-great-workshopnotes-for-great-scots-presentation",
    "href": "outlines_and_organization/unorganized-outline.html#part-2-how-to-have-great-workshopnotes-for-great-scots-presentation",
    "title": "Unorganized Thoughts About Workshop",
    "section": "Part 2: How to have great workshopNotes for great Scots presentation",
    "text": "Part 2: How to have great workshopNotes for great Scots presentation\n\nMake clear goals in the beginning**\nMake activities relate to goals\nInclude time for them to do actual work\nSchedule in break time for talking about\nI will have a list of where they are coming from\nSend them message about prep\nSend them information about lelve of content to do so before hand via email\nPrepare for No Shows"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html#part-3-misc",
    "href": "outlines_and_organization/unorganized-outline.html#part-3-misc",
    "title": "Unorganized Thoughts About Workshop",
    "section": "Part 3: Misc",
    "text": "Part 3: Misc\n\nGet API Access Token, use univerisal API, to make sure process goes by fast\nCost of API, need to make sure they know it possible to get more info but it costs\nStatus codes, teach them about the extent of status codes that will leave them with a foundation\nThe guts of the HTML complexity really is within the code to make the data clean whereas the guts of the API complexity is how the JSON is brought into R that requires deeper thought and intention around\nwebsite/coursekata will house everything, this github will be for me to organize everything\nProcess of viewers to client should be obvious and easy done\nMaterials: 4 Separate quarto files ‚Äì&gt; 4 separate jupyter notebook (potentiall 6, two additional notebooks for activity, split class to do API and others do html so they do not feel overwhelm, bring together and how them presnt their finding and how it can be imlplemnted in tjheir classes)\nHave questions, discussion questions, best practice within each quarto\nCombine strategies, i.e web scrape for ice cream sales, sports, other, and pull in weather API to join and look for patterns, Transformations and visualizations for each\nBring variety of candy\nHvae prizes for raffle\n\nHave this paradigm within each session - Goals & Objectives - Theory (I do not know what should be here for the introduction & conclusion) - Practice (what we do with API & HTML extraction) - (I do not know what to put here)"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#deliverable",
    "href": "outlines_and_organization/organized-outline.html#deliverable",
    "title": "Organized Workshop Outline",
    "section": "Deliverable",
    "text": "Deliverable\n\nFinal output: R Quarto Document(s) (4‚Äì6 files aligned with each session and additional activities)"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#general-timeline",
    "href": "outlines_and_organization/organized-outline.html#general-timeline",
    "title": "Organized Workshop Outline",
    "section": "General Timeline",
    "text": "General Timeline\n\n8:30 ‚Äì 10:15 Session 1: Introduction to Data Extraction\n10:15 ‚Äì 10:45 Snack Break 1\n10:45 ‚Äì 12:30 Session 2: APIs and Practice\n12:30 ‚Äì 1:30 Lunch\n1:30 ‚Äì 2:30 Session 3: Web Scraping with HTML\n2:30 ‚Äì 3:30 Snack Break 2\n3:30 ‚Äì 4:15 Session 4: Deep Dives into Complete Lessons"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#session-1-introduction-to-data-extraction",
    "href": "outlines_and_organization/organized-outline.html#session-1-introduction-to-data-extraction",
    "title": "Organized Workshop Outline",
    "section": "Session 1: Introduction to Data Extraction",
    "text": "Session 1: Introduction to Data Extraction\n\nSet expectations and workshop goals\nWhy data extraction matters: relevance to real-world education\nOverview of the layout / table of contents\nDiscuss libraries used (tidyverse, rvest, httr, etc.)\nBest practices (e.g., avoiding hardcoding, consistent comments)\nAdapting to changing APIs/websites\nAnecdote: Spotify example of lost API access\nExplain tidy data: snake_case column names, correct data types\nEmphasize code flexibility ‚Äî developers can change APIs overnight\nActivity: Scaffolding + Code review using example(s)\n\n\nGoals & Objectives\n\nIdentify the value of real-world data in statistics education\nDescribe the distinction between extraction, transformation, and visualization (ETv)\nRecognize challenges associated with pulling live data from the web\nApply tidy data principles to imported datasets\n\n\n\nConceptual Foundation\n\nWhy use live data?\nWhat is extraction and why it matters for teaching modern statistics\nETv framework: introduction to the first stage (Extraction)\nImportance of code flexibility and the fragility of external sources (e.g., Spotify anecdote)\nTidy data principles: naming conventions, structure, and data types\n\n\n\nHands-On Coding Activity\n\nExtracting from accessible sources such as:\n\nA static .csv hosted online (warm-up)\nA Wikipedia table using rvest and janitor\n\nIntroduce read_csv() and rvest::html_table()\nAdd cleaning steps to enforce tidy principles (snake_case, correct types)\n\n\n\nReflection\n\nHow can you introduce real-world messiness without overwhelming students?\nHow would you scaffold tidy principles at the intro-level?"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#session-2-api-fundamentals",
    "href": "outlines_and_organization/organized-outline.html#session-2-api-fundamentals",
    "title": "Organized Workshop Outline",
    "section": "Session 2: API Fundamentals",
    "text": "Session 2: API Fundamentals\n\nWhat is an API? Examples (Spotify, Weather, one bonus example)\nBasic API structure: request URLs, endpoints, tokens\nHTTP protocols and status codes\nCRUD operations (Create, Read, Update, Delete)\nAPI best practices (e.g., pagination, authentication, caching)\nTidyverse-friendly workflows (avoid deep nesting, use readable steps)\nActivity:\n\nModify API request (e.g., hometown weather)\nScaffolded practice (fill-in-the-blank)\nOptional take-home transformation/visualization\n\n\n\nGoals & Objectives\n\nExplain what an API is and how it supports data extraction\nMake requests to a public API and interpret the JSON response\nUnderstand and apply HTTP status codes and API keys\nWrite clean, readable code to extract and parse API data\n\n\n\nConceptual Foundation\n\nRESTful APIs: endpoints, parameters, keys\nAuthentication: tokens, secrets, and environment variables\nStatus codes and error handling (focus on 200, 401, 403, 404)\nJSON structure: nested data and tidy conversion\n\n\n\nHands-On Coding Activity\n\nWeather API (e.g., OpenWeatherMap):\n\nRetrieve current weather for participant‚Äôs hometown\nModify query parameters (e.g., units, location)\nParse and visualize simple results (e.g., temperature, humidity)\n\nScaffold activity: prewritten functions + one blank section\n\n\n\nReflection\n\nWhere could API data naturally integrate in your curriculum?\nWhat are the pitfalls (rate limits, authentication) students need to know?"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#session-3-html-scraping-in-r",
    "href": "outlines_and_organization/organized-outline.html#session-3-html-scraping-in-r",
    "title": "Organized Workshop Outline",
    "section": "Session 3: HTML Scraping in R",
    "text": "Session 3: HTML Scraping in R\n\nWhat is HTML and why it‚Äôs useful?\nExamples: Wikipedia, sports sites (NFL, Olympics)\nStructured vs unstructured web data\nReading the webpage source and locating tables/divs\nPractice: Going back and forth between R and browser to inspect structure\nTidy HTML scraping practices using rvest and janitor\nDifferent approaches:\n\nFull walkthrough\nPartial scaffold\n\nActivity:\n\nScrape 2 sources (in pairs), compare\nClean the data: name 3 needed transformations\nUse visualization and interpretation\nDiscuss hardcoding and fragile selectors\n\n\n\nGoals & Objectives\n\nIdentify basic HTML structure relevant for scraping\nScrape tables and text from structured web pages\nClean scraped data using tidyverse tools\nCompare different websites in terms of data accessibility\n\n\n\nConceptual Foundation\n\nHTML basics: tags, attributes, structure of web tables\nUsing rvest to read web pages and extract data\nThe importance of inspecting elements with browser tools\nStructured vs.¬†unstructured sites: Wikipedia vs.¬†ESPN\n\n\n\nHands-On Coding Activity\n\nScrape sports statistics from a reliable table:\n\nExample: Wikipedia table of Olympic medal counts or NBA season stats\nClean using janitor::clean_names()\nCompare scraped data from 2 sites (optional pair task)\n\n\n\n\nReflection\n\nHow could students use scraped data in a final project?\nWhat scaffolds would help students inspect and trust their source?"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#session-4-deep-dives-into-lessons",
    "href": "outlines_and_organization/organized-outline.html#session-4-deep-dives-into-lessons",
    "title": "Organized Workshop Outline",
    "section": "Session 4: Deep Dives into Lessons",
    "text": "Session 4: Deep Dives into Lessons\n\nWork through 2 complete lessons ‚Äî each with:\n\nAPI-based extraction and visualization\nHTML-based extraction and transformation\n\nSplit class into two groups: API vs HTML, then reconvene\nHighlight pedagogical framing: how this can be implemented in class\nBuild reflection and discussion time: What will you bring into your course?\n\n\nGoals & Objectives\n\nReview key takeaways from API and HTML extraction\nCollaborate with peers on a structured mini-project\nReflect on how to implement extraction in your own course\nShare classroom-ready ideas with other educators\n\n\n\nRecap (Conceptual Foundation)\n\nExtraction is not ‚Äújust tech‚Äù ‚Äî it‚Äôs pedagogy\nAPI vs.¬†HTML: strengths, limitations, educational value\nDesigning learning activities around messy data: student engagement, real-world relevance\n\n\n\nHands-On Coding Activity\n\nParticipants are randomly assigned:\n\nGroup A: Use an API (weather, Spotify, etc.)\nGroup B: Scrape HTML data (sports, Wikipedia, etc.)\n\nWork in small groups to clean, transform, and visualize\nPrepare a brief ‚Äúteaching demo‚Äù of how this could be used in class\n\n\n\nDiscussion & Reflection\n\nWhat worked in your group?\nWhat teaching goals does this type of project help support?\nHow would you modify it for your students‚Äô level and context?"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#supporting-infrastructure",
    "href": "outlines_and_organization/organized-outline.html#supporting-infrastructure",
    "title": "Organized Workshop Outline",
    "section": "Supporting Infrastructure",
    "text": "Supporting Infrastructure\n\nUse recent versions of all packages\nAll materials hosted on:\n\nCourseKata for workshop delivery\nGitHub repo for behind-the-scenes development organization\n\nEach session includes:\n\nCode chunk scaffolds (empty/fill-in versions)\nSolution files\nPedagogical notes and discussion questions\n\nOutput files:\n\n4‚Äì6 R Quarto files\nMatching Jupyter notebooks for hands-on use\n\nBonus activities:\n\nCombine scraping + API for multi-source projects (e.g., sports + weather + sales)"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#workshop-strategy-logistics",
    "href": "outlines_and_organization/organized-outline.html#workshop-strategy-logistics",
    "title": "Organized Workshop Outline",
    "section": "Workshop Strategy & Logistics",
    "text": "Workshop Strategy & Logistics\n\nOutcomes After Workshop\n\nInvite participants to join a newsletter and mailing list\nPromote:\n\nMonthly subscription model ($25/month) for materials\nTeaching-focused seminars\nCulturally relevant virtual textbook\nConference workshops\n\n\n\n\nPlanning Tips for Success\n\nMake goals explicit from the start\nAlign each activity with stated goals\nInclude participant work time and discussions\nSchedule breaks for casual conversation and social connection\nKnow your audience (collect pre-attendance data)\nSend prep info (software setup, expectations) in advance\nAnticipate and plan for no-shows\n\n\n\nAdditional Notes\n\nProvide API tokens ahead of time (Spotify key for all)\nExplain API rate limits and possible costs\nTeach foundational status codes\nClarify complexity differences:\n\nHTML: Cleaning/structure focus\nAPI: Parsing/logic focus (JSON)\n\nRaffle and candy: build fun and engagement\nEncourage pair programming and peer instruction\nAllow participants to present their work at the end"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#rationale-for-combinations",
    "href": "outlines_and_organization/organized-outline.html#rationale-for-combinations",
    "title": "Organized Workshop Outline",
    "section": "Rationale for Combinations",
    "text": "Rationale for Combinations\n\nSession 1 & General Thoughts: Merged all teaching philosophy related to data extraction fundamentals here.\nAPI Examples & API Best Practices: Combined into Session 2 for cohesion and clarity.\nHTML Examples & Cleaning Strategies: Combined into Session 3 for a unified focus on web scraping.\nSession 4 & Misc Deep Dives: Naturally fit as a concluding session to synthesize all techniques and apply in pedagogically rich lessons.\nOutcomes, Planning, and Misc Strategy: Organized into coherent post-workshop and infrastructure support categories.\n\nLet me know if you want me to turn this into a .qmd, Google Doc, or a GitHub README.md."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html",
    "href": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html",
    "title": "Session 3: Extraction of NFL Data - HTML Scraping",
    "section": "",
    "text": "Identify basic HTML structure relevant for scraping\nScrape tables and text from structured web pages\nClean scraped data using tidyverse tools\nCompare different websites in terms of data accessibility\n\n\n\n\n\nHTML basics: tags, attributes, structure of web tables\nUsing rvest to read web pages and extract data\nThe importance of inspecting elements with browser tools\nStructured vs.¬†unstructured sites: Wikipedia vs.¬†ESPN\n\n\n\n\nlibrary(rvest)      # Web scraping\nlibrary(dplyr)      # Data manipulation\nlibrary(stringr)    # String cleaning\nlibrary(rlang)      # Advanced evaluation\nlibrary(purrr)      # Functional tools\nlibrary(ggplot2)    # Visualizations\n\nDiscussion: Why are some of these packages (like purrr or rlang) useful for scraping tasks?\n\n\n\n\nWe start by creating the target URL for a given team and year.\n\n# Step 2: Define team and year\nteam_name &lt;- \"was\"\nyear &lt;- 2023\n\n# Step 2a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n\nDiscussion: How could we make this part of a function so it is reusable?\nSee Also: https://www.pro-football-reference.com/teams/was/2023.htm#games\n\n\n\n\n\n# Step 3: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n\nWhen you run read_html(url), it returns an HTML node pointer, not human-readable content.\nThis pointer references the structure of the web page in memory, but doesn‚Äôt display actual text or data.\nIf you try to print this object directly, you‚Äôll see something such as:\nwebpage[[1]] &lt;pointer: 0x00000225...&gt;\n\n\n\nR is showing memory addresses of HTML elements, not the content.\nThis is because the HTML content must still be parsed or extracted.\n\nUse rvest!\n\nhtml_table() : extracts data from &lt;table&gt; elements.\nhtml_text() : extracts plain text from HTML nodes.\nhtml_nodes() or html_elements() : selects multiple nodes using CSS or XPath.\nhtml_element() : selects a single node.\n\nDiscussion: Why do you think web scraping tools separate ‚Äústructure‚Äù from ‚Äúcontent‚Äù? What are the pros and cons of working with HTML nodes directly?\nFrom the webpage, grab the HTML tables using rvest::html_table().\n\n# Step 3a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\nThe result is a list containing HTML table elements.\nDiscussion: What does this data structure look like?\nSelect the desired table, the 2023 Regular Season Table, which is the second table on the webpage. Use purrr::pluck() to select the table.\n\n\n\nThis is the table we are after\n\n\n\n# Step 3b: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n\nDiscussion: Why might this index (2) break in the future? What alternatives could we use to select the correct table more reliably?\n\n\n\n\nOur first row contains more information regarding the columns than the header of the actual table. The merged cells in the header end up being repeated over the entire column group they represent, without providing useful information.\n\n# Step 4a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\nDiscussion: Why can‚Äôt we use dplyr slice()?\n\n# Step 4b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n\n# Step 4c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n\n# Step 4d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\nBecause these columns are neither labeled in the first row or the header, we must manually assign them names.\n\n\n\nNotice the unlabeled columns?\n\n\n\n# Step 4e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n\nDiscussion: What are the risks or tradeoffs in hardcoding columns like result and game_location? How could this break?\n\n\n\n\nHere we will use dplyr select and filter to drop columns that are not relevant, as well as the Bye Week where the team does not play a game.\n\n# Step 5: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n\n# Step 5a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n\n# Step 5b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n\n# Step 5c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\nDiscussion: Why convert categorical variables like score_rslt or game_location to factors? What impact could that have on modeling or plotting?\n\n\n\n\nBy putting it all together, we can input a year for the Washington Commanders and get an extracted and cleaned table out.\n\n# Step 6: Year-only function\nwas_year &lt;- function(year) {\n  # Step 1: Define team and year\nteam_name &lt;- \"was\"\n\n# Step 1a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n  \n # Step 2: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n# Step 2a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\n# Step 3: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n # Step 3a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\n# Step 3b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n# Step 3c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n# Step 3d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\n# Step 3e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n# Step 4: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n# Step 4a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n# Step 4b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n# Step 4c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\n  return(table_7)\n}\n\nTest Year Only Function\n\nhead(was_year(2022))\n\n\n\n\nNow we will do the same task but while supplying team_name as a parameter as well as year.\n\n# Step 7: Generalized function\nfn_team_year &lt;- function(team_name, year) {\n\n# Step 2a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n  \n # Step 3: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n# Step 3a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\n# Step 3b: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n # Step 4a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\n# Step 4b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n# Step 4c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n# Step 4d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\n# Step 4e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n# Step 5: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n# Step 5a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n# Step 5b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n# Step 5c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\n  return(table_7)\n}\n\nTest Function (Team + Year)\n\nhead(fn_team_year(\"sfo\", 2024))\n\n\n\n\n\nUse ggplot2 to create simple and insightful visualizations.\n\n# Step 8: Line plot of points scored by Week\nggplot(fn_team_year(\"sfo\", 2024), aes(x = week, y = tm)) +\n  geom_line(color = \"steelblue\", linewidth = 1.2) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Points Scored Over Time\",\n    x = \"Week\",\n    y = \"Points Scored\"\n  ) +\n  theme_minimal()\n\n\n# Step 8a: Compare performance by game location\nggplot(fn_team_year(\"sfo\", 2024), aes(x = game_location, y = tm, fill = game_location)) +\n  geom_boxplot() +\n  labs(\n    title = \"Points Scored: Home vs Away\",\n    x = \"Location\",\n    y = \"Points Scored\"\n  ) +\n  theme_minimal()\n\nDiscussion: How might you visualize win/loss trends over the season? Could you include opponent information or passing yards?\nNow that you‚Äôre familiar with HTML elements and scraping, this activity will walk through extracting, cleaning, and visualizing NFL team performance data.\n\n\n\n\n\n\nScrape sports statistics from a reliable table:\n\nExample: Wikipedia table of Olympic medal counts or NBA season stats\nClean using janitor::clean_names()\nCompare scraped data from 2 sites (optional pair task)\n\n\n\n\n\n\nHow could students use scraped data in a final project?\nWhat scaffolds would help students inspect and trust their source?\n\n\n\n\n\nWhat is HTML and why it‚Äôs useful?\nExamples: Wikipedia, sports sites (NFL, Olympics)\nStructured vs unstructured web data\nReading the webpage source and locating tables/divs\nPractice: Going back and forth between R and browser to inspect structure\nTidy HTML scraping practices using rvest and janitor\nDifferent approaches:\n\nFull walkthrough\nPartial scaffold\n\nActivity:\n\nScrape 2 sources (in pairs), compare\nClean the data: name 3 needed transformations\nUse visualization and interpretation\nDiscuss hardcoding and fragile selectors"
  },
  {
    "objectID": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html#session-3-html-web-scraping---nfl-data-extraction",
    "href": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html#session-3-html-web-scraping---nfl-data-extraction",
    "title": "Session 3: Extraction of NFL Data - HTML Scraping",
    "section": "",
    "text": "Identify basic HTML structure relevant for scraping\nScrape tables and text from structured web pages\nClean scraped data using tidyverse tools\nCompare different websites in terms of data accessibility\n\n\n\n\n\nHTML basics: tags, attributes, structure of web tables\nUsing rvest to read web pages and extract data\nThe importance of inspecting elements with browser tools\nStructured vs.¬†unstructured sites: Wikipedia vs.¬†ESPN\n\n\n\n\nlibrary(rvest)      # Web scraping\nlibrary(dplyr)      # Data manipulation\nlibrary(stringr)    # String cleaning\nlibrary(rlang)      # Advanced evaluation\nlibrary(purrr)      # Functional tools\nlibrary(ggplot2)    # Visualizations\n\nDiscussion: Why are some of these packages (like purrr or rlang) useful for scraping tasks?\n\n\n\n\nWe start by creating the target URL for a given team and year.\n\n# Step 2: Define team and year\nteam_name &lt;- \"was\"\nyear &lt;- 2023\n\n# Step 2a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n\nDiscussion: How could we make this part of a function so it is reusable?\nSee Also: https://www.pro-football-reference.com/teams/was/2023.htm#games\n\n\n\n\n\n# Step 3: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n\nWhen you run read_html(url), it returns an HTML node pointer, not human-readable content.\nThis pointer references the structure of the web page in memory, but doesn‚Äôt display actual text or data.\nIf you try to print this object directly, you‚Äôll see something such as:\nwebpage[[1]] &lt;pointer: 0x00000225...&gt;\n\n\n\nR is showing memory addresses of HTML elements, not the content.\nThis is because the HTML content must still be parsed or extracted.\n\nUse rvest!\n\nhtml_table() : extracts data from &lt;table&gt; elements.\nhtml_text() : extracts plain text from HTML nodes.\nhtml_nodes() or html_elements() : selects multiple nodes using CSS or XPath.\nhtml_element() : selects a single node.\n\nDiscussion: Why do you think web scraping tools separate ‚Äústructure‚Äù from ‚Äúcontent‚Äù? What are the pros and cons of working with HTML nodes directly?\nFrom the webpage, grab the HTML tables using rvest::html_table().\n\n# Step 3a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\nThe result is a list containing HTML table elements.\nDiscussion: What does this data structure look like?\nSelect the desired table, the 2023 Regular Season Table, which is the second table on the webpage. Use purrr::pluck() to select the table.\n\n\n\nThis is the table we are after\n\n\n\n# Step 3b: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n\nDiscussion: Why might this index (2) break in the future? What alternatives could we use to select the correct table more reliably?\n\n\n\n\nOur first row contains more information regarding the columns than the header of the actual table. The merged cells in the header end up being repeated over the entire column group they represent, without providing useful information.\n\n# Step 4a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\nDiscussion: Why can‚Äôt we use dplyr slice()?\n\n# Step 4b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n\n# Step 4c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n\n# Step 4d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\nBecause these columns are neither labeled in the first row or the header, we must manually assign them names.\n\n\n\nNotice the unlabeled columns?\n\n\n\n# Step 4e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n\nDiscussion: What are the risks or tradeoffs in hardcoding columns like result and game_location? How could this break?\n\n\n\n\nHere we will use dplyr select and filter to drop columns that are not relevant, as well as the Bye Week where the team does not play a game.\n\n# Step 5: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n\n# Step 5a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n\n# Step 5b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n\n# Step 5c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\nDiscussion: Why convert categorical variables like score_rslt or game_location to factors? What impact could that have on modeling or plotting?\n\n\n\n\nBy putting it all together, we can input a year for the Washington Commanders and get an extracted and cleaned table out.\n\n# Step 6: Year-only function\nwas_year &lt;- function(year) {\n  # Step 1: Define team and year\nteam_name &lt;- \"was\"\n\n# Step 1a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n  \n # Step 2: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n# Step 2a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\n# Step 3: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n # Step 3a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\n# Step 3b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n# Step 3c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n# Step 3d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\n# Step 3e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n# Step 4: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n# Step 4a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n# Step 4b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n# Step 4c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\n  return(table_7)\n}\n\nTest Year Only Function\n\nhead(was_year(2022))\n\n\n\n\nNow we will do the same task but while supplying team_name as a parameter as well as year.\n\n# Step 7: Generalized function\nfn_team_year &lt;- function(team_name, year) {\n\n# Step 2a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n  \n # Step 3: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n# Step 3a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\n# Step 3b: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n # Step 4a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\n# Step 4b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n# Step 4c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n# Step 4d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\n# Step 4e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n# Step 5: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n# Step 5a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n# Step 5b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n# Step 5c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\n  return(table_7)\n}\n\nTest Function (Team + Year)\n\nhead(fn_team_year(\"sfo\", 2024))\n\n\n\n\n\nUse ggplot2 to create simple and insightful visualizations.\n\n# Step 8: Line plot of points scored by Week\nggplot(fn_team_year(\"sfo\", 2024), aes(x = week, y = tm)) +\n  geom_line(color = \"steelblue\", linewidth = 1.2) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Points Scored Over Time\",\n    x = \"Week\",\n    y = \"Points Scored\"\n  ) +\n  theme_minimal()\n\n\n# Step 8a: Compare performance by game location\nggplot(fn_team_year(\"sfo\", 2024), aes(x = game_location, y = tm, fill = game_location)) +\n  geom_boxplot() +\n  labs(\n    title = \"Points Scored: Home vs Away\",\n    x = \"Location\",\n    y = \"Points Scored\"\n  ) +\n  theme_minimal()\n\nDiscussion: How might you visualize win/loss trends over the season? Could you include opponent information or passing yards?\nNow that you‚Äôre familiar with HTML elements and scraping, this activity will walk through extracting, cleaning, and visualizing NFL team performance data.\n\n\n\n\n\n\nScrape sports statistics from a reliable table:\n\nExample: Wikipedia table of Olympic medal counts or NBA season stats\nClean using janitor::clean_names()\nCompare scraped data from 2 sites (optional pair task)\n\n\n\n\n\n\nHow could students use scraped data in a final project?\nWhat scaffolds would help students inspect and trust their source?\n\n\n\n\n\nWhat is HTML and why it‚Äôs useful?\nExamples: Wikipedia, sports sites (NFL, Olympics)\nStructured vs unstructured web data\nReading the webpage source and locating tables/divs\nPractice: Going back and forth between R and browser to inspect structure\nTidy HTML scraping practices using rvest and janitor\nDifferent approaches:\n\nFull walkthrough\nPartial scaffold\n\nActivity:\n\nScrape 2 sources (in pairs), compare\nClean the data: name 3 needed transformations\nUse visualization and interpretation\nDiscuss hardcoding and fragile selectors"
  },
  {
    "objectID": "sessions/session_1/01_Extraction_Introduction.html",
    "href": "sessions/session_1/01_Extraction_Introduction.html",
    "title": "Census API",
    "section": "",
    "text": "Use of CourseKata SPill (chatgpt - more info is needed)\nOrganization of workshop of lecture and questions throughout\n\nParts\nQuestions\nThere are your:\n\nQuestions\nBugs/Errors\n\n\n\n\n\n\nSession 1: Introduction\nUnderstand the importance of extracting dynamic data (via HTML and APIs) in modern data analysis and teaching\nSession 2: Getting Weather Data via OpenWeather API\nIn this session, we dive into OpenWeather API and learn to use packages like httr2 to execute API calls. We will also discuss URLs, queries, data structures, and more.\nSession 3: Scraping NFL Sports Data\nIn this session, we will use Pro-Football Reference to learn how to extract and clean HTML table data for use in statistical analysis and visualizations.\nSession 4: Putting it All Together (Project)\nIn this project, we will use HTML scraping joined with the OpenWeather API to create our own cloropleth map of Iowa.\n\n\n\nChloropleth Map (geoapify.com)\n\n\n\n\n\nAnalyzed static player statistics by loading an Excel file into R to filter the data and create a comparative boxplot.\nIntroduced dynamic data extraction by explaining how to use web APIs to send a request containing a query for structured JSON data from external servers.\nDemonstrated web scraping by using an R package to directly extract a data table from an HTML webpage.\nAdvocated for a modern educational approach that teaches students to actively find and extract live data rather than passively using clean, static files.\n\n#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n\n\n\n\n\n\n\nMy mentor, Allan, says ‚Äúask good questions‚Äù‚Ä¶\nStatistical Question: Who had the most impactful first season in terms of points: Michael Jordan, LeBron James, or Kobe Bryant?\nLet‚Äôs answer this question.\nI recently submitted a manuscript on this exact dataset, so let‚Äôs use it as our starting point.\nWe‚Äôll begin by working with a static excel file, named nba_data.xlsx, that contains per-game stats for each player‚Äôs 15 seasons in the NBA.\n\n\n\nLet‚Äôs Load the pertinent libraries\nTask 1: ____ Fill in the code based common tidyverse packages.\n\n\n## FILLED VERSION\nlibrary(readxl)      ## Data Extraction      --- E\nlibrary(dplyr)       ## Data Transformation  --- T\nlibrary(ggplot2)     ## Data Visualization   --- V\n\n\n## EMPTY VERSION\n# library(____)      ## Data Extraction      --- E\n# library(____)       ## Data Transformation  --- T\n# library(____)     ## Data Visualization   --- V\n\n\nTask 2. ____ Complete the appropriate function name and fill in the file name into below.\n\n\n## FILLED VERSION\nnba_df &lt;- read_xlsx(\"nba_data.xlsx\", sheet = \"modern_nba_legends_08302019\")\n\n\n## EMPTY VERSION\n# nba_df &lt;- read_*(\"____\", sheet = \"modern_nba_legends_08302019\")\n\n\nLet‚Äôs view the data ‚Ä¶\nTask 3. ____ Use the glimpse function to view the data\n\n\n## FILLED VERSION\nglimpse(nba_df)\n\n\n## EMPTY VERSION\n\n\n\n\nFirst, take a moment to look over the data yourself. Then, discuss with your peers if you see any issues that need cleaning.\n\n\n\nThe dataset is clean; the data types are properly assigned, with numeric variables stored as numbers and categorical variables stored as characters.\nNote 1: Look at the season variable\n\n\nNow let‚Äôs clean the focus on the data frame that we are after\nTask 4. ____ Use the filter() function to select only the rows where the Season column is equal to ‚Äúseason_1‚Äù.\n\n\n## FILLED VERSION\nseason_1_df &lt;- nba_df %&gt;% \n  filter(Season == \"season_1\")\n\n\n## EMPTY VERSION\n# season_1_df &lt;- nba_df %&gt;% \n#   ___( _____ == \"season_1\")\n\n\nNow lets look at a plot of their points to answer the statistical question\n\nTask 5. ____ Pipe the season_1_df data into ggplot, map the Name column to the x-axis and PTS to the y-axis, and then add a geom_boxplot() layer to visualize the data.\n\n## FILLED VERSION\nseason_1_df %&gt;% \n  ggplot(aes(x = Name, y = PTS)) +\n  geom_boxplot() +\n  theme_bw() \n\n\n## EMPTY VERSION\n# ____ %&gt;% \n#   ggplot(aes(x = ____, y = ____)) +\n#   geom_*() +\n#   theme_bw() \n\n\nNote 2: We could have spruced it up but here we just wanted to answer the question, if you have the urge please do so.\n\n\n\n\nWhat conclusions could be made about this plot?\n\n\n\nBased on the median values, Michael Jordan (MJ) had the most impressive 1st season, followed by LeBron James (LJ), and then Kobe Bryant (KB).\nThe plot also reveals that MJ‚Äôs scoring was the most variable and reached the highest peak, while KB‚Äôs point distribution was the lowest and most concentrated.\n\n\nNow what about, Magic Johnson or Wilt Chamberlain (historic players)\nMaybe Luka Donƒçiƒá or Ja Morant (more recent players)\nIf I wanted to add this data I need to go to the original source not an excel sheet to do this\n\n#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n\n\n\n\nShift students from being passive data users to active data seekers\nMove beyond the idea of ‚Äúwaiting for clean data‚Äù to learning how to access, validate, and clean it themselves\nTeach both the skill to extract and the capacity to teach extraction\n\nNote 3: (TODOs: Make this into a P and Note)\n\nWhy this matters: We as instructors should not just rely on pre-built packages or static datasets. The digital world changes constantly ‚Äî websites, APIs, and file structures evolve.\nOur responsibility: Teach students (and ourselves) how to adapt and access real-world data sources. Equip learners with skills to extract, not just consume pre-extracted content.\nDespite the growing importance of live data, most introductory courses still rely heavily on static, pre-cleaned datasets. This limits students‚Äô exposure to the realities of modern data work.\nKey idea: Flat files can still be dynamic depending on how they‚Äôre maintained ‚Äî but we use the term ‚Äúdynamic‚Äù here to emphasize external, real-time data access through APIs and web scraping.\nThe availability of dynamic, frequently updated data ‚Äî especially via web APIs ‚Äî has grown exponentially in recent years. This shift demands new strategies in how we teach data access.\n\n\n\n\n\n\n\n#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n\n\n\n2.a. Static Files or Sources Extraction\n[IMAGE of this]\n\nExamples: CSV, Excel files\nTypically unchanging unless manually edited\nOften pre-loaded into classroom activities\nMay still require cleaning (e.g., column names, missing data)\nNote 4:\n\nMessy data is not always a bad thing\n2.b. Dynamic Sources Extraction\n\nDefinition: Data sources that update over time or are externally controlled (i.e., you don‚Äôt own the source)\nTwo primary types:\n\nApplication Programming Interface APIs ‚Äì Designed to serve structured data upon request (e.g., player stats, weather)\n\n\n[IMAGE of this]\n2. Hypertext Markup Language **HTML/Web Pages** ‚Äì Seen as dynamic when content changes\n  (especially sports, news, etc.)\n[IMAGE of this]\n\nHTML pages are primarily designed for human readability, while APIs are designed for structured machine access. Both offer pathways to dynamic data, each with different advantages and challenges.\nBridging the gap between classroom exercises and real-world data practice requires that students learn not just how to analyze data ‚Äî but how to find it, extract it, and prepare it themselves.\nNote 5:\n\nHTML can be treated as static or dynamic depending on how frequently the page updates. For this workshop, we treat HTML as dynamic, especially for sports data.\n\n\n\n\n\n\n#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n\n\n\n\nThere are many kinds of APIs, but in this workshop, we‚Äôll focus specifically on web APIs ‚Äî tools designed to let us request and retrieve data from online sources.\nIn R, we‚Äôll act like a piece of software making those requests, allowing us to query live data programmatically.\n\n\n\n\nFlow of Data via API (vrogue.co)\n\n\n\nAPI stand for Application Programming Interfaces\nIt is a way for software to communicate with one another\nOne way it work is that it allow programs to request a query from a data base directly from external servers in a structured format (most often JSON).\n\n\n\n\n\n\n\n{\n  \"player\": \"LeBron James\",\n  \"points\": 27.1,\n  \"team\": \"Lakers\"\n}\n\nThe keys are players, pointsand team\nThe values for the corresponding keys are LeBron James, 27.1, Lakers\n\nNote 7:\n\nThere are a lot of acronyms\nJSON - Java Script Object Notation - javascript is web developing software (chatgpt)\n\n(Mention Querying data base more as an action)\nQ3. What does JSON? Answer: Java script object notation\nNote 8:\n\nDescribe Image Flow of Data via API (vrogue.co)\nA user sends a request via the internet ‚Üí the API talks to the server ‚Üí the server queries the database ‚Üí the API responds with data, often as JSON. (Mention Querying data base more as an action)\nP15.\nLearning to work with web APIs teaches students more than just how to extract data ‚Äî it gives them the tools to:\n\nLocate relevant APIs (e.g., weather, sports, music)\nConstruct and test their own API requests\nInterpret JSON responses (including nested structures)\nTransform the results into tidy formats ready for analysis\n\n\n(Mention Querying data base more as an action)\n\nP16\nAPIs aren‚Äôt just technical tools, they‚Äôre increasingly the primary way to access and query data stored in external databases.\nIn today‚Äôs fast-changing digital environment, students must be equipped to retrieve and work with information from live, external sources, not just rely on pre-cleaned datasets.\nNote 9:\n\nThis is what pushes students from passive observers of data to active agents in its collection, structure, and use. It aligns closely with what real-world data science jobs require, especially when you‚Äôre no longer just analyzing data, but acquiring it.\n\n\n\nVolume of Data Created (Statista)\n\n\nThe use of APIs requires keys, which are unique and secret codes that are used to authorize your request and identify your user and billing information. Consequently, keeping these codes secret is imperative. To do so, store API keys in environment files which reside on your computer, and not coded into variables or available in plain text on your working files.\n#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n\n\n\nIntroduction to tidycensus\nThe tidycensus package is a wrapper for the U.S. Census Bureau‚Äôs APIs. It is designed to make it simple to download, manage, and map Census data within R. It handles the API requests and returns clean, tidy data frames ready for analysis.\n\nStep 1: Get a Census API Key\nBefore using the API, you need a key. This is a simple, one-time process.\n\nGo to the Census API Key request page: https://api.census.gov/data/key_signup.html\nFill out the short form with your organization and email address.\nYour API key will be sent to your email almost immediately. Keep it handy.\n\n\nStep 2: Set Up Credentials\nThe tidycensus package includes a function to store your API key securely in the .Renviron file, so you only have to do this once per computer.\n\n## FILLED VERSION\n# install.packages(\"tidycensus\") # Run this once if needed\nlibrary(tidycensus)\n\n\n## EMPTY VERSION\n# install.packages(\"tidycensus\") # Run this once if needed\n# library(____)\n\n\n## FILLED VERSION\n# Replace \"YOUR_KEY_HERE\" with the key you received via email.\n# The `install = TRUE` argument saves it to your .Renviron file.\n\n# census_api_key(\"YOUR_KEY_HERE\", install = TRUE)\n\n\n## EMPTY VERSION\n# The `install = TRUE` argument saves it to your .Renviron file for future use.\n# census_api_key(\"____\", install = ____)\n\n[We are going to run this in a moment]\n‚ö†Ô∏è Crucially, you must restart your R session for the key to be available. Go to Session &gt; Restart R in RStudio. From now on, tidycensus will automatically find and use your key.\n\nStep 3: Load Required Packages\nFor this analysis, we‚Äôll need tidycensus for data retrieval and dplyr and ggplot2 for data wrangling and visualization.\n\n## FILLED VERSION\nlibrary(tidycensus)      ## Data Extraction     --- E\nlibrary(dplyr)           ## Data Transformation --- T\nlibrary(ggplot2)         ## Data Visualization  --- V\n\n\n## EMPTY VERSION\n# library(____)      ## Data Extraction     --- E\n# library(____)           ## Data Transformation --- T\n# library(____)         ## Data Visualization  --- V\n\n\nStep 4: Find Your Variables\nThe Census Bureau offers thousands of variables. A key step is finding the specific codes for the data you need. We can use the load_variables() function to search. Let‚Äôs find the variable code for ‚Äúmedian household income‚Äù in the 2022 American Community Survey (ACS) 5-year estimates.\n\n## FILLED VERSION\n# Load all variables from the 2022 5-year ACS dataset\nv22 &lt;- load_variables(2022, \"acs5\")\n## *********** Look at how many rows this data frame has    ************* ##\n\n\n# Search for the variable we want\nv22 %&gt;% \n  filter(grepl(\"Median Household Income\", label, ignore.case = TRUE))\n\n\n## EMPTY VERSION\n# Load all variables from the 2022 5-year ACS dataset\n# v22 &lt;- load_variables(____, \"acs5\")\n\n# Search for the variable we want by filling in the string below\n# v22 %&gt;% \n#   filter(grepl(\"____\", label, ignore.case = TRUE))\n\nThe search reveals that the variable code we want is B19013_001.\n\nStep 5: Request Census Data\nNow we use the main function, get_acs(), to download the data. We‚Äôll request the median household income for every county in Iowa.\n\n## FILLED VERSION\n# Request the data for Iowa counties\niowa_income_df &lt;- get_acs(\n  geography = \"county\",\n  variables = c(med_income = \"B19013_001\"), # Provide the variable code\n  state = \"IA\",\n  year = 2022\n)\n\n\n## EMPTY VERSION\n# Request the data for Iowa counties\n# iowa_income_df &lt;- get_acs(\n#   geography = \"____\",\n#   variables = c(med_income = \"____\"), # Provide the variable code\n#   state = \"____\",\n#   year = ____\n# )\n\n\nStep 6: Explore and Visualize the Dataset\nUse glimpse() to examine the data structure. You‚Äôll see it returns a tidy data frame with columns for the estimate and the margin of error (moe).\n\n## FILLED VERSION\nglimpse(iowa_income_df)\n\n\n## EMPTY VERSION\n# glimpse(____)\n\nNow, let‚Äôs create a simple plot of the 10 counties with the highest median income.\n\n## FILLED VERSION\niowa_income_df %&gt;%\n  slice_max(order_by = estimate, n = 10) %&gt;%\n  ggplot(aes(x = estimate, y = reorder(NAME, estimate))) +\n  geom_col(fill = \"dodgerblue\") +\n  labs(\n    title = \"Top 10 Iowa Counties by Median Household Income (2022)\",\n    x = \"Median Household Income (USD)\",\n    y = \"County\"\n  ) +\n  theme_minimal()\n\n\n## EMPTY VERSION\n# iowa_income_df %&gt;%\n#   slice_max(order_by = ____, n = ____) %&gt;%\n#   ggplot(aes(x = ____, y = reorder(NAME, ____))) +\n#   geom_col(fill = \"dodgerblue\") +\n#   labs(\n#     title = \"____\",\n#     x = \"____\",\n#     y = \"____\"\n#   ) +\n#   theme_minimal()\n\n[Make Question Transition to then Web Scrapping]\n\n\n\n\n\n\n#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n\n\n\n\n\n\nProcess of HTML Scraping (sstm2.com)\n\n\n\nP17:\nWebsites are structured using HTML (Hypertext Markup Language), which acts as the backbone for displaying and organizing content on the internet.\nWhen data is arranged in rows and columns‚Äîlike sports stats, schedules, or financial figures‚ÄîHTML tables offer a clear and structured way to present that information directly on the page.\nTables make it easy for both humans and computers to interpret patterns, compare values, and extract key insights.\n\nNote 12:\n\nWe don‚Äôt want to waste time copying and pasting tables into a CSV, then reformatting and cleaning them again in R.\nIdeally, we want to access the data directly and bring it into R in a structured format ‚Äî where we expect to do some cleaning, but we skip the unnecessary manual steps.\nP18:\n\nBelow is an image of code for html table and the actual table that it would produce\n{width=‚Äú200‚Äù, height = ‚Äú1000‚Äù}\n\nNote 13:\n\nHighlight the following concepts:\n\nbeginning and the end of table\nthe column name\neach Row\nHow it translate into a human readable table\n\nEmphasize that we‚Äôre only focusing on &lt;table&gt; tags for this workshop\n\nP19.\n\nNow lets see one of the libraries that allows us to scrape in R\nNote 14: You do not have to install this but it is not part of CRAN, it used to be, so you have to install it via this way using this repo\n\n# install.packages(\"devtools\")\n# devtools::install_github(\"crubba/htmltab\")\n\n\nlibrary(htmltab)\n\nNote 14: Lets go to the url via webbroswer\n\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n\nP20.\nThis function requires 2 args url and table number we can guess at it and may work\n\niowa_state_counties_df &lt;- htmltab(url,1)\n\n\niowa_state_counties_df &lt;- htmltab(url,2)\n\nNote 15:\n\nUnless you know html and want to look at the source code or you what exactly a table looks like you will have to guess sometimes\nWe can get the warning to go away by ‚Ä¶\nP21.\n\nThis is what I would call static because the counties are note changeing but if we wanted baseball data at which currently everyday new data is displayed it is ideal that we have a more robust method of fgetting this data rather using htmltabs\n\nP22.\n\nCheck out article: Web Scraping in the Statistics and Data Science Curriculum: Challenges and Opportunities - Mine Dogucu & Mine √áetinkaya-Rundel\n\nP23.\n\nMuch like APIs, lots of relevant and useful information is available directly on webpages, which are readable by humans rather than APIs which are designed for machine access.\nBy learning this skill, students are able to:\n\nLocate relevant sources (e.g., sports data from Pro Football Reference)\nUnderstand how websites deliver and organize content\nTransform and clean data for analysis and visualization\n\nOften times, HTML tables contain unexpected structures or data types (images, links, etc) and can present a challenge that develops not only data cleaning skills, but intention, planning, and adaptability when handling and analyzing difficult data."
  },
  {
    "objectID": "sessions/session_1/01_Extraction_Introduction.html#session-1-introduction-to-data-extraction",
    "href": "sessions/session_1/01_Extraction_Introduction.html#session-1-introduction-to-data-extraction",
    "title": "Census API",
    "section": "",
    "text": "Use of CourseKata SPill (chatgpt - more info is needed)\nOrganization of workshop of lecture and questions throughout\n\nParts\nQuestions\nThere are your:\n\nQuestions\nBugs/Errors\n\n\n\n\n\n\nSession 1: Introduction\nUnderstand the importance of extracting dynamic data (via HTML and APIs) in modern data analysis and teaching\nSession 2: Getting Weather Data via OpenWeather API\nIn this session, we dive into OpenWeather API and learn to use packages like httr2 to execute API calls. We will also discuss URLs, queries, data structures, and more.\nSession 3: Scraping NFL Sports Data\nIn this session, we will use Pro-Football Reference to learn how to extract and clean HTML table data for use in statistical analysis and visualizations.\nSession 4: Putting it All Together (Project)\nIn this project, we will use HTML scraping joined with the OpenWeather API to create our own cloropleth map of Iowa.\n\n\n\nChloropleth Map (geoapify.com)\n\n\n\n\n\nAnalyzed static player statistics by loading an Excel file into R to filter the data and create a comparative boxplot.\nIntroduced dynamic data extraction by explaining how to use web APIs to send a request containing a query for structured JSON data from external servers.\nDemonstrated web scraping by using an R package to directly extract a data table from an HTML webpage.\nAdvocated for a modern educational approach that teaches students to actively find and extract live data rather than passively using clean, static files.\n\n#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n\n\n\n\n\n\n\nMy mentor, Allan, says ‚Äúask good questions‚Äù‚Ä¶\nStatistical Question: Who had the most impactful first season in terms of points: Michael Jordan, LeBron James, or Kobe Bryant?\nLet‚Äôs answer this question.\nI recently submitted a manuscript on this exact dataset, so let‚Äôs use it as our starting point.\nWe‚Äôll begin by working with a static excel file, named nba_data.xlsx, that contains per-game stats for each player‚Äôs 15 seasons in the NBA.\n\n\n\nLet‚Äôs Load the pertinent libraries\nTask 1: ____ Fill in the code based common tidyverse packages.\n\n\n## FILLED VERSION\nlibrary(readxl)      ## Data Extraction      --- E\nlibrary(dplyr)       ## Data Transformation  --- T\nlibrary(ggplot2)     ## Data Visualization   --- V\n\n\n## EMPTY VERSION\n# library(____)      ## Data Extraction      --- E\n# library(____)       ## Data Transformation  --- T\n# library(____)     ## Data Visualization   --- V\n\n\nTask 2. ____ Complete the appropriate function name and fill in the file name into below.\n\n\n## FILLED VERSION\nnba_df &lt;- read_xlsx(\"nba_data.xlsx\", sheet = \"modern_nba_legends_08302019\")\n\n\n## EMPTY VERSION\n# nba_df &lt;- read_*(\"____\", sheet = \"modern_nba_legends_08302019\")\n\n\nLet‚Äôs view the data ‚Ä¶\nTask 3. ____ Use the glimpse function to view the data\n\n\n## FILLED VERSION\nglimpse(nba_df)\n\n\n## EMPTY VERSION\n\n\n\n\nFirst, take a moment to look over the data yourself. Then, discuss with your peers if you see any issues that need cleaning.\n\n\n\nThe dataset is clean; the data types are properly assigned, with numeric variables stored as numbers and categorical variables stored as characters.\nNote 1: Look at the season variable\n\n\nNow let‚Äôs clean the focus on the data frame that we are after\nTask 4. ____ Use the filter() function to select only the rows where the Season column is equal to ‚Äúseason_1‚Äù.\n\n\n## FILLED VERSION\nseason_1_df &lt;- nba_df %&gt;% \n  filter(Season == \"season_1\")\n\n\n## EMPTY VERSION\n# season_1_df &lt;- nba_df %&gt;% \n#   ___( _____ == \"season_1\")\n\n\nNow lets look at a plot of their points to answer the statistical question\n\nTask 5. ____ Pipe the season_1_df data into ggplot, map the Name column to the x-axis and PTS to the y-axis, and then add a geom_boxplot() layer to visualize the data.\n\n## FILLED VERSION\nseason_1_df %&gt;% \n  ggplot(aes(x = Name, y = PTS)) +\n  geom_boxplot() +\n  theme_bw() \n\n\n## EMPTY VERSION\n# ____ %&gt;% \n#   ggplot(aes(x = ____, y = ____)) +\n#   geom_*() +\n#   theme_bw() \n\n\nNote 2: We could have spruced it up but here we just wanted to answer the question, if you have the urge please do so.\n\n\n\n\nWhat conclusions could be made about this plot?\n\n\n\nBased on the median values, Michael Jordan (MJ) had the most impressive 1st season, followed by LeBron James (LJ), and then Kobe Bryant (KB).\nThe plot also reveals that MJ‚Äôs scoring was the most variable and reached the highest peak, while KB‚Äôs point distribution was the lowest and most concentrated.\n\n\nNow what about, Magic Johnson or Wilt Chamberlain (historic players)\nMaybe Luka Donƒçiƒá or Ja Morant (more recent players)\nIf I wanted to add this data I need to go to the original source not an excel sheet to do this\n\n#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n\n\n\n\nShift students from being passive data users to active data seekers\nMove beyond the idea of ‚Äúwaiting for clean data‚Äù to learning how to access, validate, and clean it themselves\nTeach both the skill to extract and the capacity to teach extraction\n\nNote 3: (TODOs: Make this into a P and Note)\n\nWhy this matters: We as instructors should not just rely on pre-built packages or static datasets. The digital world changes constantly ‚Äî websites, APIs, and file structures evolve.\nOur responsibility: Teach students (and ourselves) how to adapt and access real-world data sources. Equip learners with skills to extract, not just consume pre-extracted content.\nDespite the growing importance of live data, most introductory courses still rely heavily on static, pre-cleaned datasets. This limits students‚Äô exposure to the realities of modern data work.\nKey idea: Flat files can still be dynamic depending on how they‚Äôre maintained ‚Äî but we use the term ‚Äúdynamic‚Äù here to emphasize external, real-time data access through APIs and web scraping.\nThe availability of dynamic, frequently updated data ‚Äî especially via web APIs ‚Äî has grown exponentially in recent years. This shift demands new strategies in how we teach data access.\n\n\n\n\n\n\n\n#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n\n\n\n2.a. Static Files or Sources Extraction\n[IMAGE of this]\n\nExamples: CSV, Excel files\nTypically unchanging unless manually edited\nOften pre-loaded into classroom activities\nMay still require cleaning (e.g., column names, missing data)\nNote 4:\n\nMessy data is not always a bad thing\n2.b. Dynamic Sources Extraction\n\nDefinition: Data sources that update over time or are externally controlled (i.e., you don‚Äôt own the source)\nTwo primary types:\n\nApplication Programming Interface APIs ‚Äì Designed to serve structured data upon request (e.g., player stats, weather)\n\n\n[IMAGE of this]\n2. Hypertext Markup Language **HTML/Web Pages** ‚Äì Seen as dynamic when content changes\n  (especially sports, news, etc.)\n[IMAGE of this]\n\nHTML pages are primarily designed for human readability, while APIs are designed for structured machine access. Both offer pathways to dynamic data, each with different advantages and challenges.\nBridging the gap between classroom exercises and real-world data practice requires that students learn not just how to analyze data ‚Äî but how to find it, extract it, and prepare it themselves.\nNote 5:\n\nHTML can be treated as static or dynamic depending on how frequently the page updates. For this workshop, we treat HTML as dynamic, especially for sports data.\n\n\n\n\n\n\n#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n\n\n\n\nThere are many kinds of APIs, but in this workshop, we‚Äôll focus specifically on web APIs ‚Äî tools designed to let us request and retrieve data from online sources.\nIn R, we‚Äôll act like a piece of software making those requests, allowing us to query live data programmatically.\n\n\n\n\nFlow of Data via API (vrogue.co)\n\n\n\nAPI stand for Application Programming Interfaces\nIt is a way for software to communicate with one another\nOne way it work is that it allow programs to request a query from a data base directly from external servers in a structured format (most often JSON).\n\n\n\n\n\n\n\n{\n  \"player\": \"LeBron James\",\n  \"points\": 27.1,\n  \"team\": \"Lakers\"\n}\n\nThe keys are players, pointsand team\nThe values for the corresponding keys are LeBron James, 27.1, Lakers\n\nNote 7:\n\nThere are a lot of acronyms\nJSON - Java Script Object Notation - javascript is web developing software (chatgpt)\n\n(Mention Querying data base more as an action)\nQ3. What does JSON? Answer: Java script object notation\nNote 8:\n\nDescribe Image Flow of Data via API (vrogue.co)\nA user sends a request via the internet ‚Üí the API talks to the server ‚Üí the server queries the database ‚Üí the API responds with data, often as JSON. (Mention Querying data base more as an action)\nP15.\nLearning to work with web APIs teaches students more than just how to extract data ‚Äî it gives them the tools to:\n\nLocate relevant APIs (e.g., weather, sports, music)\nConstruct and test their own API requests\nInterpret JSON responses (including nested structures)\nTransform the results into tidy formats ready for analysis\n\n\n(Mention Querying data base more as an action)\n\nP16\nAPIs aren‚Äôt just technical tools, they‚Äôre increasingly the primary way to access and query data stored in external databases.\nIn today‚Äôs fast-changing digital environment, students must be equipped to retrieve and work with information from live, external sources, not just rely on pre-cleaned datasets.\nNote 9:\n\nThis is what pushes students from passive observers of data to active agents in its collection, structure, and use. It aligns closely with what real-world data science jobs require, especially when you‚Äôre no longer just analyzing data, but acquiring it.\n\n\n\nVolume of Data Created (Statista)\n\n\nThe use of APIs requires keys, which are unique and secret codes that are used to authorize your request and identify your user and billing information. Consequently, keeping these codes secret is imperative. To do so, store API keys in environment files which reside on your computer, and not coded into variables or available in plain text on your working files.\n#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n\n\n\nIntroduction to tidycensus\nThe tidycensus package is a wrapper for the U.S. Census Bureau‚Äôs APIs. It is designed to make it simple to download, manage, and map Census data within R. It handles the API requests and returns clean, tidy data frames ready for analysis.\n\nStep 1: Get a Census API Key\nBefore using the API, you need a key. This is a simple, one-time process.\n\nGo to the Census API Key request page: https://api.census.gov/data/key_signup.html\nFill out the short form with your organization and email address.\nYour API key will be sent to your email almost immediately. Keep it handy.\n\n\nStep 2: Set Up Credentials\nThe tidycensus package includes a function to store your API key securely in the .Renviron file, so you only have to do this once per computer.\n\n## FILLED VERSION\n# install.packages(\"tidycensus\") # Run this once if needed\nlibrary(tidycensus)\n\n\n## EMPTY VERSION\n# install.packages(\"tidycensus\") # Run this once if needed\n# library(____)\n\n\n## FILLED VERSION\n# Replace \"YOUR_KEY_HERE\" with the key you received via email.\n# The `install = TRUE` argument saves it to your .Renviron file.\n\n# census_api_key(\"YOUR_KEY_HERE\", install = TRUE)\n\n\n## EMPTY VERSION\n# The `install = TRUE` argument saves it to your .Renviron file for future use.\n# census_api_key(\"____\", install = ____)\n\n[We are going to run this in a moment]\n‚ö†Ô∏è Crucially, you must restart your R session for the key to be available. Go to Session &gt; Restart R in RStudio. From now on, tidycensus will automatically find and use your key.\n\nStep 3: Load Required Packages\nFor this analysis, we‚Äôll need tidycensus for data retrieval and dplyr and ggplot2 for data wrangling and visualization.\n\n## FILLED VERSION\nlibrary(tidycensus)      ## Data Extraction     --- E\nlibrary(dplyr)           ## Data Transformation --- T\nlibrary(ggplot2)         ## Data Visualization  --- V\n\n\n## EMPTY VERSION\n# library(____)      ## Data Extraction     --- E\n# library(____)           ## Data Transformation --- T\n# library(____)         ## Data Visualization  --- V\n\n\nStep 4: Find Your Variables\nThe Census Bureau offers thousands of variables. A key step is finding the specific codes for the data you need. We can use the load_variables() function to search. Let‚Äôs find the variable code for ‚Äúmedian household income‚Äù in the 2022 American Community Survey (ACS) 5-year estimates.\n\n## FILLED VERSION\n# Load all variables from the 2022 5-year ACS dataset\nv22 &lt;- load_variables(2022, \"acs5\")\n## *********** Look at how many rows this data frame has    ************* ##\n\n\n# Search for the variable we want\nv22 %&gt;% \n  filter(grepl(\"Median Household Income\", label, ignore.case = TRUE))\n\n\n## EMPTY VERSION\n# Load all variables from the 2022 5-year ACS dataset\n# v22 &lt;- load_variables(____, \"acs5\")\n\n# Search for the variable we want by filling in the string below\n# v22 %&gt;% \n#   filter(grepl(\"____\", label, ignore.case = TRUE))\n\nThe search reveals that the variable code we want is B19013_001.\n\nStep 5: Request Census Data\nNow we use the main function, get_acs(), to download the data. We‚Äôll request the median household income for every county in Iowa.\n\n## FILLED VERSION\n# Request the data for Iowa counties\niowa_income_df &lt;- get_acs(\n  geography = \"county\",\n  variables = c(med_income = \"B19013_001\"), # Provide the variable code\n  state = \"IA\",\n  year = 2022\n)\n\n\n## EMPTY VERSION\n# Request the data for Iowa counties\n# iowa_income_df &lt;- get_acs(\n#   geography = \"____\",\n#   variables = c(med_income = \"____\"), # Provide the variable code\n#   state = \"____\",\n#   year = ____\n# )\n\n\nStep 6: Explore and Visualize the Dataset\nUse glimpse() to examine the data structure. You‚Äôll see it returns a tidy data frame with columns for the estimate and the margin of error (moe).\n\n## FILLED VERSION\nglimpse(iowa_income_df)\n\n\n## EMPTY VERSION\n# glimpse(____)\n\nNow, let‚Äôs create a simple plot of the 10 counties with the highest median income.\n\n## FILLED VERSION\niowa_income_df %&gt;%\n  slice_max(order_by = estimate, n = 10) %&gt;%\n  ggplot(aes(x = estimate, y = reorder(NAME, estimate))) +\n  geom_col(fill = \"dodgerblue\") +\n  labs(\n    title = \"Top 10 Iowa Counties by Median Household Income (2022)\",\n    x = \"Median Household Income (USD)\",\n    y = \"County\"\n  ) +\n  theme_minimal()\n\n\n## EMPTY VERSION\n# iowa_income_df %&gt;%\n#   slice_max(order_by = ____, n = ____) %&gt;%\n#   ggplot(aes(x = ____, y = reorder(NAME, ____))) +\n#   geom_col(fill = \"dodgerblue\") +\n#   labs(\n#     title = \"____\",\n#     x = \"____\",\n#     y = \"____\"\n#   ) +\n#   theme_minimal()\n\n[Make Question Transition to then Web Scrapping]\n\n\n\n\n\n\n#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n\n\n\n\n\n\nProcess of HTML Scraping (sstm2.com)\n\n\n\nP17:\nWebsites are structured using HTML (Hypertext Markup Language), which acts as the backbone for displaying and organizing content on the internet.\nWhen data is arranged in rows and columns‚Äîlike sports stats, schedules, or financial figures‚ÄîHTML tables offer a clear and structured way to present that information directly on the page.\nTables make it easy for both humans and computers to interpret patterns, compare values, and extract key insights.\n\nNote 12:\n\nWe don‚Äôt want to waste time copying and pasting tables into a CSV, then reformatting and cleaning them again in R.\nIdeally, we want to access the data directly and bring it into R in a structured format ‚Äî where we expect to do some cleaning, but we skip the unnecessary manual steps.\nP18:\n\nBelow is an image of code for html table and the actual table that it would produce\n{width=‚Äú200‚Äù, height = ‚Äú1000‚Äù}\n\nNote 13:\n\nHighlight the following concepts:\n\nbeginning and the end of table\nthe column name\neach Row\nHow it translate into a human readable table\n\nEmphasize that we‚Äôre only focusing on &lt;table&gt; tags for this workshop\n\nP19.\n\nNow lets see one of the libraries that allows us to scrape in R\nNote 14: You do not have to install this but it is not part of CRAN, it used to be, so you have to install it via this way using this repo\n\n# install.packages(\"devtools\")\n# devtools::install_github(\"crubba/htmltab\")\n\n\nlibrary(htmltab)\n\nNote 14: Lets go to the url via webbroswer\n\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n\nP20.\nThis function requires 2 args url and table number we can guess at it and may work\n\niowa_state_counties_df &lt;- htmltab(url,1)\n\n\niowa_state_counties_df &lt;- htmltab(url,2)\n\nNote 15:\n\nUnless you know html and want to look at the source code or you what exactly a table looks like you will have to guess sometimes\nWe can get the warning to go away by ‚Ä¶\nP21.\n\nThis is what I would call static because the counties are note changeing but if we wanted baseball data at which currently everyday new data is displayed it is ideal that we have a more robust method of fgetting this data rather using htmltabs\n\nP22.\n\nCheck out article: Web Scraping in the Statistics and Data Science Curriculum: Challenges and Opportunities - Mine Dogucu & Mine √áetinkaya-Rundel\n\nP23.\n\nMuch like APIs, lots of relevant and useful information is available directly on webpages, which are readable by humans rather than APIs which are designed for machine access.\nBy learning this skill, students are able to:\n\nLocate relevant sources (e.g., sports data from Pro Football Reference)\nUnderstand how websites deliver and organize content\nTransform and clean data for analysis and visualization\n\nOften times, HTML tables contain unexpected structures or data types (images, links, etc) and can present a challenge that develops not only data cleaning skills, but intention, planning, and adaptability when handling and analyzing difficult data."
  },
  {
    "objectID": "Tasks_Lists.html#session-1-tasks",
    "href": "Tasks_Lists.html#session-1-tasks",
    "title": "Tasks List",
    "section": "Session 1 Tasks",
    "text": "Session 1 Tasks"
  },
  {
    "objectID": "Tasks_Lists.html#session-2-tasks",
    "href": "Tasks_Lists.html#session-2-tasks",
    "title": "Tasks List",
    "section": "Session 2 Tasks",
    "text": "Session 2 Tasks\n\nPurchase API Key"
  },
  {
    "objectID": "Tasks_Lists.html#session-3-tasks",
    "href": "Tasks_Lists.html#session-3-tasks",
    "title": "Tasks List",
    "section": "Session 3 Tasks",
    "text": "Session 3 Tasks"
  },
  {
    "objectID": "Tasks_Lists.html#session-4-tasks",
    "href": "Tasks_Lists.html#session-4-tasks",
    "title": "Tasks List",
    "section": "Session 4 Tasks",
    "text": "Session 4 Tasks"
  },
  {
    "objectID": "Tasks_Lists.html#end-of-workshop-tasks",
    "href": "Tasks_Lists.html#end-of-workshop-tasks",
    "title": "Tasks List",
    "section": "End of Workshop Tasks",
    "text": "End of Workshop Tasks"
  },
  {
    "objectID": "sessions/session_1/census_api_instructions.html",
    "href": "sessions/session_1/census_api_instructions.html",
    "title": "Census API",
    "section": "",
    "text": "Of course. Here is a walkthrough for using the tidycensus package, structured just like the Spotify example, to get data from the U.S. Census Bureau API.\n\n\nIntroduction to tidycensus\nThe tidycensus package is a wrapper for the U.S. Census Bureau‚Äôs APIs. It is designed to make it simple to download, manage, and map Census data within R. It handles the API requests and returns clean, tidy data frames ready for analysis.\n\n\n\nStep 1: Get a Census API Key\nBefore using the API, you need a key. This is a simple, one-time process.\n\nGo to the Census API Key request page: https://api.census.gov/data/key_signup.html\nFill out the short form with your organization and email address.\nYour API key will be sent to your email almost immediately. Keep it handy.\n\n\n\n\nStep 2: Set Up Credentials\nThe tidycensus package includes a function to store your API key securely in the .Renviron file, so you only have to do this once per computer.\n\n# install.packages(\"tidycensus\") # Run this once if needed\nlibrary(tidycensus)\n\n# Replace \"YOUR_KEY_HERE\" with the key you received via email.\n# The `install = TRUE` argument saves it to your .Renviron file.\ncensus_api_key(\"YOUR_KEY_HERE\", install = TRUE)\n\n‚ö†Ô∏è Crucially, you must restart your R session for the key to be available. Go to Session &gt; Restart R in RStudio. From now on, tidycensus will automatically find and use your key.\n\n\n\nStep 3: Load Required Packages\nFor this analysis, we‚Äôll need tidycensus for data retrieval and dplyr and ggplot2 for data wrangling and visualization.\n\nlibrary(tidycensus)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n\n\nStep 4: Find Your Variables\nThe Census Bureau offers thousands of variables. A key step is finding the specific codes for the data you need. We can use the load_variables() function to search. Let‚Äôs find the variable code for ‚Äúmedian household income‚Äù in the 2022 American Community Survey (ACS) 5-year estimates.\n\n# Load all variables from the 2022 5-year ACS dataset\nv22 &lt;- load_variables(2022, \"acs5\")\n\n# Search for the variable we want\nv22 %&gt;% \n  filter(grepl(\"Median Household Income\", label, ignore.case = TRUE))\n\n# A tibble: 25 √ó 4\n   name        label                                           concept geography\n   &lt;chr&gt;       &lt;chr&gt;                                           &lt;chr&gt;   &lt;chr&gt;    \n 1 B19013A_001 Estimate!!Median household income in the past ‚Ä¶ Median‚Ä¶ tract    \n 2 B19013B_001 Estimate!!Median household income in the past ‚Ä¶ Median‚Ä¶ tract    \n 3 B19013C_001 Estimate!!Median household income in the past ‚Ä¶ Median‚Ä¶ tract    \n 4 B19013D_001 Estimate!!Median household income in the past ‚Ä¶ Median‚Ä¶ tract    \n 5 B19013E_001 Estimate!!Median household income in the past ‚Ä¶ Median‚Ä¶ county   \n 6 B19013F_001 Estimate!!Median household income in the past ‚Ä¶ Median‚Ä¶ tract    \n 7 B19013G_001 Estimate!!Median household income in the past ‚Ä¶ Median‚Ä¶ tract    \n 8 B19013H_001 Estimate!!Median household income in the past ‚Ä¶ Median‚Ä¶ tract    \n 9 B19013I_001 Estimate!!Median household income in the past ‚Ä¶ Median‚Ä¶ tract    \n10 B19013_001  Estimate!!Median household income in the past ‚Ä¶ Median‚Ä¶ block gr‚Ä¶\n# ‚Ñπ 15 more rows\n\n\nThe search reveals that the variable code we want is B19013_001.\n\n\n\nStep 5: Request Census Data\nNow we use the main function, get_acs(), to download the data. We‚Äôll request the median household income for every county in Iowa.\n\n# Request the data for Iowa counties\niowa_income_df &lt;- get_acs(\n  geography = \"county\",\n  variables = c(med_income = \"B19013_001\"), # Provide the variable code\n  state = \"IA\",\n  year = 2022\n)\n\nGetting data from the 2018-2022 5-year ACS\n\n\nWarning: ‚Ä¢ You have not set a Census API key. Users without a key are limited to 500\nqueries per day and may experience performance limitations.\n‚Ñπ For best results, get a Census API key at\nhttp://api.census.gov/data/key_signup.html and then supply the key to the\n`census_api_key()` function to use it throughout your tidycensus session.\nThis warning is displayed once per session.\n\n\n\n\n\nStep 6: Explore and Visualize the Dataset\nUse glimpse() to examine the data structure. You‚Äôll see it returns a tidy data frame with columns for the estimate and the margin of error (moe).\n\nglimpse(iowa_income_df)\n\nRows: 99\nColumns: 5\n$ GEOID    &lt;chr&gt; \"19001\", \"19003\", \"19005\", \"19007\", \"19009\", \"19011\", \"19013\"‚Ä¶\n$ NAME     &lt;chr&gt; \"Adair County, Iowa\", \"Adams County, Iowa\", \"Allamakee County‚Ä¶\n$ variable &lt;chr&gt; \"med_income\", \"med_income\", \"med_income\", \"med_income\", \"med_‚Ä¶\n$ estimate &lt;dbl&gt; 63172, 64750, 64049, 50684, 54973, 79444, 62329, 75759, 84727‚Ä¶\n$ moe      &lt;dbl&gt; 4508, 10683, 2577, 4665, 6341, 4092, 2004, 5710, 3606, 4553, ‚Ä¶\n\n\nNow, let‚Äôs create a simple plot of the 10 counties with the highest median income.\n\niowa_income_df %&gt;%\n  slice_max(order_by = estimate, n = 10) %&gt;%\n  ggplot(aes(x = estimate, y = reorder(NAME, estimate))) +\n  geom_col(fill = \"dodgerblue\") +\n  labs(\n    title = \"Top 10 Iowa Counties by Median Household Income (2022)\",\n    x = \"Median Household Income (USD)\",\n    y = \"County\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis process‚Äîfinding variables, requesting data by geography, and getting a clean data frame‚Äîis the core workflow of tidycensus, making it incredibly powerful for demographic analysis."
  }
]