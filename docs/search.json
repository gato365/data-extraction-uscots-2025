[
  {
    "objectID": "sessions/session_1/libraries_of_packages.html",
    "href": "sessions/session_1/libraries_of_packages.html",
    "title": "Library of Packages",
    "section": "",
    "text": "Chloropleth Map (geoapify.com)\n\n\n\n\nIntroduction to Libraries & Glossary\nThis project, when possible, utilizes tidyverse packages. tidyverse is a collection of open-source packages that are well-integrated to tackle problems of data extraction, manipulation, transformation, exploration, and visualization.\n\n\n\nCollection of Tidyverse packages.\n\n\n\nlibrary(rvest)      # Web scraping\nlibrary(dplyr)      # Data manipulation\nlibrary(stringr)    # String cleaning\nlibrary(rlang)      # Advanced evaluation\nlibrary(purrr)      # Functional tools\nlibrary(ggplot2)    # Visualizations\nlibrary(httr2)      # Makes web requests\nlibrary(tibble)     # Easier and prettier data frames\nlibrary(lubridate)  # Handles dates\nlibrary(dotenv)     # Loads environment variables from .Renviron\nlibrary(glue)       # Easier string concatenation\nlibrary(tigris)     # U.S. shapefiles for mapping\n\nThis is a list of R packages used in this project, accompanied by brief descriptions of what they do.\nrvest\n\nUsed for web scraping: parses HTML/XML documents into a navigable format.\nExtracts structured data using CSS selectors, XPath, or tag-based search (html_table(), html_text(), etc.).\n\n\ndplyr\n\nCore package for tidyverse-style data manipulation (filter(), mutate(), select(), etc.).\nSupports chaining operations with %&gt;% / |&gt;, making data workflows readable and efficient.\n\n\nstringr\n\nProvides a consistent set of functions for string manipulation.\nHandles pattern matching, replacement, splitting, and formatting.\n\n\nrlang\n\nSupports advanced evaluation and programming with tidyverse tools.\nUseful when writing custom functions that dynamically reference or modify variables.\n\n\npurrr\n\nEnables functional programming with mapping tools like map(), map_df(), walk(), etc.\nReplaces (many) for-loops and supports clean iteration over lists and vectors.\n\n\nggplot2\n\nGraphics package for building layered, flexible visualizations.\nSupports various plot types, themes, scales, and faceting for data storytelling.\n\n\nhttr2\n\nModern HTTP client designed for tidyverse-like API interaction.\nReplaces httr with a more intuitive and pipeable interface (request() |&gt; req_perform()).\n\n\ntibble\n\nA modern rethinking of the data.frame that prints cleaner and behaves more predictably.\nDefault in tidyverse workflows; avoids surprises like string-to-factor conversion.\n\n\nlubridate\n\nSimplifies working with dates and times: parsing, formatting, and arithmetic.\nMakes it easy to extract components (e.g., month, weekday) and perform date math.\n\n\ndotenv\n\nLoads environment variables from a .env or .Renviron file into R.\nKeeps sensitive data like API keys out of your scripts and version control.\n\n\nglue\n\nProvides string interpolation (e.g., glue(\"Hello {name}\")).\nCleaner and safer than paste() for building URLs, messages, or SQL queries.\n\n\ntigris\n\nDownloads shapefiles and geographic boundary data (e.g., counties, states) from the U.S. Census Bureau.\nReturns spatial sf data frames, making it easy to map and visualize geographic data."
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_to_fill_07032025.html",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_to_fill_07032025.html",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "",
    "text": "Session 1: Introduction\nUnderstand the importance of extracting dynamic data (via HTML and APIs) in modern data analysis and teaching\nSession 2: Getting Weather Data via OpenWeather API\nIn this session, we dive into OpenWeather API and learn to use packages like httr2 to execute API calls. We will also discuss URLs, queries, data structures, and more.\nSession 3: Scraping NFL Sports Data\nIn this session, we will use Pro-Football Reference to learn how to extract and clean HTML table data for use in statistical analysis and visualizations.\nSession 4: Putting it All Together (Project)\nIn this project, we will use HTML scraping joined with the OpenWeather API to create our own cloropleth map of Iowa.\n[[Change Photo to reflect something else]]"
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_to_fill_07032025.html#goals-objectives-entire-workshop",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_to_fill_07032025.html#goals-objectives-entire-workshop",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "",
    "text": "Session 1: Introduction\nUnderstand the importance of extracting dynamic data (via HTML and APIs) in modern data analysis and teaching\nSession 2: Getting Weather Data via OpenWeather API\nIn this session, we dive into OpenWeather API and learn to use packages like httr2 to execute API calls. We will also discuss URLs, queries, data structures, and more.\nSession 3: Scraping NFL Sports Data\nIn this session, we will use Pro-Football Reference to learn how to extract and clean HTML table data for use in statistical analysis and visualizations.\nSession 4: Putting it All Together (Project)\nIn this project, we will use HTML scraping joined with the OpenWeather API to create our own cloropleth map of Iowa.\n[[Change Photo to reflect something else]]"
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_to_fill_07032025.html#a.-goals-for-introduction",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_to_fill_07032025.html#a.-goals-for-introduction",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "1.a. Goals for Introduction",
    "text": "1.a. Goals for Introduction\n\nAnalyzed static player statistics by loading an Excel file into R to filter the data and create a comparative boxplot.\nIntroduced dynamic data extraction by explaining how to use web APIs to send a request containing a query for structured JSON data from external servers.\nDemonstrated web scraping by using an R package to directly extract a data table from an HTML webpage.\nAdvocated for a modern educational approach that teaches students to actively find and extract live data rather than passively using clean, static files."
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_to_fill_07032025.html#conceptual-foundation",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_to_fill_07032025.html#conceptual-foundation",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "2. Conceptual Foundation",
    "text": "2. Conceptual Foundation\n\nP1. Traditional Approach\n\nMy mentor, Allan, says “ask good questions”…\nStatistical Question: Who had the most impactful first season in terms of points: Michael Jordan, LeBron James, or Kobe Bryant?\nLet’s answer this question.\nWe’ll begin by working with a static excel file, named nba_data.xlsx, that contains per-game stats for each player’s 15 seasons in the NBA.\n\n\nStep 1. Fill in the code based common tidyverse packages.\n\n## EMPTY VERSION\nlibrary(readxl)      ## Data Extraction      --- E\nlibrary(dplyr)       ## Data Transformation  --- T\nlibrary(ggplot2)     ## Data Visualization   --- V\n\nStep 2. Complete the appropriate function name and fill in the file name into below.\n\n## EMPTY VERSION\nnba_df &lt;- read_xlsx(\"nba_data.xlsx\", sheet = \"modern_nba_legends_08302019\")\n\n\nLet’s view the data …\n\nStep 3. Use the glimpse function to view the data\n\n## EMPTY VERSION\nglimpse(nba_df)\n\n\n\nNow let’s clean the focus on the data frame that we are after\n\nStep 4. Use the filter() function to select only the rows where the Season column is equal to “season_1”.\n\n## EMPTY VERSION\nseason_1_df &lt;- nba_df %&gt;%\n  filter( Season == \"season_1\")\n\n\nNow lets look at a plot of their points to answer the statistical question\n\nStep 5. Pipe the season_1_df data into ggplot, map the Name column to the x-axis and PTS to the y-axis, and then add a geom_boxplot() layer to visualize the data.\n\n## EMPTY VERSION\nseason_1_df %&gt;%\n  ggplot(aes(x = Name, y = PTS)) +\n  geom_boxplot() +\n  theme_bw()\n\n\nAre there only 3 players that only played in the NBA?\n\nNow what about, Magic Johnson or Wilt Chamberlain (historic players)\n\n\n\n\nHistoric Players\n\n\n\nMaybe Luka Dončić or Ja Morant (more recent players)\n\n\n\n\nRecenr Players\n\n\n\nIf I wanted to add this data I need to go to the original source not an excel sheet to do this"
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_to_fill_07032025.html#session-1-activity-census-api-and-html-in-practice",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_to_fill_07032025.html#session-1-activity-census-api-and-html-in-practice",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "3. Session 1 Activity: Census API and HTML in Practice",
    "text": "3. Session 1 Activity: Census API and HTML in Practice\nThis activity will give you a chance to apply the skills you’ve just learned. We’ll start by fetching live demographic data for Iowa, visualize it, and finally, join it with data scraped from a Wikipedia page.\n\nP8. Task 1: Get County-Level Census Data\nYour first task is to get demographic data for all counties in Iowa using tidycensus. Let’s grab two variables at once: median age (B01002_001) and total population (B01003_001). After retrieving the data, use glimpse() to inspect its structure.\n\n## EMPTY VERSION\n# iowa_df &lt;- get_acs(\n#   geography = \"____\",\n#   variables = c(\n#     median_age = \"____\", # Median Age Code\n#     total_pop = \"____\"   # Total Population Code\n#     ), \n#   state = \"____\",\n#   year = 2022\n# )\n# \n# ____(iowa_df)\n\n\nQ9.\nTake a look at the output from glimpse(). How is this data structured differently than a typical “wide” dataset with one row per county? What does the moe column represent?\n\n\nAns Q9.\n\n\n\n\nP9. Task 2: Wrangle and Visualize the Data\nNow, let’s turn that raw data into an insight. Your task is to create a scatter plot to see the relationship between a county’s population and its median age. You will need to:\n\nUse pivot_wider() to transform the data from a long to a wide format, creating separate columns for median_age and total_pop.\nPipe this into ggplot to create a scatter plot.\n\n\n\n## EMPTY VERSION\n# iowa_df %&gt;% \n#   select(____, NAME, variable, estimate) %&gt;% # Select only needed columns\n#   pivot_wider(names_from = ____, values_from = ____) %&gt;% \n#   ggplot(aes(x = ____, y = ____)) +\n#   geom_point() + \n#   theme_bw()\n\n\nQ10.\nBased on your plot, what relationship, if any, do you observe between a county’s population and its median age in Iowa?\n\n\nAns Q10.\n\n\n\n\nP10. Task 3: Join with Scraped HTML Data\nAPIs give us great data, but sometimes we need to supplement it. Let’s grab the county seat for each Iowa county from Wikipedia and join it with our Census data.\nThe URL is: \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\nYour task is to scrape the main table, select and rename the relevant columns, and then perform a left_join to add the county seat to your tidycensus data.\n\n## EMPTY VERSION\n# library(htmltab)\n# \n# # 1. Scrape the data\n# url &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n# scraped_df &lt;- htmltab(doc = url, which = ____)\n# \n# # 2. Clean and select scraped data\n# seats_df &lt;- scraped_df %&gt;% \n#   select(NAME = `____`, seat = `____`)\n# \n# # 3. Widen the census data (from previous step)\n# iowa_wide_df &lt;- iowa_df %&gt;% \n#   select(GEOID, NAME, variable, estimate) %&gt;%\n#   pivot_wider(names_from = variable, values_from = estimate)\n# \n# # 4. Join the two datasets\n# combined_df &lt;- left_join(____, ____, by = \"NAME\")\n# \n# head(combined_df)\n\n\nQ12.\nWhat was the biggest challenge in joining the data from tidycensus and the scraped Wikipedia table?\n\n\nAns Q12."
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_to_fill_07032025.html#reflection-make-questions-within-coursekata-to-solidyfy-approach",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_to_fill_07032025.html#reflection-make-questions-within-coursekata-to-solidyfy-approach",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "4. Reflection (make questions within CourseKata to solidyfy approach)",
    "text": "4. Reflection (make questions within CourseKata to solidyfy approach)\n\nWhat did we learn?\nHow does this connect to the original Goals & Objectives of the session?\nHow do you see yourself using this in your classroom?\nWhat kinds of APIs or HTML sources would be most relevant for your students?"
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_07032025.html",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_07032025.html",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "",
    "text": "Session 1: Introduction\nUnderstand the importance of extracting dynamic data (via HTML and APIs) in modern data analysis and teaching\nSession 2: Getting Weather Data via OpenWeather API\nIn this session, we dive into OpenWeather API and learn to use packages like httr2 to execute API calls. We will also discuss URLs, queries, data structures, and more.\nSession 3: Scraping NFL Sports Data\nIn this session, we will use Pro-Football Reference to learn how to extract and clean HTML table data for use in statistical analysis and visualizations.\nSession 4: Putting it All Together (Project)\nIn this project, we will use HTML scraping joined with the OpenWeather API to create our own cloropleth map of Iowa.\n[[Change Photo to reflect something else]]"
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_07032025.html#goals-objectives-entire-workshopr",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_07032025.html#goals-objectives-entire-workshopr",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "",
    "text": "Session 1: Introduction\nUnderstand the importance of extracting dynamic data (via HTML and APIs) in modern data analysis and teaching\nSession 2: Getting Weather Data via OpenWeather API\nIn this session, we dive into OpenWeather API and learn to use packages like httr2 to execute API calls. We will also discuss URLs, queries, data structures, and more.\nSession 3: Scraping NFL Sports Data\nIn this session, we will use Pro-Football Reference to learn how to extract and clean HTML table data for use in statistical analysis and visualizations.\nSession 4: Putting it All Together (Project)\nIn this project, we will use HTML scraping joined with the OpenWeather API to create our own cloropleth map of Iowa.\n[[Change Photo to reflect something else]]"
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_07032025.html#a.-goals-for-introduction",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_07032025.html#a.-goals-for-introduction",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "1.a. Goals for Introduction",
    "text": "1.a. Goals for Introduction\n\nAnalyzed static player statistics by loading an Excel file into R to filter the data and create a comparative boxplot.\nIntroduced dynamic data extraction by explaining how to use web APIs to send a request containing a query for structured JSON data from external servers.\nDemonstrated web scraping by using an R package to directly extract a data table from an HTML webpage.\nAdvocated for a modern educational approach that teaches students to actively find and extract live data rather than passively using clean, static files."
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_07032025.html#conceptual-foundation",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_07032025.html#conceptual-foundation",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "2. Conceptual Foundation",
    "text": "2. Conceptual Foundation\n\nP1. Traditional Approach\n\nMy mentor, Allan, says “ask good questions”…\nStatistical Question: Who had the most impactful first season in terms of points: Michael Jordan, LeBron James, or Kobe Bryant?\n\n[Image]\n\nLet’s answer this question.\nWe’ll begin by working with a static excel file, named nba_data.xlsx, that contains per-game stats for each player’s 15 seasons in the NBA.\n\n\nStep 1. Fill in the code based common tidyverse packages.\n\n## EMPTY VERSION\n# library(____)      ## Data Extraction      --- E\n# library(____)       ## Data Transformation  --- T\n# library(____)     ## Data Visualization   --- V\n\nStep 2. Complete the appropriate function name and fill in the file name into below.\n\n## EMPTY VERSION\n# nba_df &lt;- read_*(\"____\", sheet = \"modern_nba_legends_08302019\")\n\n\nLet’s view the data …\n\nStep 3. Use the glimpse function to view the data\n\n## EMPTY VERSION\n\n\n\nNow let’s clean the focus on the data frame that we are after\n\nStep 4. Use the filter() function to select only the rows where the Season column is equal to “season_1”.\n\n## EMPTY VERSION\n# season_1_df &lt;- nba_df %&gt;% \n#   ___( _____ == \"season_1\")\n\n\nNow lets look at a plot of their points to answer the statistical question\n\nStep 5. Pipe the season_1_df data into ggplot, map the Name column to the x-axis and PTS to the y-axis, and then add a geom_boxplot() layer to visualize the data.\n\n## EMPTY VERSION\n# ____ %&gt;% \n#   ggplot(aes(x = ____, y = ____)) +\n#   geom_*() +\n#   theme_bw()\n\n\nAre there only 3 players that only played in the NBA?\n\nNow what about, Magic Johnson or Wilt Chamberlain (historic players)\n\n\n\n\nHistoric Players\n\n\n\nMaybe Luka Dončić or Ja Morant (more recent players)\n\n\n\n\nRecenr Players\n\n\n\nIf I wanted to add this data I need to go to the original source not an excel sheet to do this"
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_07032025.html#session-1-activity-census-api-and-html-in-practice",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_07032025.html#session-1-activity-census-api-and-html-in-practice",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "3. Session 1 Activity: Census API and HTML in Practice",
    "text": "3. Session 1 Activity: Census API and HTML in Practice\nThis activity will give you a chance to apply the skills you’ve just learned. We’ll start by fetching live demographic data for Iowa, visualize it, and finally, join it with data scraped from a Wikipedia page.\n\nP8. Task 1: Get County-Level Census Data\nYour first task is to get demographic data for all counties in Iowa using tidycensus. Let’s grab two variables at once: median age (B01002_001) and total population (B01003_001). After retrieving the data, use glimpse() to inspect its structure.\n\n## EMPTY VERSION\n# iowa_df &lt;- get_acs(\n#   geography = \"____\",\n#   variables = c(\n#     median_age = \"____\", # Median Age Code\n#     total_pop = \"____\"   # Total Population Code\n#     ), \n#   state = \"____\",\n#   year = 2022\n# )\n# \n# ____(iowa_df)\n\n\nQ9.\nTake a look at the output from glimpse(). How is this data structured differently than a typical “wide” dataset with one row per county? What does the moe column represent?\n\n\nAns Q9.\n\n\n\n\nP9. Task 2: Wrangle and Visualize the Data\nNow, let’s turn that raw data into an insight. Your task is to create a scatter plot to see the relationship between a county’s population and its median age. You will need to:\n\nUse pivot_wider() to transform the data from a long to a wide format, creating separate columns for median_age and total_pop.\nPipe this into ggplot to create a scatter plot.\n\n\n\n## EMPTY VERSION\n# iowa_df %&gt;% \n#   select(____, NAME, variable, estimate) %&gt;% # Select only needed columns\n#   pivot_wider(names_from = ____, values_from = ____) %&gt;% \n#   ggplot(aes(x = ____, y = ____)) +\n#   geom_point() + \n#   theme_bw()\n\n\nQ10.\nBased on your plot, what relationship, if any, do you observe between a county’s population and its median age in Iowa?\n\n\nAns Q10.\n\n\n\n\nP10. Task 3: Join with Scraped HTML Data\nAPIs give us great data, but sometimes we need to supplement it. Let’s grab the county seat for each Iowa county from Wikipedia and join it with our Census data.\nThe URL is: \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\nYour task is to scrape the main table, select and rename the relevant columns, and then perform a left_join to add the county seat to your tidycensus data.\n\n## EMPTY VERSION\n# library(htmltab)\n# \n# # 1. Scrape the data\n# url &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n# scraped_df &lt;- htmltab(doc = url, which = ____)\n# \n# # 2. Clean and select scraped data\n# seats_df &lt;- scraped_df %&gt;% \n#   select(NAME = `____`, seat = `____`)\n# \n# # 3. Widen the census data (from previous step)\n# iowa_wide_df &lt;- iowa_df %&gt;% \n#   select(GEOID, NAME, variable, estimate) %&gt;%\n#   pivot_wider(names_from = variable, values_from = estimate)\n# \n# # 4. Join the two datasets\n# combined_df &lt;- left_join(____, ____, by = \"NAME\")\n# \n# head(combined_df)\n\n\nQ12.\nWhat was the biggest challenge in joining the data from tidycensus and the scraped Wikipedia table?\n\n\nAns Q12."
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_07032025.html#reflection-make-questions-within-coursekata-to-solidyfy-approach",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_empty_07032025.html#reflection-make-questions-within-coursekata-to-solidyfy-approach",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "4. Reflection (make questions within CourseKata to solidyfy approach)",
    "text": "4. Reflection (make questions within CourseKata to solidyfy approach)\n\nWhat did we learn?\nHow does this connect to the original Goals & Objectives of the session?\nHow do you see yourself using this in your classroom?\nWhat kinds of APIs or HTML sources would be most relevant for your students?"
  },
  {
    "objectID": "sessions/session_4/04_Extraction_FinalActivity.html",
    "href": "sessions/session_4/04_Extraction_FinalActivity.html",
    "title": "Session 4: Final Activity",
    "section": "",
    "text": "Download starter .qmd file"
  },
  {
    "objectID": "sessions/session_4/04_Extraction_FinalActivity.html#session-4-final-activity",
    "href": "sessions/session_4/04_Extraction_FinalActivity.html#session-4-final-activity",
    "title": "Session 4: Final Activity",
    "section": "Session 4: Final Activity",
    "text": "Session 4: Final Activity\n\n1. Weather Activity\nNow that we have worked with HTML elements and API calls, let’s put these skills together to create a weather map of Iowa counties based on the maximum temperature at each county’s administrative seat (county seat).\n\n\n\nChloropleth Map (geoapify.com)\n\n\n\n\nStep 1: Load Libraries\nWe’ll begin by loading the libraries necessary for data manipulation, API calls, and visualization.\n\n# Step 1: Load Libraries\n# Step 1a: General utilities\nlibrary(httr2)       # Makes web requests\nlibrary(tibble)      # Easier and prettier data frames\nlibrary(lubridate)   # Handles dates\nlibrary(ggplot2)     # Data visualization\nlibrary(dplyr)       # Data manipulation\nlibrary(dotenv)      # Loads environment variables from .Renviron\nlibrary(glue)        # Easier string concatenation\nlibrary(purrr)       # Functional programming tools\nlibrary(rvest)       # Web scraping\nlibrary(tigris)      # U.S. shapefiles for mapping\nlibrary(stringr)     # String manipulation and handling\n\n\n# Step 1b: Load API key from .Renviron.txt\ndotenv::load_dot_env(file = \".Renviron.txt\")\n\n\n\n\nStep 2: Extract County and Seat Information from Wikipedia\nHere we extract a table from Wikipedia listing all Iowa counties and their corresponding county seats.\n\n# Step 2: Scrape HTML Table from Wikipedia\n# Step 2a: Set URL\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n\n# Step 2b: Read HTML\nwebpage &lt;- url |&gt;  \n  rvest::read_html()\n\n# Step 2c: Extract all HTML tables from the page\nhtml_tables &lt;- webpage |&gt; \n  rvest::html_table()\n\n# Step 2d: Select the correct table (based on inspection)\ntable1 &lt;- html_tables |&gt; \n  purrr::pluck(2)\n\n# Step 2e: Clean and prepare county seat names\n#         Add ', IA, USA' to ensure geolocation works with OpenWeather API\n#         Use only the first County Seat city name when mulitple exist\ncounty_seats_df &lt;- table1 |&gt; \n  mutate(\n    `County seat[4]` = str_split(`County seat[4]`, \" and \") |&gt; sapply(`[`, 1),\n    city = paste0(`County seat[4]`, \", IA, USA\")\n  )\n\nDiscussion: Why would we need to append”, IA, USA” to the city name?\nWhat would happen if we do not?\nA: This helps geocoding APIs return accurate coordinates. (Just “Ames” returns a rural town of 600 in Northern France)\n\n\n\nNot Exactly Ames, IA, USA. Church of Ames, France (Wikipedia)\n\n\n\n\n\nStep 3: Pull Weather Data for Each County Seat\nWe now define a function to call the OpenWeather Geocoding and One Call APIs. Then we use it to retrieve temperature data for each city.\n\n# Step 3: Define function to get geocoded weather data from OpenWeather\nget_city_weather &lt;- function(city, date = Sys.Date()) {\n  # Step 3a: Get coordinates from geocoding API\n  geo_url &lt;- glue(\n    \"http://api.openweathermap.org/geo/1.0/direct?\",\n    \"q=\", URLencode(city),\n    \"&limit=1&appid=\", Sys.getenv(\"API_KEY\"))\n  geo_response &lt;- req_perform(request(geo_url))\n\n  if (resp_status(geo_response) == 200) {\n    geo_data &lt;- as.data.frame(resp_body_json(geo_response))\n    if (nrow(geo_data) == 0) return(NULL)\n\n    lat &lt;- geo_data$lat\n    lon &lt;- geo_data$lon\n\n    # Step 3b: Call the weather summary API\n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", format(date, \"%Y-%m-%d\"),\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\")\n    weather_response &lt;- req_perform(request(weather_url))\n\n    if (resp_status(weather_response) == 200) {\n      weather_data &lt;- resp_body_json(weather_response)\n      tibble(\n        city = city,\n        date = date,\n        lat = lat,\n        lon = lon,\n        temp_max = weather_data$temperature$max,\n        temp_min = weather_data$temperature$min\n      )\n    } else return(NULL)\n  } else return(NULL)\n}\n\n\n# Step 3c: Apply the function to all county seat cities\ncities &lt;- county_seats_df |&gt; \n  pull(12)\nweather_df &lt;- bind_rows(lapply(cities, get_city_weather))\n\nDiscussion: What happens if one of the cities fails to return a result?\nWe could add error handling or a fallback method, such as purrr::possibly()?\n\n\n\nStep 4: Merge Weather with Spatial County Boundaries\nNow we join our temperature data with spatial geometries (map pieces) from the tigris package so we can map it.\n\n# Step 4: Merge weather data into spatial map\n# Step 4a: Join temperature data with counties via county seat\ncounty_temp_df &lt;- county_seats_df |&gt; \n  left_join(weather_df, by = \"city\")  # joins on city (admin chair)\n\n# Step 4b: Load Iowa county shapes from tigris\niowa_counties &lt;- tigris::counties(state = \"IA\", cb = TRUE, class = \"sf\")\n\n# Step 4c: Join temperature data into spatial counties by NAME\n# (Be sure county names match exactly)\niowa_map_filled &lt;- iowa_counties |&gt; \n  left_join(county_temp_df, by = c(\"NAMELSAD\" = \"County\"))\n\nDiscussion: How could we verify that all counties successfully matched?\n\n\n\nStep 5: Visualize the Map\nFinally, we use ggplot2 and geom_sf() to create a choropleth map of Iowa counties filled by temperature.\n\n# Step 5: Plot the map\n# Step 5a: Use fill aesthetic to show temperature per county\n\nggplot(iowa_map_filled) +\n  geom_sf(aes(fill = temp_max), color = \"white\") +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Max Temp (°F)\", na.value = \"grey90\") +\n  labs(\n    title = \"Iowa County Temperatures by County Seat\",\n    subtitle = paste(\"Date:\", unique(weather_df$date)),\n    caption = \"Source: OpenWeather API\"\n  ) +\n  theme_minimal(base_size = 14)\n\nIdeas to expand this visualization:\n- Add interactivity with plotly\n- Animate changes over multiple dates\n- Compare actual temps to historical averages\n- Add city point labels or icons\n\n\n\n2. Sports Activity\n\n\n3. Wrap & Homework"
  },
  {
    "objectID": "sessions/session_3/html_quick_steps.html",
    "href": "sessions/session_3/html_quick_steps.html",
    "title": "Untitled",
    "section": "",
    "text": "library(rvest)      # ≠≠Data Extraction   ---- E (Web scraping)\nlibrary(dplyr)      # Data Transformation ---- T \n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)    # Data Transformation ---- T (String cleaning)\nlibrary(glue)\nlibrary(rlang)      # Data Transformation ---- T (Advanced evaluation)\nlibrary(purrr)      # Data Transformation ---- T (Functional tools)\n\n\nAttaching package: 'purrr'\n\n\nThe following objects are masked from 'package:rlang':\n\n    %@%, flatten, flatten_chr, flatten_dbl, flatten_int, flatten_lgl,\n    flatten_raw, invoke, splice\n\nlibrary(ggplot2)    # Data Visualizations ---- V\n\n\n# Step 1: Define team and year\nteam_name &lt;- \"was\"\nyear &lt;- 2023\n\n# Step 2: Construct full URL\ngeneric_url &lt;- glue(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n\n# Step 3: Read HTML page\nwebpage &lt;- generic_url %&gt;% \n  read_html()\n\n# Step 4: Extract all HTML tables\nweb_tables &lt;- webpage %&gt;%  \n  html_table()\n\n# Step 5: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables %&gt;%  \n  pluck(2)\n\nNote: Sometimes we have to refer to base R syntax because the new mechanism does not work with the task we experience\n\n# Step 6: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] %&gt;% \n  unlist() %&gt;% \n  as.character()\n\n# Step 7: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n# Step 8: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n# Step 9: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\n\n# Step 10: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 %&gt;% \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n\n\n# Step 11: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 %&gt;% \n  select(!(x:x_2)) %&gt;% \n  filter(opp != \"Bye Week\")\n\n\n# Step 12: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 %&gt;%  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n\n# Step 13: Handle factors and location labels\ntable_6 &lt;- table_5 %&gt;% \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) %&gt;%  as.factor()\n  )\n\n# Step 14: Final column cleanup\ntable_7 &lt;- table_6 %&gt;% \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))"
  },
  {
    "objectID": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_empty_fillable_07072025.html",
    "href": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_empty_fillable_07072025.html",
    "title": "Session 2: Weather Data - OpenWeatherAPI - Empty",
    "section": "",
    "text": "By the end of this session, participants will be able to:\n\nUnderstand the foundational concepts of APIs as a bridge for data exchange, including how they function in modern software and support real-time data extraction.\nQuery public APIs effectively, forming well-structured requests that interact with remote databases and return meaningful results.\nInterpret JSON responses, with a focus on the data element, while also distinguishing between metadata and status codes. Develop an understanding of how HTTP status codes and API keys work to validate and secure data access.\nWrite clean, purposeful R code to send API requests, handle responses, and parse structured data into tidy, analyzable formats.\n\n\n\n\n\n\n\nPart A. Theoretical ideas of APIs\n\n\nIt is the ability for software to communicate\n\n\n\nAPI Call (PhoenixNap.com)\n\n\n\n\n\n\n\n\nLets go deeper into understanding Define:\nClient (request) –&gt; API –&gt; Server –&gt; Database\nClient &lt;– API &lt;– Server (response) &lt;– Database\n\n\n\nGATO365 API Request & Response\n\n\n\n\n\n\n\n\nLets spend some more time on the request and response\nThe client sends a request asking for info (like Taylor Swift or today’s weather). This request includes:\n\nA URL (e.g., with parameters like ?q=San+Luis+Obispo)\nPossibly an API key\nA method (e.g., GET or POST)\n\nThe request are in the form of a url string (more on this soon…)\nThe server then returns a response which contains:\n\ndata (temperature, artist name, forecast, etc.)\nmetadata (This is information about the response.)\nstatus code (Tells you whether the request was successful)\n\nThis information is traditionally provided in JSON Format. (more on this soon…)\n\n\n\n\n\n\n\nLet’s focus on what the response is 1st (what we receive from the server):\nBelow is an example GIF of the information sent from the server in JSON format:\n\n\n\n\nGATO365 Anatomy JSON\n\n\n\n\n\n\n\n\nStatus codes tell you what happened with your request:\n\n100s: Info\n200s: Success (highlight: 200 OK)\n300s: Redirect\n400s: Client error\n500s: Server error\n\nNote 5:\n\nEmphasize: In most data APIs, your goal is to get a 200 response.\nUse examples like making up a nonexistent city or artist to show how an API might respond with a 400 or 404. |\n\n** Client Request *************************\n\n\n\n\n\n\nWhat type of client requests can we make?\nCRUD Framework (Create, Read, Update, Delete)\n\nThough APIs allow all four, Read (GET) is most common in data science.\nRESTful API mapping:\n\nCreate → POST\nRead → GET\nUpdate → PUT/PATCH\nDelete → DELETE\n\n\n\n\n\n\n\n\n\n\n\n\n\nGATO365 Get Request\n\n\nHere is the description of a GET request from that perspective.\n\nClient constructs a request for a resource.\nAPI receives and validates the client's request.\nServer locates the requested data within database.\nClient receives requested data from the server.\n\n\n\n\n\n\n\n\n\n\nGATO365 Post Request\n\n\n\nHere is the description of a POST request.\n\nClient sends new data within the request body.\nAPI receives and validates the client's new data.\nServer creates a new record in the database.\nClient receives a confirmation for the new record.\n\n\n\n\n\n\n\n\n\n[[Based on time do One of the three steps]]\n[[1. email attendees to go to the weather website and get API key or whatever information needed before the conference. Create a video that’s displaying how to do this]]\n[[1a. Discuss .Renviron.txt, how we use: to create and edit API key: usethis::edit_r_environ()]]\n[[1b. Use Sys.getenv(\"API_KEY\") to see API in console]]\n[[Note that you have to use the 1a to see the api key again]] Restart R\n[[2. have attendees get the key during the break session if they have not done so already]]\n[[3. use a common key, but tell them it is bad practice]]\n[[regardless of the decision made of the three options above have attendees store information in the environment file]]\n\n\n\n\n\n\n\nSo what we’re going to first do is create our request and the most ideal way.\nA request defined by a URL, which contains both:\n\nThe endpoint (base address of the API)\nThe query string (additional key-value pairs that modify the request)\n\nWe often need to glue strings together to build this full URL dynamically.\nA request is not “automatically” turned into JSON when sent — it’s the response that’s usually formatted as JSON. The request is often URL-encoded if it’s a GET.\n\n\n\n\n\n\n\n\nWhen we use a URL like ...?q=San+Luis+Obispo&appid=..., we’re constructing a query string, which is appended to the base URL.\nThink of this as “asking the question”—the query string shapes the request.\nThe server receives the request, processes it, and responds with structured data (typically JSON).\nWe’re not sending JSON in this case—we’re sending a URL with parameters. JSON is returned to us as a response format.\n\n\n\n\nGATO365 Post Request\n\n\n\n\n\n\n\n\n\n\nLoad Libraries\n\n## EMPTY VERSION\nlibrary(httr2)\nlibrary(glue)\n\nSpecify city to query\n\n## EMPTY VERSION\ncity_name &lt;- \"San Luis Obispo\"\n\nUse this url as the base url, https://api.openweathermap.org/data/2.5/weather? (Imporve Instructions using empty and fill)\n\n## EMPTY VERSION\ncurrent_weather_url &lt;- glue(\"https://api.openweathermap.org/data/2.5/weather?\",                        ## Base URL / Endpoint\n                            \"q=\", URLencode(city_name),         ## City Name\n                            \"&appid=\", Sys.getenv(\"API_KEY\"), ## Use of API Key\n                            \"&units=imperial\")                 ## Specify the units\n\n\n\n\nPrint the url string\n\n## EMPTY VERSION\ncurrent_weather_url\n\nMake the request formal\n\n## EMPTY VERSION\nreq &lt;- request(current_weather_url)\n\nPrint the formal request\n\n## EMPTY VERSION\nreq\n\n\n\n\n\n\n\nStep 1: Build Request Object\n\n## EMPTY VERSION\nreq &lt;- request(\"https://api.openweathermap.org/data/2.5/weather?\") %&gt;%\n  req_url_query(\n    q = city_name,\n    appid = Sys.getenv(\"API_KEY\"),\n    units = \"imperial\"\n  )\n\n\n## EMPTY VERSION\nreq\n\n\n\n\nStep 2: Make request\n\n## EMPTY VERSION\nresponse &lt;- req_perform(req)\n\n\n## EMPTY VERSION\n# ____\n\n\nGET: The method used to request data.\nURL: The full address the request was sent to.\nStatus: 200 OK: The request was successful.\nContent-Type: The returned data is in JSON format.\nBody: The weather data is downloaded and in memory.\n\n\n\n\nNot a step: View content Type\n\n## EMPTY VERSION\n# content_type &lt;- resp_content_type(____)\n\n\n## EMPTY VERSION\n# ____\n\n\n\n\nStep 3: Process the Response\n\n## EMPTY VERSION\n# library(____)\n\n\n## EMPTY VERSION\n## IF the status code is 200 we are good\n# if (resp_status(____) == ____) {\n#   \n#   # Parse JSON\n#   result &lt;- resp_body_json(____)\n#   \n#   # Print Results as JSON but in R it is a list\n#   print(____)\n#   \n#   #---------------------------------------\n#   \n#   # Convert to Data Frame directly\n#   current_weather_df &lt;- as.data.frame(____)\n#   \n#   # Print Results as Data Frame, using dplyr\n#   current_weather_df %&gt;% \n#     select(____, ____, ____, ____, ____) %&gt;% \n#     print()\n#   \n#   \n# ## ELSE state there is an Error\n# } else {\n#   cat(\"Failed. Status code:\", resp_status(____), \"\\n\")\n# }\n\n\n\n\n\n\n\n\nProvide instructions (Improve Instructions using empty and fill)\n\n## EMPTY VERSION\n## Step 1: Define function \"get_city_coords\" that accepts the parameter \"city\"\n# get_city_coords &lt;- function(____){\n#   \n# ## Step 2: Create API request URL\n#   geo_url &lt;- glue(\n#     \"____\", # Endpoint\n#     \"q=\", URLencode(____),\n#     \"&limit=1&appid=\", Sys.getenv(\"____\")\n#   )\n#   \n# ## Step 3: Use req_perform() and request() to call the API with the URL request  \n#   geo_response &lt;- req_perform(request(____))\n#   \n# ## Step 4: If the status code is 200 (OK), parse the response\n#   if (resp_status(____) == ____) {\n#     geo_data_df &lt;- resp_body_json(____) %&gt;% \n#       as.data.frame()\n#     \n#   ## Step 5: Assess if the output has 0 length, meaning no result.\n#     if (length(____) == 0) {\n#       stop(\"____\")\n#     }\n#     \n#   ## Step 6: Round latitude and longitude to 2 decimal places.\n#     mod_1_geo_data_df &lt;- geo_data_df %&gt;% \n#       mutate(lat = round(____, 2),\n#              lon = round(____, 2))\n# \n#   ## Step 7: Select and rename columns\n#     mod_2_geo_data_df &lt;- mod_1_geo_data_df %&gt;% \n#       select(____, ____, ____, ____) %&gt;% \n#       rename(city = ____)\n#     \n#   ## Step 8: Return the final data frame\n#     return(____)\n#     \n#   }\n# }\n\n\n\n\nLets try out this new function on the city\n\n## EMPTY VERSION\n# get_city_coords(____)\n\n\n## EMPTY VERSION\n# library(____)\n# \n# # List of cities you want to geocode\n# cities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n# \n# # Use map_df() to apply the function to each city\n# map_df(____, ____)\n\n\n\n\n\n\n\n\n## EMPTY VERSION\n# library(____)\n\n\n## EMPTY VERSION\n# get_past_weather_by_city &lt;- function(____, ____) {\n#   # Step 1: Get city coordinates\n#   coords_df &lt;- get_city_coords(____)\n#   lat &lt;- coords_df$lat\n#   lon &lt;- coords_df$lon\n#   \n#   cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n#   \n#   # Step 2: Create vector of past dates\n#   date_range &lt;- as.character(today() - days(1:____))\n#   \n#   # Step 3: Define function for single date\n#   fetch_day_summary &lt;- function(____) {\n#     weather_url &lt;- glue(\n#       \"____\", # Endpoint\n#       \"lat=\", ____,\n#       \"&lon=\", ____,\n#       \"&date=\", ____,\n#       \"&appid=\", Sys.getenv(\"____\"),\n#       \"&units=imperial\"\n#     )\n#     \n#     response &lt;- req_perform(request(____))\n#     \n#     if (resp_status(____) == 200) {\n#       resp_body_json(____) %&gt;% \n#         as.data.frame() %&gt;% \n#         mutate(city = ____, date = ____)\n#     } else {\n#       warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(response)))\n#       return(NULL)\n#     }\n#   }\n#   \n#   # Step 4: Map over date_range and bind into a single data frame\n#   map_dfr(____, ____)\n# }\n\n\n\n\n\n## EMPTY VERSION\n# get_past_weather_by_city(____, ____)\n\n\n## FILLED RESPONSE\nget_past_weather_by_city(city_name, 5)\n\n\n\n\nThere are a lot of ways of doing this, I decided not to use for loop in R\n\n## EMPTY VERSION\n# num_days &lt;- 5\n# \n# # Get historical weather data for each city using map_dfr\n# all_weather_df &lt;- map_dfr(\n#   ____,\n#   ~ get_past_weather_by_city(____, ____)\n# )"
  },
  {
    "objectID": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_empty_fillable_07072025.html#goals-objectives",
    "href": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_empty_fillable_07072025.html#goals-objectives",
    "title": "Session 2: Weather Data - OpenWeatherAPI - Empty",
    "section": "",
    "text": "By the end of this session, participants will be able to:\n\nUnderstand the foundational concepts of APIs as a bridge for data exchange, including how they function in modern software and support real-time data extraction.\nQuery public APIs effectively, forming well-structured requests that interact with remote databases and return meaningful results.\nInterpret JSON responses, with a focus on the data element, while also distinguishing between metadata and status codes. Develop an understanding of how HTTP status codes and API keys work to validate and secure data access.\nWrite clean, purposeful R code to send API requests, handle responses, and parse structured data into tidy, analyzable formats."
  },
  {
    "objectID": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_empty_fillable_07072025.html#conceptual-foundation",
    "href": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_empty_fillable_07072025.html#conceptual-foundation",
    "title": "Session 2: Weather Data - OpenWeatherAPI - Empty",
    "section": "",
    "text": "Part A. Theoretical ideas of APIs\n\n\nIt is the ability for software to communicate\n\n\n\nAPI Call (PhoenixNap.com)\n\n\n\n\n\n\n\n\nLets go deeper into understanding Define:\nClient (request) –&gt; API –&gt; Server –&gt; Database\nClient &lt;– API &lt;– Server (response) &lt;– Database\n\n\n\nGATO365 API Request & Response\n\n\n\n\n\n\n\n\nLets spend some more time on the request and response\nThe client sends a request asking for info (like Taylor Swift or today’s weather). This request includes:\n\nA URL (e.g., with parameters like ?q=San+Luis+Obispo)\nPossibly an API key\nA method (e.g., GET or POST)\n\nThe request are in the form of a url string (more on this soon…)\nThe server then returns a response which contains:\n\ndata (temperature, artist name, forecast, etc.)\nmetadata (This is information about the response.)\nstatus code (Tells you whether the request was successful)\n\nThis information is traditionally provided in JSON Format. (more on this soon…)\n\n\n\n\n\n\n\nLet’s focus on what the response is 1st (what we receive from the server):\nBelow is an example GIF of the information sent from the server in JSON format:\n\n\n\n\nGATO365 Anatomy JSON\n\n\n\n\n\n\n\n\nStatus codes tell you what happened with your request:\n\n100s: Info\n200s: Success (highlight: 200 OK)\n300s: Redirect\n400s: Client error\n500s: Server error\n\nNote 5:\n\nEmphasize: In most data APIs, your goal is to get a 200 response.\nUse examples like making up a nonexistent city or artist to show how an API might respond with a 400 or 404. |\n\n** Client Request *************************\n\n\n\n\n\n\nWhat type of client requests can we make?\nCRUD Framework (Create, Read, Update, Delete)\n\nThough APIs allow all four, Read (GET) is most common in data science.\nRESTful API mapping:\n\nCreate → POST\nRead → GET\nUpdate → PUT/PATCH\nDelete → DELETE\n\n\n\n\n\n\n\n\n\n\n\n\n\nGATO365 Get Request\n\n\nHere is the description of a GET request from that perspective.\n\nClient constructs a request for a resource.\nAPI receives and validates the client's request.\nServer locates the requested data within database.\nClient receives requested data from the server.\n\n\n\n\n\n\n\n\n\n\nGATO365 Post Request\n\n\n\nHere is the description of a POST request.\n\nClient sends new data within the request body.\nAPI receives and validates the client's new data.\nServer creates a new record in the database.\nClient receives a confirmation for the new record.\n\n\n\n\n\n\n\n\n\n[[Based on time do One of the three steps]]\n[[1. email attendees to go to the weather website and get API key or whatever information needed before the conference. Create a video that’s displaying how to do this]]\n[[1a. Discuss .Renviron.txt, how we use: to create and edit API key: usethis::edit_r_environ()]]\n[[1b. Use Sys.getenv(\"API_KEY\") to see API in console]]\n[[Note that you have to use the 1a to see the api key again]] Restart R\n[[2. have attendees get the key during the break session if they have not done so already]]\n[[3. use a common key, but tell them it is bad practice]]\n[[regardless of the decision made of the three options above have attendees store information in the environment file]]\n\n\n\n\n\n\n\nSo what we’re going to first do is create our request and the most ideal way.\nA request defined by a URL, which contains both:\n\nThe endpoint (base address of the API)\nThe query string (additional key-value pairs that modify the request)\n\nWe often need to glue strings together to build this full URL dynamically.\nA request is not “automatically” turned into JSON when sent — it’s the response that’s usually formatted as JSON. The request is often URL-encoded if it’s a GET.\n\n\n\n\n\n\n\n\nWhen we use a URL like ...?q=San+Luis+Obispo&appid=..., we’re constructing a query string, which is appended to the base URL.\nThink of this as “asking the question”—the query string shapes the request.\nThe server receives the request, processes it, and responds with structured data (typically JSON).\nWe’re not sending JSON in this case—we’re sending a URL with parameters. JSON is returned to us as a response format.\n\n\n\n\nGATO365 Post Request\n\n\n\n\n\n\n\n\n\n\nLoad Libraries\n\n## EMPTY VERSION\nlibrary(httr2)\nlibrary(glue)\n\nSpecify city to query\n\n## EMPTY VERSION\ncity_name &lt;- \"San Luis Obispo\"\n\nUse this url as the base url, https://api.openweathermap.org/data/2.5/weather? (Imporve Instructions using empty and fill)\n\n## EMPTY VERSION\ncurrent_weather_url &lt;- glue(\"https://api.openweathermap.org/data/2.5/weather?\",                        ## Base URL / Endpoint\n                            \"q=\", URLencode(city_name),         ## City Name\n                            \"&appid=\", Sys.getenv(\"API_KEY\"), ## Use of API Key\n                            \"&units=imperial\")                 ## Specify the units\n\n\n\n\nPrint the url string\n\n## EMPTY VERSION\ncurrent_weather_url\n\nMake the request formal\n\n## EMPTY VERSION\nreq &lt;- request(current_weather_url)\n\nPrint the formal request\n\n## EMPTY VERSION\nreq\n\n\n\n\n\n\n\nStep 1: Build Request Object\n\n## EMPTY VERSION\nreq &lt;- request(\"https://api.openweathermap.org/data/2.5/weather?\") %&gt;%\n  req_url_query(\n    q = city_name,\n    appid = Sys.getenv(\"API_KEY\"),\n    units = \"imperial\"\n  )\n\n\n## EMPTY VERSION\nreq\n\n\n\n\nStep 2: Make request\n\n## EMPTY VERSION\nresponse &lt;- req_perform(req)\n\n\n## EMPTY VERSION\n# ____\n\n\nGET: The method used to request data.\nURL: The full address the request was sent to.\nStatus: 200 OK: The request was successful.\nContent-Type: The returned data is in JSON format.\nBody: The weather data is downloaded and in memory.\n\n\n\n\nNot a step: View content Type\n\n## EMPTY VERSION\n# content_type &lt;- resp_content_type(____)\n\n\n## EMPTY VERSION\n# ____\n\n\n\n\nStep 3: Process the Response\n\n## EMPTY VERSION\n# library(____)\n\n\n## EMPTY VERSION\n## IF the status code is 200 we are good\n# if (resp_status(____) == ____) {\n#   \n#   # Parse JSON\n#   result &lt;- resp_body_json(____)\n#   \n#   # Print Results as JSON but in R it is a list\n#   print(____)\n#   \n#   #---------------------------------------\n#   \n#   # Convert to Data Frame directly\n#   current_weather_df &lt;- as.data.frame(____)\n#   \n#   # Print Results as Data Frame, using dplyr\n#   current_weather_df %&gt;% \n#     select(____, ____, ____, ____, ____) %&gt;% \n#     print()\n#   \n#   \n# ## ELSE state there is an Error\n# } else {\n#   cat(\"Failed. Status code:\", resp_status(____), \"\\n\")\n# }\n\n\n\n\n\n\n\n\nProvide instructions (Improve Instructions using empty and fill)\n\n## EMPTY VERSION\n## Step 1: Define function \"get_city_coords\" that accepts the parameter \"city\"\n# get_city_coords &lt;- function(____){\n#   \n# ## Step 2: Create API request URL\n#   geo_url &lt;- glue(\n#     \"____\", # Endpoint\n#     \"q=\", URLencode(____),\n#     \"&limit=1&appid=\", Sys.getenv(\"____\")\n#   )\n#   \n# ## Step 3: Use req_perform() and request() to call the API with the URL request  \n#   geo_response &lt;- req_perform(request(____))\n#   \n# ## Step 4: If the status code is 200 (OK), parse the response\n#   if (resp_status(____) == ____) {\n#     geo_data_df &lt;- resp_body_json(____) %&gt;% \n#       as.data.frame()\n#     \n#   ## Step 5: Assess if the output has 0 length, meaning no result.\n#     if (length(____) == 0) {\n#       stop(\"____\")\n#     }\n#     \n#   ## Step 6: Round latitude and longitude to 2 decimal places.\n#     mod_1_geo_data_df &lt;- geo_data_df %&gt;% \n#       mutate(lat = round(____, 2),\n#              lon = round(____, 2))\n# \n#   ## Step 7: Select and rename columns\n#     mod_2_geo_data_df &lt;- mod_1_geo_data_df %&gt;% \n#       select(____, ____, ____, ____) %&gt;% \n#       rename(city = ____)\n#     \n#   ## Step 8: Return the final data frame\n#     return(____)\n#     \n#   }\n# }\n\n\n\n\nLets try out this new function on the city\n\n## EMPTY VERSION\n# get_city_coords(____)\n\n\n## EMPTY VERSION\n# library(____)\n# \n# # List of cities you want to geocode\n# cities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n# \n# # Use map_df() to apply the function to each city\n# map_df(____, ____)\n\n\n\n\n\n\n\n\n## EMPTY VERSION\n# library(____)\n\n\n## EMPTY VERSION\n# get_past_weather_by_city &lt;- function(____, ____) {\n#   # Step 1: Get city coordinates\n#   coords_df &lt;- get_city_coords(____)\n#   lat &lt;- coords_df$lat\n#   lon &lt;- coords_df$lon\n#   \n#   cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n#   \n#   # Step 2: Create vector of past dates\n#   date_range &lt;- as.character(today() - days(1:____))\n#   \n#   # Step 3: Define function for single date\n#   fetch_day_summary &lt;- function(____) {\n#     weather_url &lt;- glue(\n#       \"____\", # Endpoint\n#       \"lat=\", ____,\n#       \"&lon=\", ____,\n#       \"&date=\", ____,\n#       \"&appid=\", Sys.getenv(\"____\"),\n#       \"&units=imperial\"\n#     )\n#     \n#     response &lt;- req_perform(request(____))\n#     \n#     if (resp_status(____) == 200) {\n#       resp_body_json(____) %&gt;% \n#         as.data.frame() %&gt;% \n#         mutate(city = ____, date = ____)\n#     } else {\n#       warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(response)))\n#       return(NULL)\n#     }\n#   }\n#   \n#   # Step 4: Map over date_range and bind into a single data frame\n#   map_dfr(____, ____)\n# }\n\n\n\n\n\n## EMPTY VERSION\n# get_past_weather_by_city(____, ____)\n\n\n## FILLED RESPONSE\nget_past_weather_by_city(city_name, 5)\n\n\n\n\nThere are a lot of ways of doing this, I decided not to use for loop in R\n\n## EMPTY VERSION\n# num_days &lt;- 5\n# \n# # Get historical weather data for each city using map_dfr\n# all_weather_df &lt;- map_dfr(\n#   ____,\n#   ~ get_past_weather_by_city(____, ____)\n# )"
  },
  {
    "objectID": "sessions/session_2/02_Extraction_Weather_Data_API.html",
    "href": "sessions/session_2/02_Extraction_Weather_Data_API.html",
    "title": "Session 2: Weather Data - OpenWeatherAPI",
    "section": "",
    "text": "Download starter .qmd file"
  },
  {
    "objectID": "sessions/session_2/02_Extraction_Weather_Data_API.html#goals-objectives",
    "href": "sessions/session_2/02_Extraction_Weather_Data_API.html#goals-objectives",
    "title": "Session 2: Weather Data - OpenWeatherAPI",
    "section": "1. Goals & Objectives",
    "text": "1. Goals & Objectives\nBy the end of this session, participants will be able to:\n\nUnderstand the foundational concepts of APIs as a bridge for data exchange, including how they function in modern software and support real-time data extraction.\nQuery public APIs effectively, forming well-structured requests that interact with remote databases and return meaningful results.\nInterpret JSON responses, with a focus on the data element, while also distinguishing between metadata and status codes. Develop an understanding of how HTTP status codes and API keys work to validate and secure data access.\nWrite clean, purposeful R code to send API requests, handle responses, and parse structured data into tidy, analyzable formats."
  },
  {
    "objectID": "sessions/session_2/02_Extraction_Weather_Data_API.html#conceptual-foundation",
    "href": "sessions/session_2/02_Extraction_Weather_Data_API.html#conceptual-foundation",
    "title": "Session 2: Weather Data - OpenWeatherAPI",
    "section": "2. Conceptual Foundation",
    "text": "2. Conceptual Foundation\nPart A. Theoretical ideas of APIs\nNote 1:\n\nThis is not a webdeveloper nor a CS course but with a decent understanding of the logic, you and your students will appreciate the utilizartion of web scrapiing more\n\n\nP1. What is an API (again)?\nIt is the ability for software to communicate\n\n\n\nAPI Call (PhoenixNap.com)\n\n\n\nQ1: What is its utility of APIs? (multiple choice)\n\nNote 2:\n\nThis image is overly simplified in that a client left makes request through an api to a server/database then the server/database provides responses\nA client and server can exist on the same computer. This is often what’s happening in local development (e.g., querying a local database from R)\n\n\n\n\n\n\nP2. API Logic\nLets go deeper into understanding Define:\nClient (request) –&gt; API –&gt; Server –&gt; Database\nClient &lt;– API &lt;– Server (response) &lt;– Database\n\n\n\nGATO365 API Request & Response\n\n\nQ2. Matching You might show this flow visually and say:\n“The [API] is the waiter.”\n“The [client] is the customer.”\n“The [server] is the kitchen.”\n“The [database] is the fridge or pantry.”\nNote 3:\n\nAction: Client makes a request\nAction: Server queries Database provides a response\n\n\n\n\n\n\nP3. Requests and Responses\nLets spend some more time on the request and response\nThe client sends a request asking for info (like Taylor Swift or today’s weather). This request includes:\n\nA URL (e.g., with parameters like ?q=San+Luis+Obispo)\nPossibly an API key\nA method (e.g., GET or POST)\n\nThe request are in the form of a url string (more on this soon…)\nThe server then returns a response which contains:\n\ndata (temperature, artist name, forecast, etc.)\nmetadata (This is information about the response.)\nstatus code (Tells you whether the request was successful)\n\nThis information is traditionally provided in JSON Format. (more on this soon…)\n\n\n\n\n\nP4. Anatomy of JSON\n\nLet’s focus on what the response is 1st (what we receive from the server):\nBelow is an example GIF of the information sent from the server in JSON format:\n\n\n\n\nGATO365 Anatomy JSON\n\n\nNote 4:\n\nWhen we send a request to an API, we get a response body, which includes the content — typically JSON — divided into data (what we wanted), metadata (info about the data), and a status_code telling us if the request worked.\n\n\n\n\n\n\nP5. Status Code\nStatus codes tell you what happened with your request:\n\n100s: Info\n200s: Success (highlight: 200 OK)\n300s: Redirect\n400s: Client error\n500s: Server error\n\nNote 5:\n\nEmphasize: In most data APIs, your goal is to get a 200 response.\nUse examples like making up a nonexistent city or artist to show how an API might respond with a 400 or 404. |\n\n** Client Request *************************\n\n\n\n\n\nP6. CRUD Framework\nWhat type of client requests can we make?\nCRUD Framework (Create, Read, Update, Delete)\n\nThough APIs allow all four, Read (GET) is most common in data science.\nRESTful API mapping:\n\nCreate → POST\nRead → GET\nUpdate → PUT/PATCH\nDelete → DELETE\n\n\n\nNotes:\n\nGET retrieves existing data from a server.\nPOST submits new data to the server.\nUPDATE modifies existing data on the server.\nDELETE removes a resource from the server.\nSometimes we have to implement a post to be able to gain an access token\n\n\n\n\n\n\nP7. GET Request\n\n\n\nGATO365 Get Request\n\n\nHere is the description of a GET request from that perspective.\n\nClient constructs a request for a resource.\nAPI receives and validates the client's request.\nServer locates the requested data within database.\nClient receives requested data from the server.\n\n\n\n\n\n\nP8. Post Request\n\n\n\nGATO365 Post Request\n\n\n\nHere is the description of a POST request.\n\nClient sends new data within the request body.\nAPI receives and validates the client's new data.\nServer creates a new record in the database.\nClient receives a confirmation for the new record.\n\n\n\n\n\n\n\nP9. Setup API_Key\n\n[[Based on time do One of the three steps]]\n[[1. email attendees to go to the weather website and get API key or whatever information needed before the conference. Create a video that’s displaying how to do this]]\n[[1a. Discuss .Renviron.txt, how we use: to create and edit API key: usethis::edit_r_environ()]]\n[[1b. Use Sys.getenv(\"API_KEY\") to see API in console]]\n[[Note that you have to use the 1a to see the api key again]] Restart R\n[[2. have attendees get the key during the break session if they have not done so already]]\n[[3. use a common key, but tell them it is bad practice]]\n[[regardless of the decision made of the three options above have attendees store information in the environment file]]\n\nNote 8:\nThere are many ways of doing this, but I’m going to stick with using tidyverse functions.I’m going to show you two ways to actually implement the query using the one way of one of the ways of doing this within a tiny verse using string glue\n\n\n\n\n\nP10. Requests, URLs & Queries\n\nSo what we’re going to first do is create our request and the most ideal way.\nA request defined by a URL, which contains both:\n\nThe endpoint (base address of the API)\nThe query string (additional key-value pairs that modify the request)\n\nWe often need to glue strings together to build this full URL dynamically.\nA request is not “automatically” turned into JSON when sent — it’s the response that’s usually formatted as JSON. The request is often URL-encoded if it’s a GET.\n\nNote:\n\nAn endpoint is the specific URL where an API can be accessed. Think of it as the main address for a particular set of resources. It’s the stable part of the URL that doesn’t change from one request to the next.\nA query string is used to customize the request by filtering or specifying the exact data you want from an endpoint. It always starts with a question mark (?) and is made up of key-value pairs.\n\n\n\n\n\n\nP11: What Happens Under the Hood\n\nWhen we use a URL like ...?q=San+Luis+Obispo&appid=..., we’re constructing a query string, which is appended to the base URL.\nThink of this as “asking the question”—the query string shapes the request.\nThe server receives the request, processes it, and responds with structured data (typically JSON).\nWe’re not sending JSON in this case—we’re sending a URL with parameters. JSON is returned to us as a response format.\n\n Note 9:\n\nA URL is constructed from a base URL (the endpoint) and a query string.\nThe client sends a GET request using this URL to the API.\nThe API then relays this request to the server.\nAfter processing the request, the server sends a response, which includes a status code and the requested data (often formatted as JSON), back to the client.\n\n[[Transition to openweather API Requests]]\n(TODO: Remove APIServer)\n\n\n\n\n\nP12. Two Ways to Build Request Objects\n\nMethod 1: Manual String Gluing\nLoad Libraries\n\n## EMPTY VERSION\n# library(____)\n# library(____)\n\n\n## FILLED RESPONSE\nlibrary(httr2)       # Makes web requests\nlibrary(glue)        # Glue Strings\n\nSpecify city to query\n\n## EMPTY VERSION\n# city_name &lt;- \"____\"\n\n\n## FILLED RESPONSE\ncity_name &lt;- \"San Luis Obispo\"\n\nUse this url as the base url, https://api.openweathermap.org/data/2.5/weather? (Imporve Instructions using empty and fill)\n\n## EMPTY VERSION\n# current_weather_url &lt;- glue(\"____\",                        ## Base URL / Endpoint\n#                             \"q=\", URLencode(____),         ## City Name\n#                             \"&appid=\", Sys.getenv(\"____\"), ## Use of API Key\n#                             \"&units=____\")                 ## Specify the units\n\n\n## FILLED RESPONSE\ncurrent_weather_url &lt;- glue(\"https://api.openweathermap.org/data/2.5/weather?\",\n                            \"q=\", URLencode(city_name),\n                            \"&appid=\", Sys.getenv(\"API_KEY\"),\n                            \"&units=imperial\")\n\nNote:\n\nEndpoint: The base URL that specifies the location of the API resource you want to access.\nq: The query parameter, used for the main search term (in this case, the city name).\nappid: The parameter for your unique API key, which is used to authenticate your request.\nunits=imperial: A parameter that requests the API to return data in Imperial units (e.g., Fahrenheit).\nURLencode(): A function that formats text, like city names with spaces, to be safely included in a URL.\nimperial: A parameter that tells the API to return data in Imperial units (e.g., Fahrenheit).\n\n\n\n\nPrint the url string\n\n## EMPTY VERSION\n# ____\n\n\n## FILLED RESPONSE\ncurrent_weather_url\n\nMake the request formal\n\n## EMPTY VERSION\n# req &lt;- request(____)\n\n\n## FILLED RESPONSE\nreq &lt;- request(current_weather_url)\n\nPrint the formal requst\n\n## EMPTY VERSION\n# ____\n\n\n## FILLED RESPONSE\nreq\n\nNote 13: Look at the request\n\nThis method shows the anatomy of the URL explicitly.\nGreat for emphasizing how query parameters are constructed using strings.\nHelps reinforce the idea of “asking a question via the URL.”\n\nNote 14:\n\nWe are going to do it again in a different way but we are going to process the response further here because\n\nI wanted you to understand the anatomy of the URL\nHave multiple ways of doing the same thing\n\n\n\n\n\n\n\nMethod 2: Using req_url_query()\nStep 1: Build Request Object\n\n## EMPTY VERSION\n# req &lt;- request(\"____\") %&gt;%  \n#   req_url_query(\n#     q = ____,\n#     appid = Sys.getenv(\"____\"),\n#     units = \"____\"\n#   )\n\n\n## FILLED RESPONSE\nreq &lt;- request(\"https://api.openweathermap.org/data/2.5/weather\") %&gt;% \n  req_url_query(\n    q = city_name,\n    appid = Sys.getenv(\"API_KEY\"),\n    units = \"imperial\"\n  )\n\n\n## EMPTY VERSION\n# ____\n\n\n## FILLED RESPONSE\nreq\n\nNote 15:\n\nThis method abstracts away the string building.\nIt’s cleaner and reduces chances of typos or formatting errors.\nTeaches students to treat query arguments like named inputs.\nYou can still inspect the built URL using req$url.\n\n\n\n\nStep 2: Make request\n\n## EMPTY VERSION\n# response &lt;- req_perform(____)\n\n\n## FILLED RESPONSE\nresponse &lt;- req_perform(req)\n\n\n## EMPTY VERSION\n# ____\n\n\n## FILLED RESPONSE\nresponse\n\n\nGET: The method used to request data.\nURL: The full address the request was sent to.\nStatus: 200 OK: The request was successful.\nContent-Type: The returned data is in JSON format.\nBody: The weather data is downloaded and in memory.\n\nNote 16:\n\nGET: This indicates that the request method used was GET, which is used to retrieve data from a server.\nURL: This is the full web address the request was sent to, including the endpoint and all the query parameters (like the city, your API key, and the units).\nStatus: 200 OK: This is the HTTP status code. A 200 OK status means your request was successfully received, understood, and processed.\nContent-Type: application/json: This header tells you the format of the data in the response body. In this case, the weather data was sent back in JSON format.\nBody: In memory (514 bytes): This confirms that the data returned by the API (the actual weather information) has been downloaded and is stored in your computer’s memory, ready to be used.\n\n\n\n\nNot a step: View content Type\n\n## EMPTY VERSION\n# content_type &lt;- resp_content_type(____)\n\n\n## FILLED RESPONSE\ncontent_type &lt;- resp_content_type(response)\n\n\n## EMPTY VERSION\n# ____\n\n\n## FILLED RESPONSE\ncontent_type\n\n\n\n\nStep 3: Process the Response\n\n## EMPTY VERSION\n# library(____)\n\n\n## FILLED RESPONSE\nlibrary(dplyr)\n\n[[Provide Instructions]] (Imporve Instructions using empty and fill)\n\n## EMPTY VERSION\n## IF the status code is 200 we are good\n# if (resp_status(____) == ____) {\n#   \n#   # Parse JSON\n#   result &lt;- resp_body_json(____)\n#   \n#   # Print Results as JSON but in R it is a list\n#   print(____)\n#   \n#   #---------------------------------------\n#   \n#   # Convert to Data Frame directly\n#   current_weather_df &lt;- as.data.frame(____)\n#   \n#   # Print Results as Data Frame, using dplyr\n#   current_weather_df %&gt;% \n#     select(____, ____, ____, ____, ____) %&gt;% \n#     print()\n#   \n#   \n# ## ELSE state there is an Error\n# } else {\n#   cat(\"Failed. Status code:\", resp_status(____), \"\\n\")\n# }\n\n\n## FILLED RESPONSE\n## IF the status code is 200 we are good\nif (resp_status(response) == 200) {\n  \n  # Parse JSON\n  result &lt;- resp_body_json(response)\n  \n  # Print Results as JSON but in R it is a list\n  print(result)\n  \n  #---------------------------------------\n  \n  # Convert to Data Frame directly\n  current_weather_df &lt;- as.data.frame(result)\n  \n  # Print Results as Data Frame, using dplyr\n  current_weather_df %&gt;% \n    select(name, coord.lon, coord.lat, weather.main, main.temp) %&gt;% \n    print()\n  \n  \n## ELSE state there is an Error\n} else {\n  cat(\"Failed. Status code:\", resp_status(response), \"\\n\")\n}\n\nNote 18:\n\nSelect Key Information: The API gives us much more data than we need. We use select() from the dplyr library to pull out only the specific columns we’re interested in, creating a clean, final table.\nCheck for Success First: The if (resp_status(response) == 200) statement is crucial. It’s our safety check to make sure the request was successful before we try to use the data. If the status is anything else, our code prints an error and stops.\nExtract and Parse the Data: Once we confirm success, we use resp_body_json(). This function takes the raw body of the response, parses the JSON text, and converts it into a structured R list. This list contains all the weather data returned by the API.\nConvert to a Data Frame: While the list is useful, a data frame is often easier to work with. We use as.data.frame() to transform the nested list of weather data into a standard R data frame.\n\n\n\n\n\n\n\nP13. Let’s dissect & build a function 1\nProvide instructions (Improve Instructions using empty and fill)\n\n## EMPTY VERSION\n## Step 1: Define function \"get_city_coords\" that accepts the parameter \"city\"\n# get_city_coords &lt;- function(____){\n#   \n# ## Step 2: Create API request URL\n#   geo_url &lt;- glue(\n#     \"____\", # Endpoint\n#     \"q=\", URLencode(____),\n#     \"&limit=1&appid=\", Sys.getenv(\"____\")\n#   )\n#   \n# ## Step 3: Use req_perform() and request() to call the API with the URL request  \n#   geo_response &lt;- req_perform(request(____))\n#   \n# ## Step 4: If the status code is 200 (OK), parse the response\n#   if (resp_status(____) == ____) {\n#     geo_data_df &lt;- resp_body_json(____) %&gt;% \n#       as.data.frame()\n#     \n#   ## Step 5: Assess if the output has 0 length, meaning no result.\n#     if (length(____) == 0) {\n#       stop(\"____\")\n#     }\n#     \n#   ## Step 6: Round latitude and longitude to 2 decimal places.\n#     mod_1_geo_data_df &lt;- geo_data_df %&gt;% \n#       mutate(lat = round(____, 2),\n#              lon = round(____, 2))\n# \n#   ## Step 7: Select and rename columns\n#     mod_2_geo_data_df &lt;- mod_1_geo_data_df %&gt;% \n#       select(____, ____, ____, ____) %&gt;% \n#       rename(city = ____)\n#     \n#   ## Step 8: Return the final data frame\n#     return(____)\n#     \n#   }\n# }\n\n\n## FILLED RESPONSE\n## Step 1: Define function \"get_city_coords\" that accepts the parameter \"city\"\nget_city_coords &lt;- function(city){\n  \n## Step 2: Create API request URL\n  \n  geo_url &lt;- glue(\n  \"http://api.openweathermap.org/geo/1.0/direct?\",\n  \"q=\", URLencode(city),\n  \"&limit=1&appid=\", Sys.getenv(\"API_KEY\")\n)\n ## Step 3: Use req_perform() and request() to call the API with the URL request \n\ngeo_response &lt;- req_perform(request(geo_url))\n  \n\n ## Step 4: If the status code is 200 (OK), use resp_body_json() to parse our response and as.data.frame to coerce it to data.frame.\n\nif (resp_status(geo_response) == 200) {\n  geo_data_df &lt;- resp_body_json(geo_response) %&gt;% \n    as.data.frame()\n  \n  \n  ## Step 5: Assess if the output has 0 length, meaning no result. If so, stop and display an error message.  \n  \n  if (length(geo_data_df) == 0) {\n    stop(\"City not found. Please check the city name.\")\n  }\n  \n  ## Step 6: Assign latitude and longitude to variables, and use round() to clip it down to 2 decimal places.\n  mod_1_geo_data_df &lt;- geo_data_df %&gt;% \n    mutate(lat = round(lat,2),\n           lon = round(lon,2))\n\n  ## Step 7: Select Certain Columns (chaptgpt)\n  mod_2_geo_data_df &lt;- mod_1_geo_data_df %&gt;% \n    select(country,name,lat,lon) %&gt;% \n    rename(city = name)\n  \n  ## Step 8: Return data frame with the country, city name and latitude / longitude.  \n  return(mod_2_geo_data_df)\n  \n  \n   }\n}\n\nNote 20:\nExplain the function and fill in blanks Emphasize error handling\n\n\n\nLets try out this new function on the city\n\n## EMPTY VERSION\n# get_city_coords(____)\n\n\n## FILLED RESPONSE\nget_city_coords(city_name)\n\nNote 21: Explain output\nLets Look at multiple cities using the the map_df function within the purrr package Provide instructions (Improve Instructions using empty and fill)\n\n## EMPTY VERSION\n# library(____)\n# \n# # List of cities you want to geocode\n# cities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n# \n# # Use map_df() to apply the function to each city\n# map_df(____, ____)\n\n\n## FILLED RESPONSE\nlibrary(purrr)\n\n# List of cities you want to geocode\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n\n# Use walk() from purrr to apply the function to each city\nmap_df(cities, get_city_coords)\n\nNote 22: Explain Task Explain map_df Explain output\n\n\n\n\n\nP14. Let’s dissect & build a function 2\n(TODO: Remove certain element of functions that are needed to understand the function) (highltight the function days in this)\n\n## EMPTY VERSION\n# library(____)\n\n\n## FILLED RESPONSE\nlibrary(lubridate)   # Time and date handling\n\nNote 23: Explain need for lubridate Emphasize the new endpoint (https://api.openweathermap.org/data/3.0/onecall/day_summary?) an how it is a paid subscription\n\n## EMPTY VERSION\n# get_past_weather_by_city &lt;- function(____, ____) {\n#   # Step 1: Get city coordinates\n#   coords_df &lt;- get_city_coords(____)\n#   lat &lt;- coords_df$lat\n#   lon &lt;- coords_df$lon\n#   \n#   cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n#   \n#   # Step 2: Create vector of past dates\n#   date_range &lt;- as.character(today() - days(1:____))\n#   \n#   # Step 3: Define function for single date\n#   fetch_day_summary &lt;- function(____) {\n#     weather_url &lt;- glue(\n#       \"____\", # Endpoint\n#       \"lat=\", ____,\n#       \"&lon=\", ____,\n#       \"&date=\", ____,\n#       \"&appid=\", Sys.getenv(\"____\"),\n#       \"&units=imperial\"\n#     )\n#     \n#     response &lt;- req_perform(request(____))\n#     \n#     if (resp_status(____) == 200) {\n#       resp_body_json(____) %&gt;% \n#         as.data.frame() %&gt;% \n#         mutate(city = ____, date = ____)\n#     } else {\n#       warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(response)))\n#       return(NULL)\n#     }\n#   }\n#   \n#   # Step 4: Map over date_range and bind into a single data frame\n#   map_dfr(____, ____)\n# }\n\n\n## FILLED RESPONSE\nget_past_weather_by_city &lt;- function(city, days) {\n  # Step 1: Get city coordinates\n  coords_df &lt;- get_city_coords(city)\n  lat &lt;- coords_df$lat\n  lon &lt;- coords_df$lon\n  \n  cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n  \n  # Step 2: Create vector of past dates\n  date_range &lt;- as.character(today() - days(1:days))\n  \n  # Step 3: Define function for single date\n  fetch_day_summary &lt;- function(date) {\n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", date,\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\"\n    )\n    \n    response &lt;- req_perform(request(weather_url))\n    \n    if (resp_status(response) == 200) {\n      resp_body_json(response) %&gt;% \n        as.data.frame() %&gt;% \n        mutate(city = city, date = date)\n    } else {\n      warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(response)))\n      return(NULL)\n    }\n  }\n  \n  # Step 4: Map over date_range and bind into a single data frame\n  map_dfr(date_range, fetch_day_summary)\n}\n\nNote 24: (Explanation of filling out code and what code does)\n\n## EMPTY VERSION\n# get_past_weather_by_city(____, ____)\n\n\n## FILLED RESPONSE\nget_past_weather_by_city(city_name, 5)\n\nNote 25: Explain code and output\n\n\n\nThere are alot of ways of doing this, I decided not to use for loop in R\n\n## EMPTY VERSION\n# num_days &lt;- 5\n# \n# # Get historical weather data for each city using map_dfr\n# all_weather_df &lt;- map_dfr(\n#   ____,\n#   ~ get_past_weather_by_city(____, ____)\n# )\n\n\n## FILLED RESPONSE\nnum_days &lt;- 5\n\n# Get historical weather data for each city using map_dfr\nall_weather_df &lt;- map_dfr(\n  cities,\n  ~ get_past_weather_by_city(.x, num_days)\n)\n\nNote 26: (Explanation of map_dfr and each element of input)\n\n\n\nP15. Hands on Activities\n[[ GENERAL INSRUCTIONS]]\n[[Potentially Two versions: 1 fillable and 1 blank]]\n\nHands Activity 1:get_city_current_weather()\n[[INSRUCTIONS, highlight the purpose and different endpoint]]\nStep 1: Retrieves current weather conditions for a single city. Demonstrates basic GET usage and parsing a flat JSON structure.\n(TODO: Highlight Endpoint: https://api.openweathermap.org/data/2.5/weather?)\n\n## EMPTY VERSION\n# get_city_current_weather &lt;- function(____) {\n#   url &lt;- glue::glue(\n#     \"____\", # Endpoint\n#     \"q=\", URLencode(____),\n#     \"&appid=\", Sys.getenv(\"____\"),\n#     \"&units=imperial\"\n#   )\n#   \n#   response &lt;- request(____) %&gt;% req_perform()\n#   \n#   if (resp_status(____) == 200) {\n#     response %&gt;% \n#       resp_body_json() %&gt;% \n#       purrr::pluck(\"____\") %&gt;% \n#       tibble::as_tibble() %&gt;% \n#       dplyr::select(____, ____) %&gt;% \n#       dplyr::mutate(\n#         city = ____,\n#         description = resp_body_json(____) %&gt;% purrr::pluck(\"____\", 1, \"____\")\n#       ) %&gt;% \n#       dplyr::select(____, ____, ____, ____)\n#   } else {\n#     warning(\"Failed to retrieve current weather for \", ____)\n#     return(NULL)\n#   }\n# }\n\n\n## FILLED RESPONSE\nget_city_current_weather &lt;- function(city) {\n  url &lt;- glue::glue(\n    \"https://api.openweathermap.org/data/2.5/weather?\",\n    \"q=\", URLencode(city),\n    \"&appid=\", Sys.getenv(\"API_KEY\"),\n    \"&units=imperial\"\n  )\n  \n  response &lt;- request(url) %&gt;% req_perform()\n  \n  if (resp_status(response) == 200) {\n    response %&gt;% \n      resp_body_json() %&gt;% \n      purrr::pluck(\"main\") %&gt;% \n      tibble::as_tibble() %&gt;% \n      dplyr::select(temp, humidity) %&gt;% \n      dplyr::mutate(\n        city = city,\n        description = resp_body_json(response) %&gt;% purrr::pluck(\"weather\", 1, \"description\")\n      ) %&gt;% \n      dplyr::select(city, temp, humidity, description)\n  } else {\n    warning(\"Failed to retrieve current weather for \", city)\n    return(NULL)\n  }\n}\n\n\n\n\nStep 2: Run for a Single City (e.g., “Atlanta”)\n\n## EMPTY VERSION\n# get_city_current_weather(\"____\")\n\n\n## FILLED RESPONSE\nget_city_current_weather(\"Atlanta\")\n\nStep 3: Run for a Vector of Cities Using purrr::map_dfr()\n\n## EMPTY VERSION\n# cities &lt;-  c(\"San Francisco\", \"Minneapolis\", \"St. Louis\", \"Savannah\", \"Boulder\", \"Washington D.C.\", \"Kansas City\", \"Orlando\")\n# \n# weather_df &lt;- purrr::map_dfr(____, ____)\n\n\n## FILLED RESPONSE\ncities &lt;-  c(\"San Francisco\", \"Minneapolis\", \"St. Louis\", \"Savannah\", \"Boulder\", \"Washington D.C.\", \"Kansas City\", \"Orlando\")\n\nweather_df &lt;- purrr::map_dfr(cities, get_city_current_weather)\n\n\n\n\n\n\n\nHands Activity 2: get_city_forecast_5day()\n[[INSRUCTIONS, highlight the purpose and different endpoint]]\nPurpose: Retrieves the next 5 days of forecast data (in 3-hour intervals). This introduces nested lists and flattening structures.\nNote: we are now getting times as well (TODO: Highlight ENdpoint: https://api.openweathermap.org/data/2.5/forecast?)\n\n## EMPTY VERSION\n# get_city_forecast_5day &lt;- function(____) {\n#   url &lt;- glue::glue(\n#     \"____\", # Endpoint\n#     \"q=\", URLencode(____),\n#     \"&appid=\", Sys.getenv(\"____\"),\n#     \"&units=imperial\"\n#   )\n#   \n#   response &lt;- httr2::req_perform(httr2::request(____))\n#   \n#   if (httr2::resp_status(____) == 200) {\n#     response %&gt;% \n#       resp_body_json() %&gt;% \n#       purrr::pluck(\"____\") %&gt;% \n#       purrr::map_dfr(\n#         ~ tibble::tibble(\n#             city = ____,\n#             timestamp = .x$____,\n#             temp = .x$____$____,\n#             weather = .x$____ %&gt;% purrr::pluck(1, \"____\")\n#         )\n#       )\n#   } else {\n#     warning(\"Failed to retrieve forecast for \", ____)\n#     return(NULL)\n#   }\n# }\n\n\n## FILLED RESPONSE\nget_city_forecast_5day &lt;- function(city) {\n  url &lt;- glue::glue(\n    \"https://api.openweathermap.org/data/2.5/forecast?\",\n    \"q=\", URLencode(city),\n    \"&appid=\", Sys.getenv(\"API_KEY\"),\n    \"&units=imperial\"\n  )\n  \n  response &lt;- httr2::req_perform(httr2::request(url))\n  \n  if (httr2::resp_status(response) == 200) {\n  response %&gt;% \n      resp_body_json() %&gt;% \n      purrr::pluck(\"list\") %&gt;% \n      purrr::map_dfr(\n        ~ tibble::tibble(\n            city = city,\n            timestamp = .x$dt_txt,\n            temp = .x$main$temp,\n            weather = .x$weather %&gt;% purrr::pluck(1, \"description\")\n        )\n      )\n  } else {\n    warning(\"Failed to retrieve forecast for \", city)\n    return(NULL)\n  }\n}\n\n\n\n\nTry for city notice the number of rows and columns\n\n## EMPTY VERSION\n# get_city_forecast_5day(\"____\")\n\n\n## FILLED RESPONSE\nget_city_forecast_5day(\"Atlanta\")\n\nLets Look at multiple cities\n\n## EMPTY VERSION\n# cities &lt;- c(\"Portland\", \"Salt Lake City\", \"Philadelphia\", \"Charleston\", \"Detroit\", \"Las Vegas\", \"San Diego\", \"Baltimore\")\n# \n# forecast_df &lt;- purrr::map_dfr(____, ____)\n\n\n## FILLED RESPONSE\ncities &lt;- c(\"Portland\", \"Salt Lake City\", \"Philadelphia\", \"Charleston\", \"Detroit\", \"Las Vegas\", \"San Diego\", \"Baltimore\")\n\nforecast_df &lt;- purrr::map_dfr(cities, get_city_forecast_5day)\n\n\n\n\n\n\n\nHands Activity 3: get_air_pollution_by_coords(lat, lon)\n[[INSRUCTIONS, highlight the purpose and different endpoint]] Purpose: Uses lat and lon to query current air quality. Demonstrates chaining of API requests (e.g., using get_city_coords() first), and different JSON structures.\n(TODO: Highlight ENdpoint: http://api.openweathermap.org/data/2.5/air_pollution?)\n\n## EMPTY VERSION\n# get_air_pollution_by_coords &lt;- function(____, ____) {\n#   url &lt;- glue::glue(\n#     \"____\", # Endpoint\n#     \"lat=\", ____,\n#     \"&lon=\", ____,\n#     \"&appid=\", Sys.getenv(\"____\")\n#   )\n#   \n#   response &lt;- request(____) %&gt;% req_perform()\n#   \n#   if (resp_status(____) == 200) {\n#     response %&gt;%\n#       resp_body_json() %&gt;%\n#       purrr::pluck(\"____\", 1) %&gt;%\n#       {\\(x) tibble::tibble(\n#           aqi = x$____$____,\n#           co = x$____$____,\n#           pm2_5 = x$____$____,\n#           pm10 = x$____$____\n#       )}()\n#   } else {\n#     warning(\"Failed to retrieve air pollution data for lat = \", lat, \", lon = \", lon)\n#     return(NULL)\n#   }\n# }\n\n\n## FILLED RESPONSE\nget_air_pollution_by_coords &lt;- function(lat, lon) {\n  url &lt;- glue::glue(\n    \"http://api.openweathermap.org/data/2.5/air_pollution?\",\n    \"lat=\", lat,\n    \"&lon=\", lon,\n    \"&appid=\", Sys.getenv(\"API_KEY\")\n  )\n  \n  response &lt;- request(url) %&gt;% req_perform()\n  \n  if (resp_status(response) == 200) {\n    response %&gt;%\n      resp_body_json() %&gt;%\n      purrr::pluck(\"list\", 1) %&gt;%\n      {\\(x) tibble::tibble(\n        aqi = x$main$aqi,\n        co = x$components$co,\n        pm2_5 = x$components$pm2_5,\n        pm10 = x$components$pm10\n      )}()\n  } else {\n    warning(\"Failed to retrieve air pollution data for lat = \", lat, \", lon = \", lon)\n    return(NULL)\n  }\n}\n\n\n\n\nStep-by-Step for Usage with a Data Frame, use get_city_coords to get the cities of lon and lat.\n\n## EMPTY VERSION\n# cities &lt;-  c(\"Seattle\", \"Denver\", \"Boston\", \"Austin\", \"New Orleans\", \"Miami\", \"Phoenix\", \"Nashville\")\n# city_coords_df &lt;- map_dfr(____, ____)\n\n\n## FILLED RESPONSE\ncities &lt;-  c(\"Seattle\", \"Denver\", \"Boston\", \"Austin\", \"New Orleans\", \"Miami\", \"Phoenix\", \"Nashville\")\ncity_coords_df &lt;- map_dfr(cities, get_city_coords)\n\nGet Air Pollution Data for All Cities\n\n## EMPTY VERSION\n# library(____)\n\n\n## FILLED RESPONSE\nlibrary(tidyr)\n\n\n## EMPTY VERSION\n# pollution_df &lt;- city_coords_df %&gt;% \n#   mutate(\n#     pollution = map2(____, ____, ____)\n#   ) %&gt;% \n#   unnest(____)\n\n\n## FILLED RESPONSE\npollution_df &lt;- city_coords_df %&gt;% \n  mutate(\n    pollution = map2(lat, lon, get_air_pollution_by_coords)\n  ) %&gt;% \n  unnest(pollution)"
  },
  {
    "objectID": "Tasks_Lists.html#session-1-tasks",
    "href": "Tasks_Lists.html#session-1-tasks",
    "title": "Tasks List",
    "section": "Session 1 Tasks",
    "text": "Session 1 Tasks"
  },
  {
    "objectID": "Tasks_Lists.html#session-2-tasks",
    "href": "Tasks_Lists.html#session-2-tasks",
    "title": "Tasks List",
    "section": "Session 2 Tasks",
    "text": "Session 2 Tasks\n\nPurchase API Key"
  },
  {
    "objectID": "Tasks_Lists.html#session-3-tasks",
    "href": "Tasks_Lists.html#session-3-tasks",
    "title": "Tasks List",
    "section": "Session 3 Tasks",
    "text": "Session 3 Tasks"
  },
  {
    "objectID": "Tasks_Lists.html#session-4-tasks",
    "href": "Tasks_Lists.html#session-4-tasks",
    "title": "Tasks List",
    "section": "Session 4 Tasks",
    "text": "Session 4 Tasks"
  },
  {
    "objectID": "Tasks_Lists.html#end-of-workshop-tasks",
    "href": "Tasks_Lists.html#end-of-workshop-tasks",
    "title": "Tasks List",
    "section": "End of Workshop Tasks",
    "text": "End of Workshop Tasks"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html",
    "href": "outlines_and_organization/unorganized-outline.html",
    "title": "Unorganized Thoughts About Workshop",
    "section": "",
    "text": "Deliverables: R Quarto Document\n\n\n\n8:30 - 10:15 Session 1\n10:15 - 10:45 Snack 1\n10:45 - 12:30 - Session 2\n12:30 -1:30 Lunch\n1:30 - 2:30 - Session 3\n2:30 - 3:30 - Snack 2\n3:30 - 4:15 - Session 4"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html#general-timeline",
    "href": "outlines_and_organization/unorganized-outline.html#general-timeline",
    "title": "Unorganized Thoughts About Workshop",
    "section": "",
    "text": "8:30 - 10:15 Session 1\n10:15 - 10:45 Snack 1\n10:45 - 12:30 - Session 2\n12:30 -1:30 Lunch\n1:30 - 2:30 - Session 3\n2:30 - 3:30 - Snack 2\n3:30 - 4:15 - Session 4"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html#part-1-outcomes-after-workshop",
    "href": "outlines_and_organization/unorganized-outline.html#part-1-outcomes-after-workshop",
    "title": "Unorganized Thoughts About Workshop",
    "section": "Part 1: Outcomes after workshop",
    "text": "Part 1: Outcomes after workshop\n\nJoin Newsletter and become subsribers\nPatrons puchchase product\n\n\nSubscription model where they pay $25 a month to have access information\nSeminars where I teach how to teach the material\nA textbook/virtual book\nSeminars at conferences"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html#part-2-how-to-have-great-workshopnotes-for-great-scots-presentation",
    "href": "outlines_and_organization/unorganized-outline.html#part-2-how-to-have-great-workshopnotes-for-great-scots-presentation",
    "title": "Unorganized Thoughts About Workshop",
    "section": "Part 2: How to have great workshopNotes for great Scots presentation",
    "text": "Part 2: How to have great workshopNotes for great Scots presentation\n\nMake clear goals in the beginning**\nMake activities relate to goals\nInclude time for them to do actual work\nSchedule in break time for talking about\nI will have a list of where they are coming from\nSend them message about prep\nSend them information about lelve of content to do so before hand via email\nPrepare for No Shows"
  },
  {
    "objectID": "outlines_and_organization/unorganized-outline.html#part-3-misc",
    "href": "outlines_and_organization/unorganized-outline.html#part-3-misc",
    "title": "Unorganized Thoughts About Workshop",
    "section": "Part 3: Misc",
    "text": "Part 3: Misc\n\nGet API Access Token, use univerisal API, to make sure process goes by fast\nCost of API, need to make sure they know it possible to get more info but it costs\nStatus codes, teach them about the extent of status codes that will leave them with a foundation\nThe guts of the HTML complexity really is within the code to make the data clean whereas the guts of the API complexity is how the JSON is brought into R that requires deeper thought and intention around\nwebsite/coursekata will house everything, this github will be for me to organize everything\nProcess of viewers to client should be obvious and easy done\nMaterials: 4 Separate quarto files –&gt; 4 separate jupyter notebook (potentiall 6, two additional notebooks for activity, split class to do API and others do html so they do not feel overwhelm, bring together and how them presnt their finding and how it can be imlplemnted in tjheir classes)\nHave questions, discussion questions, best practice within each quarto\nCombine strategies, i.e web scrape for ice cream sales, sports, other, and pull in weather API to join and look for patterns, Transformations and visualizations for each\nBring variety of candy\nHvae prizes for raffle\n\nHave this paradigm within each session - Goals & Objectives - Theory (I do not know what should be here for the introduction & conclusion) - Practice (what we do with API & HTML extraction) - (I do not know what to put here)"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#deliverable",
    "href": "outlines_and_organization/organized-outline.html#deliverable",
    "title": "Organized Workshop Outline",
    "section": "Deliverable",
    "text": "Deliverable\n\nFinal output: R Quarto Document(s) (4–6 files aligned with each session and additional activities)"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#general-timeline",
    "href": "outlines_and_organization/organized-outline.html#general-timeline",
    "title": "Organized Workshop Outline",
    "section": "General Timeline",
    "text": "General Timeline\n\n8:30 – 10:15 Session 1: Introduction to Data Extraction\n10:15 – 10:45 Snack Break 1\n10:45 – 12:30 Session 2: APIs and Practice\n12:30 – 1:30 Lunch\n1:30 – 2:30 Session 3: Web Scraping with HTML\n2:30 – 3:30 Snack Break 2\n3:30 – 4:15 Session 4: Deep Dives into Complete Lessons"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#session-1-introduction-to-data-extraction",
    "href": "outlines_and_organization/organized-outline.html#session-1-introduction-to-data-extraction",
    "title": "Organized Workshop Outline",
    "section": "Session 1: Introduction to Data Extraction",
    "text": "Session 1: Introduction to Data Extraction\n\nSet expectations and workshop goals\nWhy data extraction matters: relevance to real-world education\nOverview of the layout / table of contents\nDiscuss libraries used (tidyverse, rvest, httr, etc.)\nBest practices (e.g., avoiding hardcoding, consistent comments)\nAdapting to changing APIs/websites\nAnecdote: Spotify example of lost API access\nExplain tidy data: snake_case column names, correct data types\nEmphasize code flexibility — developers can change APIs overnight\nActivity: Scaffolding + Code review using example(s)\n\n\nGoals & Objectives\n\nIdentify the value of real-world data in statistics education\nDescribe the distinction between extraction, transformation, and visualization (ETv)\nRecognize challenges associated with pulling live data from the web\nApply tidy data principles to imported datasets\n\n\n\nConceptual Foundation\n\nWhy use live data?\nWhat is extraction and why it matters for teaching modern statistics\nETv framework: introduction to the first stage (Extraction)\nImportance of code flexibility and the fragility of external sources (e.g., Spotify anecdote)\nTidy data principles: naming conventions, structure, and data types\n\n\n\nHands-On Coding Activity\n\nExtracting from accessible sources such as:\n\nA static .csv hosted online (warm-up)\nA Wikipedia table using rvest and janitor\n\nIntroduce read_csv() and rvest::html_table()\nAdd cleaning steps to enforce tidy principles (snake_case, correct types)\n\n\n\nReflection\n\nHow can you introduce real-world messiness without overwhelming students?\nHow would you scaffold tidy principles at the intro-level?"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#session-2-api-fundamentals",
    "href": "outlines_and_organization/organized-outline.html#session-2-api-fundamentals",
    "title": "Organized Workshop Outline",
    "section": "Session 2: API Fundamentals",
    "text": "Session 2: API Fundamentals\n\nWhat is an API? Examples (Spotify, Weather, one bonus example)\nBasic API structure: request URLs, endpoints, tokens\nHTTP protocols and status codes\nCRUD operations (Create, Read, Update, Delete)\nAPI best practices (e.g., pagination, authentication, caching)\nTidyverse-friendly workflows (avoid deep nesting, use readable steps)\nActivity:\n\nModify API request (e.g., hometown weather)\nScaffolded practice (fill-in-the-blank)\nOptional take-home transformation/visualization\n\n\n\nGoals & Objectives\n\nExplain what an API is and how it supports data extraction\nMake requests to a public API and interpret the JSON response\nUnderstand and apply HTTP status codes and API keys\nWrite clean, readable code to extract and parse API data\n\n\n\nConceptual Foundation\n\nRESTful APIs: endpoints, parameters, keys\nAuthentication: tokens, secrets, and environment variables\nStatus codes and error handling (focus on 200, 401, 403, 404)\nJSON structure: nested data and tidy conversion\n\n\n\nHands-On Coding Activity\n\nWeather API (e.g., OpenWeatherMap):\n\nRetrieve current weather for participant’s hometown\nModify query parameters (e.g., units, location)\nParse and visualize simple results (e.g., temperature, humidity)\n\nScaffold activity: prewritten functions + one blank section\n\n\n\nReflection\n\nWhere could API data naturally integrate in your curriculum?\nWhat are the pitfalls (rate limits, authentication) students need to know?"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#session-3-html-scraping-in-r",
    "href": "outlines_and_organization/organized-outline.html#session-3-html-scraping-in-r",
    "title": "Organized Workshop Outline",
    "section": "Session 3: HTML Scraping in R",
    "text": "Session 3: HTML Scraping in R\n\nWhat is HTML and why it’s useful?\nExamples: Wikipedia, sports sites (NFL, Olympics)\nStructured vs unstructured web data\nReading the webpage source and locating tables/divs\nPractice: Going back and forth between R and browser to inspect structure\nTidy HTML scraping practices using rvest and janitor\nDifferent approaches:\n\nFull walkthrough\nPartial scaffold\n\nActivity:\n\nScrape 2 sources (in pairs), compare\nClean the data: name 3 needed transformations\nUse visualization and interpretation\nDiscuss hardcoding and fragile selectors\n\n\n\nGoals & Objectives\n\nIdentify basic HTML structure relevant for scraping\nScrape tables and text from structured web pages\nClean scraped data using tidyverse tools\nCompare different websites in terms of data accessibility\n\n\n\nConceptual Foundation\n\nHTML basics: tags, attributes, structure of web tables\nUsing rvest to read web pages and extract data\nThe importance of inspecting elements with browser tools\nStructured vs. unstructured sites: Wikipedia vs. ESPN\n\n\n\nHands-On Coding Activity\n\nScrape sports statistics from a reliable table:\n\nExample: Wikipedia table of Olympic medal counts or NBA season stats\nClean using janitor::clean_names()\nCompare scraped data from 2 sites (optional pair task)\n\n\n\n\nReflection\n\nHow could students use scraped data in a final project?\nWhat scaffolds would help students inspect and trust their source?"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#session-4-deep-dives-into-lessons",
    "href": "outlines_and_organization/organized-outline.html#session-4-deep-dives-into-lessons",
    "title": "Organized Workshop Outline",
    "section": "Session 4: Deep Dives into Lessons",
    "text": "Session 4: Deep Dives into Lessons\n\nWork through 2 complete lessons — each with:\n\nAPI-based extraction and visualization\nHTML-based extraction and transformation\n\nSplit class into two groups: API vs HTML, then reconvene\nHighlight pedagogical framing: how this can be implemented in class\nBuild reflection and discussion time: What will you bring into your course?\n\n\nGoals & Objectives\n\nReview key takeaways from API and HTML extraction\nCollaborate with peers on a structured mini-project\nReflect on how to implement extraction in your own course\nShare classroom-ready ideas with other educators\n\n\n\nRecap (Conceptual Foundation)\n\nExtraction is not “just tech” — it’s pedagogy\nAPI vs. HTML: strengths, limitations, educational value\nDesigning learning activities around messy data: student engagement, real-world relevance\n\n\n\nHands-On Coding Activity\n\nParticipants are randomly assigned:\n\nGroup A: Use an API (weather, Spotify, etc.)\nGroup B: Scrape HTML data (sports, Wikipedia, etc.)\n\nWork in small groups to clean, transform, and visualize\nPrepare a brief “teaching demo” of how this could be used in class\n\n\n\nDiscussion & Reflection\n\nWhat worked in your group?\nWhat teaching goals does this type of project help support?\nHow would you modify it for your students’ level and context?"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#supporting-infrastructure",
    "href": "outlines_and_organization/organized-outline.html#supporting-infrastructure",
    "title": "Organized Workshop Outline",
    "section": "Supporting Infrastructure",
    "text": "Supporting Infrastructure\n\nUse recent versions of all packages\nAll materials hosted on:\n\nCourseKata for workshop delivery\nGitHub repo for behind-the-scenes development organization\n\nEach session includes:\n\nCode chunk scaffolds (empty/fill-in versions)\nSolution files\nPedagogical notes and discussion questions\n\nOutput files:\n\n4–6 R Quarto files\nMatching Jupyter notebooks for hands-on use\n\nBonus activities:\n\nCombine scraping + API for multi-source projects (e.g., sports + weather + sales)"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#workshop-strategy-logistics",
    "href": "outlines_and_organization/organized-outline.html#workshop-strategy-logistics",
    "title": "Organized Workshop Outline",
    "section": "Workshop Strategy & Logistics",
    "text": "Workshop Strategy & Logistics\n\nOutcomes After Workshop\n\nInvite participants to join a newsletter and mailing list\nPromote:\n\nMonthly subscription model ($25/month) for materials\nTeaching-focused seminars\nCulturally relevant virtual textbook\nConference workshops\n\n\n\n\nPlanning Tips for Success\n\nMake goals explicit from the start\nAlign each activity with stated goals\nInclude participant work time and discussions\nSchedule breaks for casual conversation and social connection\nKnow your audience (collect pre-attendance data)\nSend prep info (software setup, expectations) in advance\nAnticipate and plan for no-shows\n\n\n\nAdditional Notes\n\nProvide API tokens ahead of time (Spotify key for all)\nExplain API rate limits and possible costs\nTeach foundational status codes\nClarify complexity differences:\n\nHTML: Cleaning/structure focus\nAPI: Parsing/logic focus (JSON)\n\nRaffle and candy: build fun and engagement\nEncourage pair programming and peer instruction\nAllow participants to present their work at the end"
  },
  {
    "objectID": "outlines_and_organization/organized-outline.html#rationale-for-combinations",
    "href": "outlines_and_organization/organized-outline.html#rationale-for-combinations",
    "title": "Organized Workshop Outline",
    "section": "Rationale for Combinations",
    "text": "Rationale for Combinations\n\nSession 1 & General Thoughts: Merged all teaching philosophy related to data extraction fundamentals here.\nAPI Examples & API Best Practices: Combined into Session 2 for cohesion and clarity.\nHTML Examples & Cleaning Strategies: Combined into Session 3 for a unified focus on web scraping.\nSession 4 & Misc Deep Dives: Naturally fit as a concluding session to synthesize all techniques and apply in pedagogically rich lessons.\nOutcomes, Planning, and Misc Strategy: Organized into coherent post-workshop and infrastructure support categories.\n\nLet me know if you want me to turn this into a .qmd, Google Doc, or a GitHub README.md."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "",
    "text": "Empowering statistics educators with real-world data skills.\nThis site houses all the materials for our hands-on workshop at USCOTS 2025.\nWe’re learning how to extract, clean, and use live data from the web and APIs—bringing authentic data science into your classroom."
  },
  {
    "objectID": "index.html#workshop-goals",
    "href": "index.html#workshop-goals",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Workshop Goals",
    "text": "Workshop Goals\n\nUnderstand why extracting dynamic data is essential in modern statistics education.\nLearn how to read HTML tables and use web APIs to collect real-world data.\nUse tidyverse tools to transform and clean this data for analysis.\nBuild confidence integrating unstructured data into your courses."
  },
  {
    "objectID": "index.html#who-this-workshop-is-for",
    "href": "index.html#who-this-workshop-is-for",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Who This Workshop Is For",
    "text": "Who This Workshop Is For\n\nStatistics educators and data science instructors\nComfortable with R and tidyverse\nCurious about web scraping, APIs, and teaching modern data practices"
  },
  {
    "objectID": "index.html#whats-inside",
    "href": "index.html#whats-inside",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "What’s Inside",
    "text": "What’s Inside\n\nSession 1 – Why and How We Extract Data\n\n️ Session 2 – Working with APIs (Weather Data)\n\nSession 3 – Scraping Sports Data from HTML\n\nSession 4 – Final Challenge and Reflection\n\nExplore these under the Sessions tab above."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Getting Started",
    "text": "Getting Started\n\nHead over to Session 1 - Introduction to begin your learning journey.\nOr, check out the Organized Outline for a roadmap of the workshop.\n\n\n##️ Tools We’ll Use\n\nrvest, httr, jsonlite, purrr, tidyverse\nJupyter via CourseKata CKHub\nRStudio / Quarto"
  },
  {
    "objectID": "index.html#quote-to-frame-the-work",
    "href": "index.html#quote-to-frame-the-work",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Quote to Frame the Work",
    "text": "Quote to Frame the Work\n\n“The world is one big dataset—if we learn how to extract it.”\n— Inspired by data educators like you"
  },
  {
    "objectID": "index.html#lets-get-started",
    "href": "index.html#lets-get-started",
    "title": "Data Extraction Workshop (USCOTS 2025)",
    "section": "Let’s Get Started!",
    "text": "Let’s Get Started!\nReady to transform how your students work with data?\n📎 Navigate using the menu above and join us in making data science real, relevant, and resilient."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Data extraction refers to retrieving and transforming data from sources like web APIs, HTML tables, or other online services into usable form. This session will cover:\nSession Table of Contents\n\n\n\n\nSession\n\n\nTopic\n\n\n\n\n\n\n1\n\n\nIntroduction to APIs & Data Extraction\n\n\n\n\n2\n\n\nWorking with Weather APIs (Querying public APIs, Parsing JSON)\n\n\n\n\n3\n\n\nScraping Sports Data via HTML Tables\n\n\n\n\n4\n\n\nFinal Project: Hands-on Extraction & Reflection"
  },
  {
    "objectID": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_empty_07072025.html",
    "href": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_empty_07072025.html",
    "title": "Session 2: Weather Data - OpenWeatherAPI - Empty",
    "section": "",
    "text": "By the end of this session, participants will be able to:\n\nUnderstand the foundational concepts of APIs as a bridge for data exchange, including how they function in modern software and support real-time data extraction.\nQuery public APIs effectively, forming well-structured requests that interact with remote databases and return meaningful results.\nInterpret JSON responses, with a focus on the data element, while also distinguishing between metadata and status codes. Develop an understanding of how HTTP status codes and API keys work to validate and secure data access.\nWrite clean, purposeful R code to send API requests, handle responses, and parse structured data into tidy, analyzable formats.\n\n\n\n\n\n\n\nPart A. Theoretical ideas of APIs\n\n\nIt is the ability for software to communicate\n\n\n\nAPI Call (PhoenixNap.com)\n\n\n\n\n\n\n\n\nLets go deeper into understanding Define:\nClient (request) –&gt; API –&gt; Server –&gt; Database\nClient &lt;– API &lt;– Server (response) &lt;– Database\n\n\n\nGATO365 API Request & Response\n\n\n\n\n\n\n\n\nLets spend some more time on the request and response\nThe client sends a request asking for info (like Taylor Swift or today’s weather). This request includes:\n\nA URL (e.g., with parameters like ?q=San+Luis+Obispo)\nPossibly an API key\nA method (e.g., GET or POST)\n\nThe request are in the form of a url string (more on this soon…)\nThe server then returns a response which contains:\n\ndata (temperature, artist name, forecast, etc.)\nmetadata (This is information about the response.)\nstatus code (Tells you whether the request was successful)\n\nThis information is traditionally provided in JSON Format. (more on this soon…)\n\n\n\n\n\n\n\nLet’s focus on what the response is 1st (what we receive from the server):\nBelow is an example GIF of the information sent from the server in JSON format:\n\n\n\n\nGATO365 Anatomy JSON\n\n\n\n\n\n\n\n\nStatus codes tell you what happened with your request:\n\n100s: Info\n200s: Success (highlight: 200 OK)\n300s: Redirect\n400s: Client error\n500s: Server error\n\nNote 5:\n\nEmphasize: In most data APIs, your goal is to get a 200 response.\nUse examples like making up a nonexistent city or artist to show how an API might respond with a 400 or 404. |\n\n** Client Request *************************\n\n\n\n\n\n\nWhat type of client requests can we make?\nCRUD Framework (Create, Read, Update, Delete)\n\nThough APIs allow all four, Read (GET) is most common in data science.\nRESTful API mapping:\n\nCreate → POST\nRead → GET\nUpdate → PUT/PATCH\nDelete → DELETE\n\n\n\n\n\n\n\n\n\n\n\n\n\nGATO365 Get Request\n\n\nHere is the description of a GET request from that perspective.\n\nClient constructs a request for a resource.\nAPI receives and validates the client's request.\nServer locates the requested data within database.\nClient receives requested data from the server.\n\n\n\n\n\n\n\n\n\n\nGATO365 Post Request\n\n\n\nHere is the description of a POST request.\n\nClient sends new data within the request body.\nAPI receives and validates the client's new data.\nServer creates a new record in the database.\nClient receives a confirmation for the new record.\n\n\n\n\n\n\n\n\n\n[[Based on time do One of the three steps]]\n[[1. email attendees to go to the weather website and get API key or whatever information needed before the conference. Create a video that’s displaying how to do this]]\n[[1a. Discuss .Renviron.txt, how we use: to create and edit API key: usethis::edit_r_environ()]]\n[[1b. Use Sys.getenv(\"API_KEY\") to see API in console]]\n[[Note that you have to use the 1a to see the api key again]] Restart R\n[[2. have attendees get the key during the break session if they have not done so already]]\n[[3. use a common key, but tell them it is bad practice]]\n[[regardless of the decision made of the three options above have attendees store information in the environment file]]\n\n\n\n\n\n\n\nSo what we’re going to first do is create our request and the most ideal way.\nA request defined by a URL, which contains both:\n\nThe endpoint (base address of the API)\nThe query string (additional key-value pairs that modify the request)\n\nWe often need to glue strings together to build this full URL dynamically.\nA request is not “automatically” turned into JSON when sent — it’s the response that’s usually formatted as JSON. The request is often URL-encoded if it’s a GET.\n\n\n\n\n\n\n\n\nWhen we use a URL like ...?q=San+Luis+Obispo&appid=..., we’re constructing a query string, which is appended to the base URL.\nThink of this as “asking the question”—the query string shapes the request.\nThe server receives the request, processes it, and responds with structured data (typically JSON).\nWe’re not sending JSON in this case—we’re sending a URL with parameters. JSON is returned to us as a response format.\n\n\n\n\nGATO365 Post Request\n\n\n\n\n\n\n\n\n\n\nLoad Libraries\n\n## EMPTY VERSION\n# library(____)\n# library(____)\n\nSpecify city to query\n\n## EMPTY VERSION\n# city_name &lt;- \"____\"\n\nUse this url as the base url, https://api.openweathermap.org/data/2.5/weather? (Imporve Instructions using empty and fill)\n\n## EMPTY VERSION\n# current_weather_url &lt;- glue(\"____\",                        ## Base URL / Endpoint\n#                             \"q=\", URLencode(____),         ## City Name\n#                             \"&appid=\", Sys.getenv(\"____\"), ## Use of API Key\n#                             \"&units=____\")                 ## Specify the units\n\n\n\n\nPrint the url string\n\n## EMPTY VERSION\n# ____\n\nMake the request formal\n\n## EMPTY VERSION\n# req &lt;- request(____)\n\nPrint the formal requst\n\n## EMPTY VERSION\n# ____\n\n\n\n\n\n\n\nStep 1: Build Request Object\n\n## EMPTY VERSION\n# req &lt;- request(\"____\") %&gt;%  \n#   req_url_query(\n#     q = ____,\n#     appid = Sys.getenv(\"____\"),\n#     units = \"____\"\n#   )\n\n\n## EMPTY VERSION\n# ____\n\n\n\n\nStep 2: Make request\n\n## EMPTY VERSION\n# response &lt;- req_perform(____)\n\n\n## EMPTY VERSION\n# ____\n\n\nGET: The method used to request data.\nURL: The full address the request was sent to.\nStatus: 200 OK: The request was successful.\nContent-Type: The returned data is in JSON format.\nBody: The weather data is downloaded and in memory.\n\n\n\n\nNot a step: View content Type\n\n## EMPTY VERSION\n# content_type &lt;- resp_content_type(____)\n\n\n## EMPTY VERSION\n# ____\n\n\n\n\nStep 3: Process the Response\n\n## EMPTY VERSION\n# library(____)\n\n\n## EMPTY VERSION\n## IF the status code is 200 we are good\n# if (resp_status(____) == ____) {\n#   \n#   # Parse JSON\n#   result &lt;- resp_body_json(____)\n#   \n#   # Print Results as JSON but in R it is a list\n#   print(____)\n#   \n#   #---------------------------------------\n#   \n#   # Convert to Data Frame directly\n#   current_weather_df &lt;- as.data.frame(____)\n#   \n#   # Print Results as Data Frame, using dplyr\n#   current_weather_df %&gt;% \n#     select(____, ____, ____, ____, ____) %&gt;% \n#     print()\n#   \n#   \n# ## ELSE state there is an Error\n# } else {\n#   cat(\"Failed. Status code:\", resp_status(____), \"\\n\")\n# }\n\n\n\n\n\n\n\n\nProvide instructions (Improve Instructions using empty and fill)\n\n## EMPTY VERSION\n## Step 1: Define function \"get_city_coords\" that accepts the parameter \"city\"\n# get_city_coords &lt;- function(____){\n#   \n# ## Step 2: Create API request URL\n#   geo_url &lt;- glue(\n#     \"____\", # Endpoint\n#     \"q=\", URLencode(____),\n#     \"&limit=1&appid=\", Sys.getenv(\"____\")\n#   )\n#   \n# ## Step 3: Use req_perform() and request() to call the API with the URL request  \n#   geo_response &lt;- req_perform(request(____))\n#   \n# ## Step 4: If the status code is 200 (OK), parse the response\n#   if (resp_status(____) == ____) {\n#     geo_data_df &lt;- resp_body_json(____) %&gt;% \n#       as.data.frame()\n#     \n#   ## Step 5: Assess if the output has 0 length, meaning no result.\n#     if (length(____) == 0) {\n#       stop(\"____\")\n#     }\n#     \n#   ## Step 6: Round latitude and longitude to 2 decimal places.\n#     mod_1_geo_data_df &lt;- geo_data_df %&gt;% \n#       mutate(lat = round(____, 2),\n#              lon = round(____, 2))\n# \n#   ## Step 7: Select and rename columns\n#     mod_2_geo_data_df &lt;- mod_1_geo_data_df %&gt;% \n#       select(____, ____, ____, ____) %&gt;% \n#       rename(city = ____)\n#     \n#   ## Step 8: Return the final data frame\n#     return(____)\n#     \n#   }\n# }\n\n\n\n\nLets try out this new function on the city\n\n## EMPTY VERSION\n# get_city_coords(____)\n\n\n## EMPTY VERSION\n# library(____)\n# \n# # List of cities you want to geocode\n# cities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n# \n# # Use map_df() to apply the function to each city\n# map_df(____, ____)\n\n\n\n\n\n\n\n\n## EMPTY VERSION\n# library(____)\n\n\n## EMPTY VERSION\n# get_past_weather_by_city &lt;- function(____, ____) {\n#   # Step 1: Get city coordinates\n#   coords_df &lt;- get_city_coords(____)\n#   lat &lt;- coords_df$lat\n#   lon &lt;- coords_df$lon\n#   \n#   cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n#   \n#   # Step 2: Create vector of past dates\n#   date_range &lt;- as.character(today() - days(1:____))\n#   \n#   # Step 3: Define function for single date\n#   fetch_day_summary &lt;- function(____) {\n#     weather_url &lt;- glue(\n#       \"____\", # Endpoint\n#       \"lat=\", ____,\n#       \"&lon=\", ____,\n#       \"&date=\", ____,\n#       \"&appid=\", Sys.getenv(\"____\"),\n#       \"&units=imperial\"\n#     )\n#     \n#     response &lt;- req_perform(request(____))\n#     \n#     if (resp_status(____) == 200) {\n#       resp_body_json(____) %&gt;% \n#         as.data.frame() %&gt;% \n#         mutate(city = ____, date = ____)\n#     } else {\n#       warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(response)))\n#       return(NULL)\n#     }\n#   }\n#   \n#   # Step 4: Map over date_range and bind into a single data frame\n#   map_dfr(____, ____)\n# }\n\n\n\n\n\n## EMPTY VERSION\n# get_past_weather_by_city(____, ____)\n\n\n## FILLED RESPONSE\nget_past_weather_by_city(city_name, 5)\n\n\n\n\nThere are a lot of ways of doing this, I decided not to use for loop in R\n\n## EMPTY VERSION\n# num_days &lt;- 5\n# \n# # Get historical weather data for each city using map_dfr\n# all_weather_df &lt;- map_dfr(\n#   ____,\n#   ~ get_past_weather_by_city(____, ____)\n# )"
  },
  {
    "objectID": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_empty_07072025.html#goals-objectives",
    "href": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_empty_07072025.html#goals-objectives",
    "title": "Session 2: Weather Data - OpenWeatherAPI - Empty",
    "section": "",
    "text": "By the end of this session, participants will be able to:\n\nUnderstand the foundational concepts of APIs as a bridge for data exchange, including how they function in modern software and support real-time data extraction.\nQuery public APIs effectively, forming well-structured requests that interact with remote databases and return meaningful results.\nInterpret JSON responses, with a focus on the data element, while also distinguishing between metadata and status codes. Develop an understanding of how HTTP status codes and API keys work to validate and secure data access.\nWrite clean, purposeful R code to send API requests, handle responses, and parse structured data into tidy, analyzable formats."
  },
  {
    "objectID": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_empty_07072025.html#conceptual-foundation",
    "href": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_empty_07072025.html#conceptual-foundation",
    "title": "Session 2: Weather Data - OpenWeatherAPI - Empty",
    "section": "",
    "text": "Part A. Theoretical ideas of APIs\n\n\nIt is the ability for software to communicate\n\n\n\nAPI Call (PhoenixNap.com)\n\n\n\n\n\n\n\n\nLets go deeper into understanding Define:\nClient (request) –&gt; API –&gt; Server –&gt; Database\nClient &lt;– API &lt;– Server (response) &lt;– Database\n\n\n\nGATO365 API Request & Response\n\n\n\n\n\n\n\n\nLets spend some more time on the request and response\nThe client sends a request asking for info (like Taylor Swift or today’s weather). This request includes:\n\nA URL (e.g., with parameters like ?q=San+Luis+Obispo)\nPossibly an API key\nA method (e.g., GET or POST)\n\nThe request are in the form of a url string (more on this soon…)\nThe server then returns a response which contains:\n\ndata (temperature, artist name, forecast, etc.)\nmetadata (This is information about the response.)\nstatus code (Tells you whether the request was successful)\n\nThis information is traditionally provided in JSON Format. (more on this soon…)\n\n\n\n\n\n\n\nLet’s focus on what the response is 1st (what we receive from the server):\nBelow is an example GIF of the information sent from the server in JSON format:\n\n\n\n\nGATO365 Anatomy JSON\n\n\n\n\n\n\n\n\nStatus codes tell you what happened with your request:\n\n100s: Info\n200s: Success (highlight: 200 OK)\n300s: Redirect\n400s: Client error\n500s: Server error\n\nNote 5:\n\nEmphasize: In most data APIs, your goal is to get a 200 response.\nUse examples like making up a nonexistent city or artist to show how an API might respond with a 400 or 404. |\n\n** Client Request *************************\n\n\n\n\n\n\nWhat type of client requests can we make?\nCRUD Framework (Create, Read, Update, Delete)\n\nThough APIs allow all four, Read (GET) is most common in data science.\nRESTful API mapping:\n\nCreate → POST\nRead → GET\nUpdate → PUT/PATCH\nDelete → DELETE\n\n\n\n\n\n\n\n\n\n\n\n\n\nGATO365 Get Request\n\n\nHere is the description of a GET request from that perspective.\n\nClient constructs a request for a resource.\nAPI receives and validates the client's request.\nServer locates the requested data within database.\nClient receives requested data from the server.\n\n\n\n\n\n\n\n\n\n\nGATO365 Post Request\n\n\n\nHere is the description of a POST request.\n\nClient sends new data within the request body.\nAPI receives and validates the client's new data.\nServer creates a new record in the database.\nClient receives a confirmation for the new record.\n\n\n\n\n\n\n\n\n\n[[Based on time do One of the three steps]]\n[[1. email attendees to go to the weather website and get API key or whatever information needed before the conference. Create a video that’s displaying how to do this]]\n[[1a. Discuss .Renviron.txt, how we use: to create and edit API key: usethis::edit_r_environ()]]\n[[1b. Use Sys.getenv(\"API_KEY\") to see API in console]]\n[[Note that you have to use the 1a to see the api key again]] Restart R\n[[2. have attendees get the key during the break session if they have not done so already]]\n[[3. use a common key, but tell them it is bad practice]]\n[[regardless of the decision made of the three options above have attendees store information in the environment file]]\n\n\n\n\n\n\n\nSo what we’re going to first do is create our request and the most ideal way.\nA request defined by a URL, which contains both:\n\nThe endpoint (base address of the API)\nThe query string (additional key-value pairs that modify the request)\n\nWe often need to glue strings together to build this full URL dynamically.\nA request is not “automatically” turned into JSON when sent — it’s the response that’s usually formatted as JSON. The request is often URL-encoded if it’s a GET.\n\n\n\n\n\n\n\n\nWhen we use a URL like ...?q=San+Luis+Obispo&appid=..., we’re constructing a query string, which is appended to the base URL.\nThink of this as “asking the question”—the query string shapes the request.\nThe server receives the request, processes it, and responds with structured data (typically JSON).\nWe’re not sending JSON in this case—we’re sending a URL with parameters. JSON is returned to us as a response format.\n\n\n\n\nGATO365 Post Request\n\n\n\n\n\n\n\n\n\n\nLoad Libraries\n\n## EMPTY VERSION\n# library(____)\n# library(____)\n\nSpecify city to query\n\n## EMPTY VERSION\n# city_name &lt;- \"____\"\n\nUse this url as the base url, https://api.openweathermap.org/data/2.5/weather? (Imporve Instructions using empty and fill)\n\n## EMPTY VERSION\n# current_weather_url &lt;- glue(\"____\",                        ## Base URL / Endpoint\n#                             \"q=\", URLencode(____),         ## City Name\n#                             \"&appid=\", Sys.getenv(\"____\"), ## Use of API Key\n#                             \"&units=____\")                 ## Specify the units\n\n\n\n\nPrint the url string\n\n## EMPTY VERSION\n# ____\n\nMake the request formal\n\n## EMPTY VERSION\n# req &lt;- request(____)\n\nPrint the formal requst\n\n## EMPTY VERSION\n# ____\n\n\n\n\n\n\n\nStep 1: Build Request Object\n\n## EMPTY VERSION\n# req &lt;- request(\"____\") %&gt;%  \n#   req_url_query(\n#     q = ____,\n#     appid = Sys.getenv(\"____\"),\n#     units = \"____\"\n#   )\n\n\n## EMPTY VERSION\n# ____\n\n\n\n\nStep 2: Make request\n\n## EMPTY VERSION\n# response &lt;- req_perform(____)\n\n\n## EMPTY VERSION\n# ____\n\n\nGET: The method used to request data.\nURL: The full address the request was sent to.\nStatus: 200 OK: The request was successful.\nContent-Type: The returned data is in JSON format.\nBody: The weather data is downloaded and in memory.\n\n\n\n\nNot a step: View content Type\n\n## EMPTY VERSION\n# content_type &lt;- resp_content_type(____)\n\n\n## EMPTY VERSION\n# ____\n\n\n\n\nStep 3: Process the Response\n\n## EMPTY VERSION\n# library(____)\n\n\n## EMPTY VERSION\n## IF the status code is 200 we are good\n# if (resp_status(____) == ____) {\n#   \n#   # Parse JSON\n#   result &lt;- resp_body_json(____)\n#   \n#   # Print Results as JSON but in R it is a list\n#   print(____)\n#   \n#   #---------------------------------------\n#   \n#   # Convert to Data Frame directly\n#   current_weather_df &lt;- as.data.frame(____)\n#   \n#   # Print Results as Data Frame, using dplyr\n#   current_weather_df %&gt;% \n#     select(____, ____, ____, ____, ____) %&gt;% \n#     print()\n#   \n#   \n# ## ELSE state there is an Error\n# } else {\n#   cat(\"Failed. Status code:\", resp_status(____), \"\\n\")\n# }\n\n\n\n\n\n\n\n\nProvide instructions (Improve Instructions using empty and fill)\n\n## EMPTY VERSION\n## Step 1: Define function \"get_city_coords\" that accepts the parameter \"city\"\n# get_city_coords &lt;- function(____){\n#   \n# ## Step 2: Create API request URL\n#   geo_url &lt;- glue(\n#     \"____\", # Endpoint\n#     \"q=\", URLencode(____),\n#     \"&limit=1&appid=\", Sys.getenv(\"____\")\n#   )\n#   \n# ## Step 3: Use req_perform() and request() to call the API with the URL request  \n#   geo_response &lt;- req_perform(request(____))\n#   \n# ## Step 4: If the status code is 200 (OK), parse the response\n#   if (resp_status(____) == ____) {\n#     geo_data_df &lt;- resp_body_json(____) %&gt;% \n#       as.data.frame()\n#     \n#   ## Step 5: Assess if the output has 0 length, meaning no result.\n#     if (length(____) == 0) {\n#       stop(\"____\")\n#     }\n#     \n#   ## Step 6: Round latitude and longitude to 2 decimal places.\n#     mod_1_geo_data_df &lt;- geo_data_df %&gt;% \n#       mutate(lat = round(____, 2),\n#              lon = round(____, 2))\n# \n#   ## Step 7: Select and rename columns\n#     mod_2_geo_data_df &lt;- mod_1_geo_data_df %&gt;% \n#       select(____, ____, ____, ____) %&gt;% \n#       rename(city = ____)\n#     \n#   ## Step 8: Return the final data frame\n#     return(____)\n#     \n#   }\n# }\n\n\n\n\nLets try out this new function on the city\n\n## EMPTY VERSION\n# get_city_coords(____)\n\n\n## EMPTY VERSION\n# library(____)\n# \n# # List of cities you want to geocode\n# cities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n# \n# # Use map_df() to apply the function to each city\n# map_df(____, ____)\n\n\n\n\n\n\n\n\n## EMPTY VERSION\n# library(____)\n\n\n## EMPTY VERSION\n# get_past_weather_by_city &lt;- function(____, ____) {\n#   # Step 1: Get city coordinates\n#   coords_df &lt;- get_city_coords(____)\n#   lat &lt;- coords_df$lat\n#   lon &lt;- coords_df$lon\n#   \n#   cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n#   \n#   # Step 2: Create vector of past dates\n#   date_range &lt;- as.character(today() - days(1:____))\n#   \n#   # Step 3: Define function for single date\n#   fetch_day_summary &lt;- function(____) {\n#     weather_url &lt;- glue(\n#       \"____\", # Endpoint\n#       \"lat=\", ____,\n#       \"&lon=\", ____,\n#       \"&date=\", ____,\n#       \"&appid=\", Sys.getenv(\"____\"),\n#       \"&units=imperial\"\n#     )\n#     \n#     response &lt;- req_perform(request(____))\n#     \n#     if (resp_status(____) == 200) {\n#       resp_body_json(____) %&gt;% \n#         as.data.frame() %&gt;% \n#         mutate(city = ____, date = ____)\n#     } else {\n#       warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(response)))\n#       return(NULL)\n#     }\n#   }\n#   \n#   # Step 4: Map over date_range and bind into a single data frame\n#   map_dfr(____, ____)\n# }\n\n\n\n\n\n## EMPTY VERSION\n# get_past_weather_by_city(____, ____)\n\n\n## FILLED RESPONSE\nget_past_weather_by_city(city_name, 5)\n\n\n\n\nThere are a lot of ways of doing this, I decided not to use for loop in R\n\n## EMPTY VERSION\n# num_days &lt;- 5\n# \n# # Get historical weather data for each city using map_dfr\n# all_weather_df &lt;- map_dfr(\n#   ____,\n#   ~ get_past_weather_by_city(____, ____)\n# )"
  },
  {
    "objectID": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_filled_07072025.html",
    "href": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_filled_07072025.html",
    "title": "Session 2: Weather Data - OpenWeatherAPI - Filled",
    "section": "",
    "text": "By the end of this session, participants will be able to:\n\nUnderstand the foundational concepts of APIs as a bridge for data exchange, including how they function in modern software and support real-time data extraction.\nQuery public APIs effectively, forming well-structured requests that interact with remote databases and return meaningful results.\nInterpret JSON responses, with a focus on the data element, while also distinguishing between metadata and status codes. Develop an understanding of how HTTP status codes and API keys work to validate and secure data access.\nWrite clean, purposeful R code to send API requests, handle responses, and parse structured data into tidy, analyzable formats.\n\n\n\n\n\n\n\nPart A. Theoretical ideas of APIs\nNote 1:\n\nThis is not a web developer nor a CS course but with a decent understanding of the logic, you and your students will appreciate the utilization of web scraping more\n\n\n\nIt is the ability for software to communicate\n\n\n\nAPI Call (PhoenixNap.com)\n\n\n\nQ1: What is its utility of APIs? (multiple choice)\n\nNote 2:\n\nThis image is overly simplified in that a client left makes request through an api to a server/database then the server/database provides responses\nA client and server can exist on the same computer. This is often what’s happening in local development (e.g., querying a local database from R)\n\n\n\n\n\n\n\nLets go deeper into understanding Define:\nClient (request) –&gt; API –&gt; Server –&gt; Database\nClient &lt;– API &lt;– Server (response) &lt;– Database\n\n\n\nGATO365 API Request & Response\n\n\nQ2. Matching You might show this flow visually and say:\n“The [API] is the waiter.”\n“The [client] is the customer.”\n“The [server] is the kitchen.”\n“The [database] is the fridge or pantry.”\nNote 3:\n\nAction: Client makes a request\nAction: Server queries Database provides a response\n\n\n\n\n\n\n\nLets spend some more time on the request and response\nThe client sends a request asking for info (like Taylor Swift or today’s weather). This request includes:\n\nA URL (e.g., with parameters like ?q=San+Luis+Obispo)\nPossibly an API key\nA method (e.g., GET or POST)\n\nThe request are in the form of a url string (more on this soon…)\nThe server then returns a response which contains:\n\ndata (temperature, artist name, forecast, etc.)\nmetadata (This is information about the response.)\nstatus code (Tells you whether the request was successful)\n\nThis information is traditionally provided in JSON Format. (more on this soon…)\n\n\n\n\n\n\n\nLet’s focus on what the response is 1st (what we receive from the server):\nBelow is an example GIF of the information sent from the server in JSON format:\n\n\n\n\nGATO365 Anatomy JSON\n\n\nNote 4:\n\nWhen we send a request to an API, we get a response body, which includes the content — typically JSON — divided into data (what we wanted), metadata (info about the data), and a status_code telling us if the request worked.\n\n\n\n\n\n\n\nStatus codes tell you what happened with your request:\n\n100s: Info\n200s: Success (highlight: 200 OK)\n300s: Redirect\n400s: Client error\n500s: Server error\n\nNote 5:\n\nEmphasize: In most data APIs, your goal is to get a 200 response.\nUse examples like making up a nonexistent city or artist to show how an API might respond with a 400 or 404. |\n\n** Client Request *************************\n\n\n\n\n\n\nWhat type of client requests can we make?\nCRUD Framework (Create, Read, Update, Delete)\n\nThough APIs allow all four, Read (GET) is most common in data science.\nRESTful API mapping:\n\nCreate → POST\nRead → GET\nUpdate → PUT/PATCH\nDelete → DELETE\n\n\n\nNotes:\n\nGET retrieves existing data from a server.\nPOST submits new data to the server.\nUPDATE modifies existing data on the server.\nDELETE removes a resource from the server.\nSometimes we have to implement a post to be able to gain an access token\n\n\n\n\n\n\n\n\n\n\nGATO365 Get Request\n\n\nHere is the description of a GET request from that perspective.\n\nClient constructs a request for a resource.\nAPI receives and validates the client's request.\nServer locates the requested data within database.\nClient receives requested data from the server.\n\n\n\n\n\n\n\n\n\n\nGATO365 Post Request\n\n\n\nHere is the description of a POST request.\n\nClient sends new data within the request body.\nAPI receives and validates the client's new data.\nServer creates a new record in the database.\nClient receives a confirmation for the new record.\n\n\n\n\n\n\n\n\n\n[[Based on time do One of the three steps]]\n[[1. email attendees to go to the weather website and get API key or whatever information needed before the conference. Create a video that’s displaying how to do this]]\n[[1a. Discuss .Renviron.txt, how we use: to create and edit API key: usethis::edit_r_environ()]]\n[[1b. Use Sys.getenv(\"API_KEY\") to see API in console]]\n[[Note that you have to use the 1a to see the api key again]] Restart R\n[[2. have attendees get the key during the break session if they have not done so already]]\n[[3. use a common key, but tell them it is bad practice]]\n[[regardless of the decision made of the three options above have attendees store information in the environment file]]\n\nNote 8:\nThere are many ways of doing this, but I’m going to stick with using tidyverse functions.I’m going to show you two ways to actually implement the query using the one way of one of the ways of doing this within a tiny verse using string glue\n\n\n\n\n\n\n\nSo what we’re going to first do is create our request and the most ideal way.\nA request defined by a URL, which contains both:\n\nThe endpoint (base address of the API)\nThe query string (additional key-value pairs that modify the request)\n\nWe often need to glue strings together to build this full URL dynamically.\nA request is not “automatically” turned into JSON when sent — it’s the response that’s usually formatted as JSON. The request is often URL-encoded if it’s a GET.\n\nNote:\n\nAn endpoint is the specific URL where an API can be accessed. Think of it as the main address for a particular set of resources. It’s the stable part of the URL that doesn’t change from one request to the next.\nA query string is used to customize the request by filtering or specifying the exact data you want from an endpoint. It always starts with a question mark (?) and is made up of key-value pairs.\n\n\n\n\n\n\n\n\nWhen we use a URL like ...?q=San+Luis+Obispo&appid=..., we’re constructing a query string, which is appended to the base URL.\nThink of this as “asking the question”—the query string shapes the request.\nThe server receives the request, processes it, and responds with structured data (typically JSON).\nWe’re not sending JSON in this case—we’re sending a URL with parameters. JSON is returned to us as a response format.\n\n Note 9:\n\nA URL is constructed from a base URL (the endpoint) and a query string.\nThe client sends a GET request using this URL to the API.\nThe API then relays this request to the server.\nAfter processing the request, the server sends a response, which includes a status code and the requested data (often formatted as JSON), back to the client.\n\n[[Transition to openweather API Requests]]\n(TODO: Remove APIServer)\n\n\n\n\n\n\n\n\nLoad Libraries\n\n## EMPTY VERSION\n# library(____)\n# library(____)\n\n\n## FILLED RESPONSE\nlibrary(httr2)       # Makes web requests\nlibrary(glue)        # Glue Strings\n\nSpecify city to query\n\n## EMPTY VERSION\n# city_name &lt;- \"____\"\n\n\n## FILLED RESPONSE\ncity_name &lt;- \"San Luis Obispo\"\n\nUse this url as the base url, https://api.openweathermap.org/data/2.5/weather? (Imporve Instructions using empty and fill)\n\n## EMPTY VERSION\n# current_weather_url &lt;- glue(\"____\",                        ## Base URL / Endpoint\n#                             \"q=\", URLencode(____),         ## City Name\n#                             \"&appid=\", Sys.getenv(\"____\"), ## Use of API Key\n#                             \"&units=____\")                 ## Specify the units\n\n\n## FILLED RESPONSE\ncurrent_weather_url &lt;- glue(\"https://api.openweathermap.org/data/2.5/weather?\",\n                            \"q=\", URLencode(city_name),\n                            \"&appid=\", Sys.getenv(\"API_KEY\"),\n                            \"&units=imperial\")\n\nNote:\n\nEndpoint: The base URL that specifies the location of the API resource you want to access.\nq: The query parameter, used for the main search term (in this case, the city name).\nappid: The parameter for your unique API key, which is used to authenticate your request.\nunits=imperial: A parameter that requests the API to return data in Imperial units (e.g., Fahrenheit).\nURLencode(): A function that formats text, like city names with spaces, to be safely included in a URL.\nimperial: A parameter that tells the API to return data in Imperial units (e.g., Fahrenheit).\n\n\n\n\nPrint the url string\n\n## EMPTY VERSION\n# ____\n\n\n## FILLED RESPONSE\ncurrent_weather_url\n\nMake the request formal\n\n## EMPTY VERSION\n# req &lt;- request(____)\n\n\n## FILLED RESPONSE\nreq &lt;- request(current_weather_url)\n\nPrint the formal requst\n\n## EMPTY VERSION\n# ____\n\n\n## FILLED RESPONSE\nreq\n\nNote 13: Look at the request\n\nThis method shows the anatomy of the URL explicitly.\nGreat for emphasizing how query parameters are constructed using strings.\nHelps reinforce the idea of “asking a question via the URL.”\n\nNote 14:\n\nWe are going to do it again in a different way but we are going to process the response further here because\n\nI wanted you to understand the anatomy of the URL\nHave multiple ways of doing the same thing\n\n\n\n\n\n\n\n\nStep 1: Build Request Object\n\n## EMPTY VERSION\n# req &lt;- request(\"____\") %&gt;%  \n#   req_url_query(\n#     q = ____,\n#     appid = Sys.getenv(\"____\"),\n#     units = \"____\"\n#   )\n\n\n## FILLED RESPONSE\nreq &lt;- request(\"https://api.openweathermap.org/data/2.5/weather\") %&gt;% \n  req_url_query(\n    q = city_name,\n    appid = Sys.getenv(\"API_KEY\"),\n    units = \"imperial\"\n  )\n\n\n## EMPTY VERSION\n# ____\n\n\n## FILLED RESPONSE\nreq\n\nNote 15:\n\nThis method abstracts away the string building.\nIt’s cleaner and reduces chances of typos or formatting errors.\nTeaches students to treat query arguments like named inputs.\nYou can still inspect the built URL using req$url.\n\n\n\n\nStep 2: Make request\n\n## EMPTY VERSION\n# response &lt;- req_perform(____)\n\n\n## FILLED RESPONSE\nresponse &lt;- req_perform(req)\n\n\n## EMPTY VERSION\n# ____\n\n\n## FILLED RESPONSE\nresponse\n\n\nGET: The method used to request data.\nURL: The full address the request was sent to.\nStatus: 200 OK: The request was successful.\nContent-Type: The returned data is in JSON format.\nBody: The weather data is downloaded and in memory.\n\nNote 16:\n\nGET: This indicates that the request method used was GET, which is used to retrieve data from a server.\nURL: This is the full web address the request was sent to, including the endpoint and all the query parameters (like the city, your API key, and the units).\nStatus: 200 OK: This is the HTTP status code. A 200 OK status means your request was successfully received, understood, and processed.\nContent-Type: application/json: This header tells you the format of the data in the response body. In this case, the weather data was sent back in JSON format.\nBody: In memory (514 bytes): This confirms that the data returned by the API (the actual weather information) has been downloaded and is stored in your computer’s memory, ready to be used.\n\n\n\n\nNot a step: View content Type\n\n## EMPTY VERSION\n# content_type &lt;- resp_content_type(____)\n\n\n## FILLED RESPONSE\ncontent_type &lt;- resp_content_type(response)\n\n\n## EMPTY VERSION\n# ____\n\n\n## FILLED RESPONSE\ncontent_type\n\n\n\n\nStep 3: Process the Response\n\n## EMPTY VERSION\n# library(____)\n\n\n## FILLED RESPONSE\nlibrary(dplyr)\n\n[[Provide Instructions]] (Imporve Instructions using empty and fill)\n\n## EMPTY VERSION\n## IF the status code is 200 we are good\n# if (resp_status(____) == ____) {\n#   \n#   # Parse JSON\n#   result &lt;- resp_body_json(____)\n#   \n#   # Print Results as JSON but in R it is a list\n#   print(____)\n#   \n#   #---------------------------------------\n#   \n#   # Convert to Data Frame directly\n#   current_weather_df &lt;- as.data.frame(____)\n#   \n#   # Print Results as Data Frame, using dplyr\n#   current_weather_df %&gt;% \n#     select(____, ____, ____, ____, ____) %&gt;% \n#     print()\n#   \n#   \n# ## ELSE state there is an Error\n# } else {\n#   cat(\"Failed. Status code:\", resp_status(____), \"\\n\")\n# }\n\n\n## FILLED RESPONSE\n## IF the status code is 200 we are good\nif (resp_status(response) == 200) {\n  \n  # Parse JSON\n  result &lt;- resp_body_json(response)\n  \n  # Print Results as JSON but in R it is a list\n  print(result)\n  \n  #---------------------------------------\n  \n  # Convert to Data Frame directly\n  current_weather_df &lt;- as.data.frame(result)\n  \n  # Print Results as Data Frame, using dplyr\n  current_weather_df %&gt;% \n    select(name, coord.lon, coord.lat, weather.main, main.temp) %&gt;% \n    print()\n  \n  \n## ELSE state there is an Error\n} else {\n  cat(\"Failed. Status code:\", resp_status(response), \"\\n\")\n}\n\nNote 18:\n\nSelect Key Information: The API gives us much more data than we need. We use select() from the dplyr library to pull out only the specific columns we’re interested in, creating a clean, final table.\nCheck for Success First: The if (resp_status(response) == 200) statement is crucial. It’s our safety check to make sure the request was successful before we try to use the data. If the status is anything else, our code prints an error and stops.\nExtract and Parse the Data: Once we confirm success, we use resp_body_json(). This function takes the raw body of the response, parses the JSON text, and converts it into a structured R list. This list contains all the weather data returned by the API.\nConvert to a Data Frame: While the list is useful, a data frame is often easier to work with. We use as.data.frame() to transform the nested list of weather data into a standard R data frame.\n\n\n\n\n\n\n\n\nProvide instructions (Improve Instructions using empty and fill)\n\n## EMPTY VERSION\n## Step 1: Define function \"get_city_coords\" that accepts the parameter \"city\"\n# get_city_coords &lt;- function(____){\n#   \n# ## Step 2: Create API request URL\n#   geo_url &lt;- glue(\n#     \"____\", # Endpoint\n#     \"q=\", URLencode(____),\n#     \"&limit=1&appid=\", Sys.getenv(\"____\")\n#   )\n#   \n# ## Step 3: Use req_perform() and request() to call the API with the URL request  \n#   geo_response &lt;- req_perform(request(____))\n#   \n# ## Step 4: If the status code is 200 (OK), parse the response\n#   if (resp_status(____) == ____) {\n#     geo_data_df &lt;- resp_body_json(____) %&gt;% \n#       as.data.frame()\n#     \n#   ## Step 5: Assess if the output has 0 length, meaning no result.\n#     if (length(____) == 0) {\n#       stop(\"____\")\n#     }\n#     \n#   ## Step 6: Round latitude and longitude to 2 decimal places.\n#     mod_1_geo_data_df &lt;- geo_data_df %&gt;% \n#       mutate(lat = round(____, 2),\n#              lon = round(____, 2))\n# \n#   ## Step 7: Select and rename columns\n#     mod_2_geo_data_df &lt;- mod_1_geo_data_df %&gt;% \n#       select(____, ____, ____, ____) %&gt;% \n#       rename(city = ____)\n#     \n#   ## Step 8: Return the final data frame\n#     return(____)\n#     \n#   }\n# }\n\n\n## FILLED RESPONSE\n## Step 1: Define function \"get_city_coords\" that accepts the parameter \"city\"\nget_city_coords &lt;- function(city){\n  \n## Step 2: Create API request URL\n  \n  geo_url &lt;- glue(\n  \"http://api.openweathermap.org/geo/1.0/direct?\",\n  \"q=\", URLencode(city),\n  \"&limit=1&appid=\", Sys.getenv(\"API_KEY\")\n)\n ## Step 3: Use req_perform() and request() to call the API with the URL request \n\ngeo_response &lt;- req_perform(request(geo_url))\n  \n\n ## Step 4: If the status code is 200 (OK), use resp_body_json() to parse our response and as.data.frame to coerce it to data.frame.\n\nif (resp_status(geo_response) == 200) {\n  geo_data_df &lt;- resp_body_json(geo_response) %&gt;% \n    as.data.frame()\n  \n  \n  ## Step 5: Assess if the output has 0 length, meaning no result. If so, stop and display an error message.  \n  \n  if (length(geo_data_df) == 0) {\n    stop(\"City not found. Please check the city name.\")\n  }\n  \n  ## Step 6: Assign latitude and longitude to variables, and use round() to clip it down to 2 decimal places.\n  mod_1_geo_data_df &lt;- geo_data_df %&gt;% \n    mutate(lat = round(lat,2),\n           lon = round(lon,2))\n\n  ## Step 7: Select Certain Columns (chaptgpt)\n  mod_2_geo_data_df &lt;- mod_1_geo_data_df %&gt;% \n    select(country,name,lat,lon) %&gt;% \n    rename(city = name)\n  \n  ## Step 8: Return data frame with the country, city name and latitude / longitude.  \n  return(mod_2_geo_data_df)\n  \n  \n   }\n}\n\nNote 20:\nExplain the function and fill in blanks Emphasize error handling\n\n\n\nLets try out this new function on the city\n\n## EMPTY VERSION\n# get_city_coords(____)\n\n\n## FILLED RESPONSE\nget_city_coords(city_name)\n\nNote 21: Explain output\nLets Look at multiple cities using the the map_df function within the purrr package Provide instructions (Improve Instructions using empty and fill)\n\n## EMPTY VERSION\n# library(____)\n# \n# # List of cities you want to geocode\n# cities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n# \n# # Use map_df() to apply the function to each city\n# map_df(____, ____)\n\n\n## FILLED RESPONSE\nlibrary(purrr)\n\n# List of cities you want to geocode\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n\n# Use walk() from purrr to apply the function to each city\nmap_df(cities, get_city_coords)\n\nNote 22: Explain Task Explain map_df Explain output\n\n\n\n\n\n\n(TODO: Remove certain element of functions that are needed to understand the function) (highltight the function days in this)\n\n## EMPTY VERSION\n# library(____)\n\n\n## FILLED RESPONSE\nlibrary(lubridate)   # Time and date handling\n\nNote 23: Explain need for lubridate Emphasize the new endpoint (https://api.openweathermap.org/data/3.0/onecall/day_summary?) an how it is a paid subscription\n\n## EMPTY VERSION\n# get_past_weather_by_city &lt;- function(____, ____) {\n#   # Step 1: Get city coordinates\n#   coords_df &lt;- get_city_coords(____)\n#   lat &lt;- coords_df$lat\n#   lon &lt;- coords_df$lon\n#   \n#   cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n#   \n#   # Step 2: Create vector of past dates\n#   date_range &lt;- as.character(today() - days(1:____))\n#   \n#   # Step 3: Define function for single date\n#   fetch_day_summary &lt;- function(____) {\n#     weather_url &lt;- glue(\n#       \"____\", # Endpoint\n#       \"lat=\", ____,\n#       \"&lon=\", ____,\n#       \"&date=\", ____,\n#       \"&appid=\", Sys.getenv(\"____\"),\n#       \"&units=imperial\"\n#     )\n#     \n#     response &lt;- req_perform(request(____))\n#     \n#     if (resp_status(____) == 200) {\n#       resp_body_json(____) %&gt;% \n#         as.data.frame() %&gt;% \n#         mutate(city = ____, date = ____)\n#     } else {\n#       warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(response)))\n#       return(NULL)\n#     }\n#   }\n#   \n#   # Step 4: Map over date_range and bind into a single data frame\n#   map_dfr(____, ____)\n# }\n\n\n## FILLED RESPONSE\nget_past_weather_by_city &lt;- function(city, days) {\n  # Step 1: Get city coordinates\n  coords_df &lt;- get_city_coords(city)\n  lat &lt;- coords_df$lat\n  lon &lt;- coords_df$lon\n  \n  cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n  \n  # Step 2: Create vector of past dates\n  date_range &lt;- as.character(today() - days(1:days))\n  \n  # Step 3: Define function for single date\n  fetch_day_summary &lt;- function(date) {\n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", date,\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\"\n    )\n    \n    response &lt;- req_perform(request(weather_url))\n    \n    if (resp_status(response) == 200) {\n      resp_body_json(response) %&gt;% \n        as.data.frame() %&gt;% \n        mutate(city = city, date = date)\n    } else {\n      warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(response)))\n      return(NULL)\n    }\n  }\n  \n  # Step 4: Map over date_range and bind into a single data frame\n  map_dfr(date_range, fetch_day_summary)\n}\n\nNote 24: (Explanation of filling out code and what code does)\n\n## EMPTY VERSION\n# get_past_weather_by_city(____, ____)\n\n\n## FILLED RESPONSE\nget_past_weather_by_city(city_name, 5)\n\nNote 25: Explain code and output\n\n\n\nThere are alot of ways of doing this, I decided not to use for loop in R\n\n## EMPTY VERSION\n# num_days &lt;- 5\n# \n# # Get historical weather data for each city using map_dfr\n# all_weather_df &lt;- map_dfr(\n#   ____,\n#   ~ get_past_weather_by_city(____, ____)\n# )\n\n\n## FILLED RESPONSE\nnum_days &lt;- 5\n\n# Get historical weather data for each city using map_dfr\nall_weather_df &lt;- map_dfr(\n  cities,\n  ~ get_past_weather_by_city(.x, num_days)\n)\n\nNote 26: (Explanation of map_dfr and each element of input)\n\n\n\n\n[[ GENERAL INSRUCTIONS]]\n[[Two versions: 1 fillable and 1 blank]]\n\n\n[[INSRUCTIONS]]\nStep 1: Retrieves current weather conditions for a single city. Demonstrates basic GET usage and parsing a flat JSON structure.\n(TODO: Highlight Endpoint: https://api.openweathermap.org/data/2.5/weather?)\n\n## EMPTY VERSION\n# get_city_current_weather &lt;- function(____) {\n#   url &lt;- glue::glue(\n#     \"____\", # Endpoint\n#     \"q=\", URLencode(____),\n#     \"&appid=\", Sys.getenv(\"____\"),\n#     \"&units=imperial\"\n#   )\n#   \n#   response &lt;- request(____) %&gt;% req_perform()\n#   \n#   if (resp_status(____) == 200) {\n#     response %&gt;% \n#       resp_body_json() %&gt;% \n#       purrr::pluck(\"____\") %&gt;% \n#       tibble::as_tibble() %&gt;% \n#       dplyr::select(____, ____) %&gt;% \n#       dplyr::mutate(\n#         city = ____,\n#         description = resp_body_json(____) %&gt;% purrr::pluck(\"____\", 1, \"____\")\n#       ) %&gt;% \n#       dplyr::select(____, ____, ____, ____)\n#   } else {\n#     warning(\"Failed to retrieve current weather for \", ____)\n#     return(NULL)\n#   }\n# }\n\n\n## FILLED RESPONSE\nget_city_current_weather &lt;- function(city) {\n  url &lt;- glue::glue(\n    \"https://api.openweathermap.org/data/2.5/weather?\",\n    \"q=\", URLencode(city),\n    \"&appid=\", Sys.getenv(\"API_KEY\"),\n    \"&units=imperial\"\n  )\n  \n  response &lt;- request(url) %&gt;% req_perform()\n  \n  if (resp_status(response) == 200) {\n    response %&gt;% \n      resp_body_json() %&gt;% \n      purrr::pluck(\"main\") %&gt;% \n      tibble::as_tibble() %&gt;% \n      dplyr::select(temp, humidity) %&gt;% \n      dplyr::mutate(\n        city = city,\n        description = resp_body_json(response) %&gt;% purrr::pluck(\"weather\", 1, \"description\")\n      ) %&gt;% \n      dplyr::select(city, temp, humidity, description)\n  } else {\n    warning(\"Failed to retrieve current weather for \", city)\n    return(NULL)\n  }\n}\n\n\n\n\nStep 2: Run for a Single City (e.g., “Atlanta”)\n\n## EMPTY VERSION\n# get_city_current_weather(\"____\")\n\n\n## FILLED RESPONSE\nget_city_current_weather(\"Atlanta\")\n\nStep 3: Run for a Vector of Cities Using purrr::map_dfr()\n\n## EMPTY VERSION\n# cities &lt;-  c(\"San Francisco\", \"Minneapolis\", \"St. Louis\", \"Savannah\", \"Boulder\", \"Washington D.C.\", \"Kansas City\", \"Orlando\")\n# \n# weather_df &lt;- purrr::map_dfr(____, ____)\n\n\n## FILLED RESPONSE\ncities &lt;-  c(\"San Francisco\", \"Minneapolis\", \"St. Louis\", \"Savannah\", \"Boulder\", \"Washington D.C.\", \"Kansas City\", \"Orlando\")\n\nweather_df &lt;- purrr::map_dfr(cities, get_city_current_weather)\n\n\n\n\n\n\n\n\n[[INSRUCTIONS]]\nPurpose: Retrieves the next 5 days of forecast data (in 3-hour intervals). This introduces nested lists and flattening structures.\nNote: we are now getting times as well (TODO: Highlight ENdpoint: https://api.openweathermap.org/data/2.5/forecast?)\n\n## EMPTY VERSION\n# get_city_forecast_5day &lt;- function(____) {\n#   url &lt;- glue::glue(\n#     \"____\", # Endpoint\n#     \"q=\", URLencode(____),\n#     \"&appid=\", Sys.getenv(\"____\"),\n#     \"&units=imperial\"\n#   )\n#   \n#   response &lt;- httr2::req_perform(httr2::request(____))\n#   \n#   if (httr2::resp_status(____) == 200) {\n#     response %&gt;% \n#       resp_body_json() %&gt;% \n#       purrr::pluck(\"____\") %&gt;% \n#       purrr::map_dfr(\n#         ~ tibble::tibble(\n#             city = ____,\n#             timestamp = .x$____,\n#             temp = .x$____$____,\n#             weather = .x$____ %&gt;% purrr::pluck(1, \"____\")\n#         )\n#       )\n#   } else {\n#     warning(\"Failed to retrieve forecast for \", ____)\n#     return(NULL)\n#   }\n# }\n\n\n## FILLED RESPONSE\nget_city_forecast_5day &lt;- function(city) {\n  url &lt;- glue::glue(\n    \"https://api.openweathermap.org/data/2.5/forecast?\",\n    \"q=\", URLencode(city),\n    \"&appid=\", Sys.getenv(\"API_KEY\"),\n    \"&units=imperial\"\n  )\n  \n  response &lt;- httr2::req_perform(httr2::request(url))\n  \n  if (httr2::resp_status(response) == 200) {\n  response %&gt;% \n      resp_body_json() %&gt;% \n      purrr::pluck(\"list\") %&gt;% \n      purrr::map_dfr(\n        ~ tibble::tibble(\n            city = city,\n            timestamp = .x$dt_txt,\n            temp = .x$main$temp,\n            weather = .x$weather %&gt;% purrr::pluck(1, \"description\")\n        )\n      )\n  } else {\n    warning(\"Failed to retrieve forecast for \", city)\n    return(NULL)\n  }\n}\n\n\n\n\nTry for city notice the number of rows and columns\n\n## EMPTY VERSION\n# get_city_forecast_5day(\"____\")\n\n\n## FILLED RESPONSE\nget_city_forecast_5day(\"Atlanta\")\n\nLets Look at multiple cities\n\n## EMPTY VERSION\n# cities &lt;- c(\"Portland\", \"Salt Lake City\", \"Philadelphia\", \"Charleston\", \"Detroit\", \"Las Vegas\", \"San Diego\", \"Baltimore\")\n# \n# forecast_df &lt;- purrr::map_dfr(____, ____)\n\n\n## FILLED RESPONSE\ncities &lt;- c(\"Portland\", \"Salt Lake City\", \"Philadelphia\", \"Charleston\", \"Detroit\", \"Las Vegas\", \"San Diego\", \"Baltimore\")\n\nforecast_df &lt;- purrr::map_dfr(cities, get_city_forecast_5day)\n\n\n\n\n\n\n\n\n[[INSRUCTIONS]] Purpose: Uses lat and lon to query current air quality. Demonstrates chaining of API requests (e.g., using get_city_coords() first), and different JSON structures.\n(TODO: Highlight ENdpoint: http://api.openweathermap.org/data/2.5/air_pollution?)\n\n## EMPTY VERSION\n# get_air_pollution_by_coords &lt;- function(____, ____) {\n#   url &lt;- glue::glue(\n#     \"____\", # Endpoint\n#     \"lat=\", ____,\n#     \"&lon=\", ____,\n#     \"&appid=\", Sys.getenv(\"____\")\n#   )\n#   \n#   response &lt;- request(____) %&gt;% req_perform()\n#   \n#   if (resp_status(____) == 200) {\n#     response %&gt;%\n#       resp_body_json() %&gt;%\n#       purrr::pluck(\"____\", 1) %&gt;%\n#       {\\(x) tibble::tibble(\n#           aqi = x$____$____,\n#           co = x$____$____,\n#           pm2_5 = x$____$____,\n#           pm10 = x$____$____\n#       )}()\n#   } else {\n#     warning(\"Failed to retrieve air pollution data for lat = \", lat, \", lon = \", lon)\n#     return(NULL)\n#   }\n# }\n\n\n## FILLED RESPONSE\nget_air_pollution_by_coords &lt;- function(lat, lon) {\n  url &lt;- glue::glue(\n    \"http://api.openweathermap.org/data/2.5/air_pollution?\",\n    \"lat=\", lat,\n    \"&lon=\", lon,\n    \"&appid=\", Sys.getenv(\"API_KEY\")\n  )\n  \n  response &lt;- request(url) %&gt;% req_perform()\n  \n  if (resp_status(response) == 200) {\n    response %&gt;%\n      resp_body_json() %&gt;%\n      purrr::pluck(\"list\", 1) %&gt;%\n      {\\(x) tibble::tibble(\n        aqi = x$main$aqi,\n        co = x$components$co,\n        pm2_5 = x$components$pm2_5,\n        pm10 = x$components$pm10\n      )}()\n  } else {\n    warning(\"Failed to retrieve air pollution data for lat = \", lat, \", lon = \", lon)\n    return(NULL)\n  }\n}\n\n\n\n\nStep-by-Step for Usage with a Data Frame, use get_city_coords to get the cities of lon and lat.\n\n## EMPTY VERSION\n# cities &lt;-  c(\"Seattle\", \"Denver\", \"Boston\", \"Austin\", \"New Orleans\", \"Miami\", \"Phoenix\", \"Nashville\")\n# city_coords_df &lt;- map_dfr(____, ____)\n\n\n## FILLED RESPONSE\ncities &lt;-  c(\"Seattle\", \"Denver\", \"Boston\", \"Austin\", \"New Orleans\", \"Miami\", \"Phoenix\", \"Nashville\")\ncity_coords_df &lt;- map_dfr(cities, get_city_coords)\n\nGet Air Pollution Data for All Cities\n\n## EMPTY VERSION\n# library(____)\n\n\n## FILLED RESPONSE\nlibrary(tidyr)\n\n\n## EMPTY VERSION\n# pollution_df &lt;- city_coords_df %&gt;% \n#   mutate(\n#     pollution = map2(____, ____, ____)\n#   ) %&gt;% \n#   unnest(____)\n\n\n## FILLED RESPONSE\npollution_df &lt;- city_coords_df %&gt;% \n  mutate(\n    pollution = map2(lat, lon, get_air_pollution_by_coords)\n  ) %&gt;% \n  unnest(pollution)"
  },
  {
    "objectID": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_filled_07072025.html#goals-objectives",
    "href": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_filled_07072025.html#goals-objectives",
    "title": "Session 2: Weather Data - OpenWeatherAPI - Filled",
    "section": "",
    "text": "By the end of this session, participants will be able to:\n\nUnderstand the foundational concepts of APIs as a bridge for data exchange, including how they function in modern software and support real-time data extraction.\nQuery public APIs effectively, forming well-structured requests that interact with remote databases and return meaningful results.\nInterpret JSON responses, with a focus on the data element, while also distinguishing between metadata and status codes. Develop an understanding of how HTTP status codes and API keys work to validate and secure data access.\nWrite clean, purposeful R code to send API requests, handle responses, and parse structured data into tidy, analyzable formats."
  },
  {
    "objectID": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_filled_07072025.html#conceptual-foundation",
    "href": "sessions/session_2/video_session_2_notes/02_Extraction_Weather_Data_API_filled_07072025.html#conceptual-foundation",
    "title": "Session 2: Weather Data - OpenWeatherAPI - Filled",
    "section": "",
    "text": "Part A. Theoretical ideas of APIs\nNote 1:\n\nThis is not a web developer nor a CS course but with a decent understanding of the logic, you and your students will appreciate the utilization of web scraping more\n\n\n\nIt is the ability for software to communicate\n\n\n\nAPI Call (PhoenixNap.com)\n\n\n\nQ1: What is its utility of APIs? (multiple choice)\n\nNote 2:\n\nThis image is overly simplified in that a client left makes request through an api to a server/database then the server/database provides responses\nA client and server can exist on the same computer. This is often what’s happening in local development (e.g., querying a local database from R)\n\n\n\n\n\n\n\nLets go deeper into understanding Define:\nClient (request) –&gt; API –&gt; Server –&gt; Database\nClient &lt;– API &lt;– Server (response) &lt;– Database\n\n\n\nGATO365 API Request & Response\n\n\nQ2. Matching You might show this flow visually and say:\n“The [API] is the waiter.”\n“The [client] is the customer.”\n“The [server] is the kitchen.”\n“The [database] is the fridge or pantry.”\nNote 3:\n\nAction: Client makes a request\nAction: Server queries Database provides a response\n\n\n\n\n\n\n\nLets spend some more time on the request and response\nThe client sends a request asking for info (like Taylor Swift or today’s weather). This request includes:\n\nA URL (e.g., with parameters like ?q=San+Luis+Obispo)\nPossibly an API key\nA method (e.g., GET or POST)\n\nThe request are in the form of a url string (more on this soon…)\nThe server then returns a response which contains:\n\ndata (temperature, artist name, forecast, etc.)\nmetadata (This is information about the response.)\nstatus code (Tells you whether the request was successful)\n\nThis information is traditionally provided in JSON Format. (more on this soon…)\n\n\n\n\n\n\n\nLet’s focus on what the response is 1st (what we receive from the server):\nBelow is an example GIF of the information sent from the server in JSON format:\n\n\n\n\nGATO365 Anatomy JSON\n\n\nNote 4:\n\nWhen we send a request to an API, we get a response body, which includes the content — typically JSON — divided into data (what we wanted), metadata (info about the data), and a status_code telling us if the request worked.\n\n\n\n\n\n\n\nStatus codes tell you what happened with your request:\n\n100s: Info\n200s: Success (highlight: 200 OK)\n300s: Redirect\n400s: Client error\n500s: Server error\n\nNote 5:\n\nEmphasize: In most data APIs, your goal is to get a 200 response.\nUse examples like making up a nonexistent city or artist to show how an API might respond with a 400 or 404. |\n\n** Client Request *************************\n\n\n\n\n\n\nWhat type of client requests can we make?\nCRUD Framework (Create, Read, Update, Delete)\n\nThough APIs allow all four, Read (GET) is most common in data science.\nRESTful API mapping:\n\nCreate → POST\nRead → GET\nUpdate → PUT/PATCH\nDelete → DELETE\n\n\n\nNotes:\n\nGET retrieves existing data from a server.\nPOST submits new data to the server.\nUPDATE modifies existing data on the server.\nDELETE removes a resource from the server.\nSometimes we have to implement a post to be able to gain an access token\n\n\n\n\n\n\n\n\n\n\nGATO365 Get Request\n\n\nHere is the description of a GET request from that perspective.\n\nClient constructs a request for a resource.\nAPI receives and validates the client's request.\nServer locates the requested data within database.\nClient receives requested data from the server.\n\n\n\n\n\n\n\n\n\n\nGATO365 Post Request\n\n\n\nHere is the description of a POST request.\n\nClient sends new data within the request body.\nAPI receives and validates the client's new data.\nServer creates a new record in the database.\nClient receives a confirmation for the new record.\n\n\n\n\n\n\n\n\n\n[[Based on time do One of the three steps]]\n[[1. email attendees to go to the weather website and get API key or whatever information needed before the conference. Create a video that’s displaying how to do this]]\n[[1a. Discuss .Renviron.txt, how we use: to create and edit API key: usethis::edit_r_environ()]]\n[[1b. Use Sys.getenv(\"API_KEY\") to see API in console]]\n[[Note that you have to use the 1a to see the api key again]] Restart R\n[[2. have attendees get the key during the break session if they have not done so already]]\n[[3. use a common key, but tell them it is bad practice]]\n[[regardless of the decision made of the three options above have attendees store information in the environment file]]\n\nNote 8:\nThere are many ways of doing this, but I’m going to stick with using tidyverse functions.I’m going to show you two ways to actually implement the query using the one way of one of the ways of doing this within a tiny verse using string glue\n\n\n\n\n\n\n\nSo what we’re going to first do is create our request and the most ideal way.\nA request defined by a URL, which contains both:\n\nThe endpoint (base address of the API)\nThe query string (additional key-value pairs that modify the request)\n\nWe often need to glue strings together to build this full URL dynamically.\nA request is not “automatically” turned into JSON when sent — it’s the response that’s usually formatted as JSON. The request is often URL-encoded if it’s a GET.\n\nNote:\n\nAn endpoint is the specific URL where an API can be accessed. Think of it as the main address for a particular set of resources. It’s the stable part of the URL that doesn’t change from one request to the next.\nA query string is used to customize the request by filtering or specifying the exact data you want from an endpoint. It always starts with a question mark (?) and is made up of key-value pairs.\n\n\n\n\n\n\n\n\nWhen we use a URL like ...?q=San+Luis+Obispo&appid=..., we’re constructing a query string, which is appended to the base URL.\nThink of this as “asking the question”—the query string shapes the request.\nThe server receives the request, processes it, and responds with structured data (typically JSON).\nWe’re not sending JSON in this case—we’re sending a URL with parameters. JSON is returned to us as a response format.\n\n Note 9:\n\nA URL is constructed from a base URL (the endpoint) and a query string.\nThe client sends a GET request using this URL to the API.\nThe API then relays this request to the server.\nAfter processing the request, the server sends a response, which includes a status code and the requested data (often formatted as JSON), back to the client.\n\n[[Transition to openweather API Requests]]\n(TODO: Remove APIServer)\n\n\n\n\n\n\n\n\nLoad Libraries\n\n## EMPTY VERSION\n# library(____)\n# library(____)\n\n\n## FILLED RESPONSE\nlibrary(httr2)       # Makes web requests\nlibrary(glue)        # Glue Strings\n\nSpecify city to query\n\n## EMPTY VERSION\n# city_name &lt;- \"____\"\n\n\n## FILLED RESPONSE\ncity_name &lt;- \"San Luis Obispo\"\n\nUse this url as the base url, https://api.openweathermap.org/data/2.5/weather? (Imporve Instructions using empty and fill)\n\n## EMPTY VERSION\n# current_weather_url &lt;- glue(\"____\",                        ## Base URL / Endpoint\n#                             \"q=\", URLencode(____),         ## City Name\n#                             \"&appid=\", Sys.getenv(\"____\"), ## Use of API Key\n#                             \"&units=____\")                 ## Specify the units\n\n\n## FILLED RESPONSE\ncurrent_weather_url &lt;- glue(\"https://api.openweathermap.org/data/2.5/weather?\",\n                            \"q=\", URLencode(city_name),\n                            \"&appid=\", Sys.getenv(\"API_KEY\"),\n                            \"&units=imperial\")\n\nNote:\n\nEndpoint: The base URL that specifies the location of the API resource you want to access.\nq: The query parameter, used for the main search term (in this case, the city name).\nappid: The parameter for your unique API key, which is used to authenticate your request.\nunits=imperial: A parameter that requests the API to return data in Imperial units (e.g., Fahrenheit).\nURLencode(): A function that formats text, like city names with spaces, to be safely included in a URL.\nimperial: A parameter that tells the API to return data in Imperial units (e.g., Fahrenheit).\n\n\n\n\nPrint the url string\n\n## EMPTY VERSION\n# ____\n\n\n## FILLED RESPONSE\ncurrent_weather_url\n\nMake the request formal\n\n## EMPTY VERSION\n# req &lt;- request(____)\n\n\n## FILLED RESPONSE\nreq &lt;- request(current_weather_url)\n\nPrint the formal requst\n\n## EMPTY VERSION\n# ____\n\n\n## FILLED RESPONSE\nreq\n\nNote 13: Look at the request\n\nThis method shows the anatomy of the URL explicitly.\nGreat for emphasizing how query parameters are constructed using strings.\nHelps reinforce the idea of “asking a question via the URL.”\n\nNote 14:\n\nWe are going to do it again in a different way but we are going to process the response further here because\n\nI wanted you to understand the anatomy of the URL\nHave multiple ways of doing the same thing\n\n\n\n\n\n\n\n\nStep 1: Build Request Object\n\n## EMPTY VERSION\n# req &lt;- request(\"____\") %&gt;%  \n#   req_url_query(\n#     q = ____,\n#     appid = Sys.getenv(\"____\"),\n#     units = \"____\"\n#   )\n\n\n## FILLED RESPONSE\nreq &lt;- request(\"https://api.openweathermap.org/data/2.5/weather\") %&gt;% \n  req_url_query(\n    q = city_name,\n    appid = Sys.getenv(\"API_KEY\"),\n    units = \"imperial\"\n  )\n\n\n## EMPTY VERSION\n# ____\n\n\n## FILLED RESPONSE\nreq\n\nNote 15:\n\nThis method abstracts away the string building.\nIt’s cleaner and reduces chances of typos or formatting errors.\nTeaches students to treat query arguments like named inputs.\nYou can still inspect the built URL using req$url.\n\n\n\n\nStep 2: Make request\n\n## EMPTY VERSION\n# response &lt;- req_perform(____)\n\n\n## FILLED RESPONSE\nresponse &lt;- req_perform(req)\n\n\n## EMPTY VERSION\n# ____\n\n\n## FILLED RESPONSE\nresponse\n\n\nGET: The method used to request data.\nURL: The full address the request was sent to.\nStatus: 200 OK: The request was successful.\nContent-Type: The returned data is in JSON format.\nBody: The weather data is downloaded and in memory.\n\nNote 16:\n\nGET: This indicates that the request method used was GET, which is used to retrieve data from a server.\nURL: This is the full web address the request was sent to, including the endpoint and all the query parameters (like the city, your API key, and the units).\nStatus: 200 OK: This is the HTTP status code. A 200 OK status means your request was successfully received, understood, and processed.\nContent-Type: application/json: This header tells you the format of the data in the response body. In this case, the weather data was sent back in JSON format.\nBody: In memory (514 bytes): This confirms that the data returned by the API (the actual weather information) has been downloaded and is stored in your computer’s memory, ready to be used.\n\n\n\n\nNot a step: View content Type\n\n## EMPTY VERSION\n# content_type &lt;- resp_content_type(____)\n\n\n## FILLED RESPONSE\ncontent_type &lt;- resp_content_type(response)\n\n\n## EMPTY VERSION\n# ____\n\n\n## FILLED RESPONSE\ncontent_type\n\n\n\n\nStep 3: Process the Response\n\n## EMPTY VERSION\n# library(____)\n\n\n## FILLED RESPONSE\nlibrary(dplyr)\n\n[[Provide Instructions]] (Imporve Instructions using empty and fill)\n\n## EMPTY VERSION\n## IF the status code is 200 we are good\n# if (resp_status(____) == ____) {\n#   \n#   # Parse JSON\n#   result &lt;- resp_body_json(____)\n#   \n#   # Print Results as JSON but in R it is a list\n#   print(____)\n#   \n#   #---------------------------------------\n#   \n#   # Convert to Data Frame directly\n#   current_weather_df &lt;- as.data.frame(____)\n#   \n#   # Print Results as Data Frame, using dplyr\n#   current_weather_df %&gt;% \n#     select(____, ____, ____, ____, ____) %&gt;% \n#     print()\n#   \n#   \n# ## ELSE state there is an Error\n# } else {\n#   cat(\"Failed. Status code:\", resp_status(____), \"\\n\")\n# }\n\n\n## FILLED RESPONSE\n## IF the status code is 200 we are good\nif (resp_status(response) == 200) {\n  \n  # Parse JSON\n  result &lt;- resp_body_json(response)\n  \n  # Print Results as JSON but in R it is a list\n  print(result)\n  \n  #---------------------------------------\n  \n  # Convert to Data Frame directly\n  current_weather_df &lt;- as.data.frame(result)\n  \n  # Print Results as Data Frame, using dplyr\n  current_weather_df %&gt;% \n    select(name, coord.lon, coord.lat, weather.main, main.temp) %&gt;% \n    print()\n  \n  \n## ELSE state there is an Error\n} else {\n  cat(\"Failed. Status code:\", resp_status(response), \"\\n\")\n}\n\nNote 18:\n\nSelect Key Information: The API gives us much more data than we need. We use select() from the dplyr library to pull out only the specific columns we’re interested in, creating a clean, final table.\nCheck for Success First: The if (resp_status(response) == 200) statement is crucial. It’s our safety check to make sure the request was successful before we try to use the data. If the status is anything else, our code prints an error and stops.\nExtract and Parse the Data: Once we confirm success, we use resp_body_json(). This function takes the raw body of the response, parses the JSON text, and converts it into a structured R list. This list contains all the weather data returned by the API.\nConvert to a Data Frame: While the list is useful, a data frame is often easier to work with. We use as.data.frame() to transform the nested list of weather data into a standard R data frame.\n\n\n\n\n\n\n\n\nProvide instructions (Improve Instructions using empty and fill)\n\n## EMPTY VERSION\n## Step 1: Define function \"get_city_coords\" that accepts the parameter \"city\"\n# get_city_coords &lt;- function(____){\n#   \n# ## Step 2: Create API request URL\n#   geo_url &lt;- glue(\n#     \"____\", # Endpoint\n#     \"q=\", URLencode(____),\n#     \"&limit=1&appid=\", Sys.getenv(\"____\")\n#   )\n#   \n# ## Step 3: Use req_perform() and request() to call the API with the URL request  \n#   geo_response &lt;- req_perform(request(____))\n#   \n# ## Step 4: If the status code is 200 (OK), parse the response\n#   if (resp_status(____) == ____) {\n#     geo_data_df &lt;- resp_body_json(____) %&gt;% \n#       as.data.frame()\n#     \n#   ## Step 5: Assess if the output has 0 length, meaning no result.\n#     if (length(____) == 0) {\n#       stop(\"____\")\n#     }\n#     \n#   ## Step 6: Round latitude and longitude to 2 decimal places.\n#     mod_1_geo_data_df &lt;- geo_data_df %&gt;% \n#       mutate(lat = round(____, 2),\n#              lon = round(____, 2))\n# \n#   ## Step 7: Select and rename columns\n#     mod_2_geo_data_df &lt;- mod_1_geo_data_df %&gt;% \n#       select(____, ____, ____, ____) %&gt;% \n#       rename(city = ____)\n#     \n#   ## Step 8: Return the final data frame\n#     return(____)\n#     \n#   }\n# }\n\n\n## FILLED RESPONSE\n## Step 1: Define function \"get_city_coords\" that accepts the parameter \"city\"\nget_city_coords &lt;- function(city){\n  \n## Step 2: Create API request URL\n  \n  geo_url &lt;- glue(\n  \"http://api.openweathermap.org/geo/1.0/direct?\",\n  \"q=\", URLencode(city),\n  \"&limit=1&appid=\", Sys.getenv(\"API_KEY\")\n)\n ## Step 3: Use req_perform() and request() to call the API with the URL request \n\ngeo_response &lt;- req_perform(request(geo_url))\n  \n\n ## Step 4: If the status code is 200 (OK), use resp_body_json() to parse our response and as.data.frame to coerce it to data.frame.\n\nif (resp_status(geo_response) == 200) {\n  geo_data_df &lt;- resp_body_json(geo_response) %&gt;% \n    as.data.frame()\n  \n  \n  ## Step 5: Assess if the output has 0 length, meaning no result. If so, stop and display an error message.  \n  \n  if (length(geo_data_df) == 0) {\n    stop(\"City not found. Please check the city name.\")\n  }\n  \n  ## Step 6: Assign latitude and longitude to variables, and use round() to clip it down to 2 decimal places.\n  mod_1_geo_data_df &lt;- geo_data_df %&gt;% \n    mutate(lat = round(lat,2),\n           lon = round(lon,2))\n\n  ## Step 7: Select Certain Columns (chaptgpt)\n  mod_2_geo_data_df &lt;- mod_1_geo_data_df %&gt;% \n    select(country,name,lat,lon) %&gt;% \n    rename(city = name)\n  \n  ## Step 8: Return data frame with the country, city name and latitude / longitude.  \n  return(mod_2_geo_data_df)\n  \n  \n   }\n}\n\nNote 20:\nExplain the function and fill in blanks Emphasize error handling\n\n\n\nLets try out this new function on the city\n\n## EMPTY VERSION\n# get_city_coords(____)\n\n\n## FILLED RESPONSE\nget_city_coords(city_name)\n\nNote 21: Explain output\nLets Look at multiple cities using the the map_df function within the purrr package Provide instructions (Improve Instructions using empty and fill)\n\n## EMPTY VERSION\n# library(____)\n# \n# # List of cities you want to geocode\n# cities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n# \n# # Use map_df() to apply the function to each city\n# map_df(____, ____)\n\n\n## FILLED RESPONSE\nlibrary(purrr)\n\n# List of cities you want to geocode\ncities &lt;- c(\"San Luis Obispo\", \"Chicago\", \"New York\", \"Atlanta\", \"Houston\", \"Des Moines\")\n\n# Use walk() from purrr to apply the function to each city\nmap_df(cities, get_city_coords)\n\nNote 22: Explain Task Explain map_df Explain output\n\n\n\n\n\n\n(TODO: Remove certain element of functions that are needed to understand the function) (highltight the function days in this)\n\n## EMPTY VERSION\n# library(____)\n\n\n## FILLED RESPONSE\nlibrary(lubridate)   # Time and date handling\n\nNote 23: Explain need for lubridate Emphasize the new endpoint (https://api.openweathermap.org/data/3.0/onecall/day_summary?) an how it is a paid subscription\n\n## EMPTY VERSION\n# get_past_weather_by_city &lt;- function(____, ____) {\n#   # Step 1: Get city coordinates\n#   coords_df &lt;- get_city_coords(____)\n#   lat &lt;- coords_df$lat\n#   lon &lt;- coords_df$lon\n#   \n#   cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n#   \n#   # Step 2: Create vector of past dates\n#   date_range &lt;- as.character(today() - days(1:____))\n#   \n#   # Step 3: Define function for single date\n#   fetch_day_summary &lt;- function(____) {\n#     weather_url &lt;- glue(\n#       \"____\", # Endpoint\n#       \"lat=\", ____,\n#       \"&lon=\", ____,\n#       \"&date=\", ____,\n#       \"&appid=\", Sys.getenv(\"____\"),\n#       \"&units=imperial\"\n#     )\n#     \n#     response &lt;- req_perform(request(____))\n#     \n#     if (resp_status(____) == 200) {\n#       resp_body_json(____) %&gt;% \n#         as.data.frame() %&gt;% \n#         mutate(city = ____, date = ____)\n#     } else {\n#       warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(response)))\n#       return(NULL)\n#     }\n#   }\n#   \n#   # Step 4: Map over date_range and bind into a single data frame\n#   map_dfr(____, ____)\n# }\n\n\n## FILLED RESPONSE\nget_past_weather_by_city &lt;- function(city, days) {\n  # Step 1: Get city coordinates\n  coords_df &lt;- get_city_coords(city)\n  lat &lt;- coords_df$lat\n  lon &lt;- coords_df$lon\n  \n  cat(\"Coordinates for\", city, \"-&gt; Latitude:\", lat, \"Longitude:\", lon, \"\\n\")\n  \n  # Step 2: Create vector of past dates\n  date_range &lt;- as.character(today() - days(1:days))\n  \n  # Step 3: Define function for single date\n  fetch_day_summary &lt;- function(date) {\n    weather_url &lt;- glue(\n      \"https://api.openweathermap.org/data/3.0/onecall/day_summary?\",\n      \"lat=\", lat,\n      \"&lon=\", lon,\n      \"&date=\", date,\n      \"&appid=\", Sys.getenv(\"API_KEY\"),\n      \"&units=imperial\"\n    )\n    \n    response &lt;- req_perform(request(weather_url))\n    \n    if (resp_status(response) == 200) {\n      resp_body_json(response) %&gt;% \n        as.data.frame() %&gt;% \n        mutate(city = city, date = date)\n    } else {\n      warning(paste(\"Failed to get weather for\", date, \"-\", resp_status(response)))\n      return(NULL)\n    }\n  }\n  \n  # Step 4: Map over date_range and bind into a single data frame\n  map_dfr(date_range, fetch_day_summary)\n}\n\nNote 24: (Explanation of filling out code and what code does)\n\n## EMPTY VERSION\n# get_past_weather_by_city(____, ____)\n\n\n## FILLED RESPONSE\nget_past_weather_by_city(city_name, 5)\n\nNote 25: Explain code and output\n\n\n\nThere are alot of ways of doing this, I decided not to use for loop in R\n\n## EMPTY VERSION\n# num_days &lt;- 5\n# \n# # Get historical weather data for each city using map_dfr\n# all_weather_df &lt;- map_dfr(\n#   ____,\n#   ~ get_past_weather_by_city(____, ____)\n# )\n\n\n## FILLED RESPONSE\nnum_days &lt;- 5\n\n# Get historical weather data for each city using map_dfr\nall_weather_df &lt;- map_dfr(\n  cities,\n  ~ get_past_weather_by_city(.x, num_days)\n)\n\nNote 26: (Explanation of map_dfr and each element of input)\n\n\n\n\n[[ GENERAL INSRUCTIONS]]\n[[Two versions: 1 fillable and 1 blank]]\n\n\n[[INSRUCTIONS]]\nStep 1: Retrieves current weather conditions for a single city. Demonstrates basic GET usage and parsing a flat JSON structure.\n(TODO: Highlight Endpoint: https://api.openweathermap.org/data/2.5/weather?)\n\n## EMPTY VERSION\n# get_city_current_weather &lt;- function(____) {\n#   url &lt;- glue::glue(\n#     \"____\", # Endpoint\n#     \"q=\", URLencode(____),\n#     \"&appid=\", Sys.getenv(\"____\"),\n#     \"&units=imperial\"\n#   )\n#   \n#   response &lt;- request(____) %&gt;% req_perform()\n#   \n#   if (resp_status(____) == 200) {\n#     response %&gt;% \n#       resp_body_json() %&gt;% \n#       purrr::pluck(\"____\") %&gt;% \n#       tibble::as_tibble() %&gt;% \n#       dplyr::select(____, ____) %&gt;% \n#       dplyr::mutate(\n#         city = ____,\n#         description = resp_body_json(____) %&gt;% purrr::pluck(\"____\", 1, \"____\")\n#       ) %&gt;% \n#       dplyr::select(____, ____, ____, ____)\n#   } else {\n#     warning(\"Failed to retrieve current weather for \", ____)\n#     return(NULL)\n#   }\n# }\n\n\n## FILLED RESPONSE\nget_city_current_weather &lt;- function(city) {\n  url &lt;- glue::glue(\n    \"https://api.openweathermap.org/data/2.5/weather?\",\n    \"q=\", URLencode(city),\n    \"&appid=\", Sys.getenv(\"API_KEY\"),\n    \"&units=imperial\"\n  )\n  \n  response &lt;- request(url) %&gt;% req_perform()\n  \n  if (resp_status(response) == 200) {\n    response %&gt;% \n      resp_body_json() %&gt;% \n      purrr::pluck(\"main\") %&gt;% \n      tibble::as_tibble() %&gt;% \n      dplyr::select(temp, humidity) %&gt;% \n      dplyr::mutate(\n        city = city,\n        description = resp_body_json(response) %&gt;% purrr::pluck(\"weather\", 1, \"description\")\n      ) %&gt;% \n      dplyr::select(city, temp, humidity, description)\n  } else {\n    warning(\"Failed to retrieve current weather for \", city)\n    return(NULL)\n  }\n}\n\n\n\n\nStep 2: Run for a Single City (e.g., “Atlanta”)\n\n## EMPTY VERSION\n# get_city_current_weather(\"____\")\n\n\n## FILLED RESPONSE\nget_city_current_weather(\"Atlanta\")\n\nStep 3: Run for a Vector of Cities Using purrr::map_dfr()\n\n## EMPTY VERSION\n# cities &lt;-  c(\"San Francisco\", \"Minneapolis\", \"St. Louis\", \"Savannah\", \"Boulder\", \"Washington D.C.\", \"Kansas City\", \"Orlando\")\n# \n# weather_df &lt;- purrr::map_dfr(____, ____)\n\n\n## FILLED RESPONSE\ncities &lt;-  c(\"San Francisco\", \"Minneapolis\", \"St. Louis\", \"Savannah\", \"Boulder\", \"Washington D.C.\", \"Kansas City\", \"Orlando\")\n\nweather_df &lt;- purrr::map_dfr(cities, get_city_current_weather)\n\n\n\n\n\n\n\n\n[[INSRUCTIONS]]\nPurpose: Retrieves the next 5 days of forecast data (in 3-hour intervals). This introduces nested lists and flattening structures.\nNote: we are now getting times as well (TODO: Highlight ENdpoint: https://api.openweathermap.org/data/2.5/forecast?)\n\n## EMPTY VERSION\n# get_city_forecast_5day &lt;- function(____) {\n#   url &lt;- glue::glue(\n#     \"____\", # Endpoint\n#     \"q=\", URLencode(____),\n#     \"&appid=\", Sys.getenv(\"____\"),\n#     \"&units=imperial\"\n#   )\n#   \n#   response &lt;- httr2::req_perform(httr2::request(____))\n#   \n#   if (httr2::resp_status(____) == 200) {\n#     response %&gt;% \n#       resp_body_json() %&gt;% \n#       purrr::pluck(\"____\") %&gt;% \n#       purrr::map_dfr(\n#         ~ tibble::tibble(\n#             city = ____,\n#             timestamp = .x$____,\n#             temp = .x$____$____,\n#             weather = .x$____ %&gt;% purrr::pluck(1, \"____\")\n#         )\n#       )\n#   } else {\n#     warning(\"Failed to retrieve forecast for \", ____)\n#     return(NULL)\n#   }\n# }\n\n\n## FILLED RESPONSE\nget_city_forecast_5day &lt;- function(city) {\n  url &lt;- glue::glue(\n    \"https://api.openweathermap.org/data/2.5/forecast?\",\n    \"q=\", URLencode(city),\n    \"&appid=\", Sys.getenv(\"API_KEY\"),\n    \"&units=imperial\"\n  )\n  \n  response &lt;- httr2::req_perform(httr2::request(url))\n  \n  if (httr2::resp_status(response) == 200) {\n  response %&gt;% \n      resp_body_json() %&gt;% \n      purrr::pluck(\"list\") %&gt;% \n      purrr::map_dfr(\n        ~ tibble::tibble(\n            city = city,\n            timestamp = .x$dt_txt,\n            temp = .x$main$temp,\n            weather = .x$weather %&gt;% purrr::pluck(1, \"description\")\n        )\n      )\n  } else {\n    warning(\"Failed to retrieve forecast for \", city)\n    return(NULL)\n  }\n}\n\n\n\n\nTry for city notice the number of rows and columns\n\n## EMPTY VERSION\n# get_city_forecast_5day(\"____\")\n\n\n## FILLED RESPONSE\nget_city_forecast_5day(\"Atlanta\")\n\nLets Look at multiple cities\n\n## EMPTY VERSION\n# cities &lt;- c(\"Portland\", \"Salt Lake City\", \"Philadelphia\", \"Charleston\", \"Detroit\", \"Las Vegas\", \"San Diego\", \"Baltimore\")\n# \n# forecast_df &lt;- purrr::map_dfr(____, ____)\n\n\n## FILLED RESPONSE\ncities &lt;- c(\"Portland\", \"Salt Lake City\", \"Philadelphia\", \"Charleston\", \"Detroit\", \"Las Vegas\", \"San Diego\", \"Baltimore\")\n\nforecast_df &lt;- purrr::map_dfr(cities, get_city_forecast_5day)\n\n\n\n\n\n\n\n\n[[INSRUCTIONS]] Purpose: Uses lat and lon to query current air quality. Demonstrates chaining of API requests (e.g., using get_city_coords() first), and different JSON structures.\n(TODO: Highlight ENdpoint: http://api.openweathermap.org/data/2.5/air_pollution?)\n\n## EMPTY VERSION\n# get_air_pollution_by_coords &lt;- function(____, ____) {\n#   url &lt;- glue::glue(\n#     \"____\", # Endpoint\n#     \"lat=\", ____,\n#     \"&lon=\", ____,\n#     \"&appid=\", Sys.getenv(\"____\")\n#   )\n#   \n#   response &lt;- request(____) %&gt;% req_perform()\n#   \n#   if (resp_status(____) == 200) {\n#     response %&gt;%\n#       resp_body_json() %&gt;%\n#       purrr::pluck(\"____\", 1) %&gt;%\n#       {\\(x) tibble::tibble(\n#           aqi = x$____$____,\n#           co = x$____$____,\n#           pm2_5 = x$____$____,\n#           pm10 = x$____$____\n#       )}()\n#   } else {\n#     warning(\"Failed to retrieve air pollution data for lat = \", lat, \", lon = \", lon)\n#     return(NULL)\n#   }\n# }\n\n\n## FILLED RESPONSE\nget_air_pollution_by_coords &lt;- function(lat, lon) {\n  url &lt;- glue::glue(\n    \"http://api.openweathermap.org/data/2.5/air_pollution?\",\n    \"lat=\", lat,\n    \"&lon=\", lon,\n    \"&appid=\", Sys.getenv(\"API_KEY\")\n  )\n  \n  response &lt;- request(url) %&gt;% req_perform()\n  \n  if (resp_status(response) == 200) {\n    response %&gt;%\n      resp_body_json() %&gt;%\n      purrr::pluck(\"list\", 1) %&gt;%\n      {\\(x) tibble::tibble(\n        aqi = x$main$aqi,\n        co = x$components$co,\n        pm2_5 = x$components$pm2_5,\n        pm10 = x$components$pm10\n      )}()\n  } else {\n    warning(\"Failed to retrieve air pollution data for lat = \", lat, \", lon = \", lon)\n    return(NULL)\n  }\n}\n\n\n\n\nStep-by-Step for Usage with a Data Frame, use get_city_coords to get the cities of lon and lat.\n\n## EMPTY VERSION\n# cities &lt;-  c(\"Seattle\", \"Denver\", \"Boston\", \"Austin\", \"New Orleans\", \"Miami\", \"Phoenix\", \"Nashville\")\n# city_coords_df &lt;- map_dfr(____, ____)\n\n\n## FILLED RESPONSE\ncities &lt;-  c(\"Seattle\", \"Denver\", \"Boston\", \"Austin\", \"New Orleans\", \"Miami\", \"Phoenix\", \"Nashville\")\ncity_coords_df &lt;- map_dfr(cities, get_city_coords)\n\nGet Air Pollution Data for All Cities\n\n## EMPTY VERSION\n# library(____)\n\n\n## FILLED RESPONSE\nlibrary(tidyr)\n\n\n## EMPTY VERSION\n# pollution_df &lt;- city_coords_df %&gt;% \n#   mutate(\n#     pollution = map2(____, ____, ____)\n#   ) %&gt;% \n#   unnest(____)\n\n\n## FILLED RESPONSE\npollution_df &lt;- city_coords_df %&gt;% \n  mutate(\n    pollution = map2(lat, lon, get_air_pollution_by_coords)\n  ) %&gt;% \n  unnest(pollution)"
  },
  {
    "objectID": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html",
    "href": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html",
    "title": "Session 3: NFL Sports Data",
    "section": "",
    "text": "Download starter .qmd file\n(TODOs: Add introduction with Images)"
  },
  {
    "objectID": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html#session-3-html-web-scraping---nfl-data-extraction",
    "href": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html#session-3-html-web-scraping---nfl-data-extraction",
    "title": "Session 3: Extraction of NFL Data - HTML Scraping",
    "section": "",
    "text": "Identify basic HTML structure relevant for scraping\nScrape tables and text from structured web pages\nClean scraped data using tidyverse tools\nCompare different websites in terms of data accessibility\n\n\n\n\n\nHTML basics: tags, attributes, structure of web tables\nUsing rvest to read web pages and extract data\nThe importance of inspecting elements with browser tools\nStructured vs. unstructured sites: Wikipedia vs. ESPN\n\n\n\n\nlibrary(rvest)      # Web scraping\nlibrary(dplyr)      # Data manipulation\nlibrary(stringr)    # String cleaning\nlibrary(rlang)      # Advanced evaluation\nlibrary(purrr)      # Functional tools\nlibrary(ggplot2)    # Visualizations\n\nDiscussion: Why are some of these packages (like purrr or rlang) useful for scraping tasks?\n\n\n\n\nWe start by creating the target URL for a given team and year.\n\n# Step 2: Define team and year\nteam_name &lt;- \"was\"\nyear &lt;- 2023\n\n# Step 2a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n\nDiscussion: How could we make this part of a function so it is reusable?\nSee Also: https://www.pro-football-reference.com/teams/was/2023.htm#games\n\n\n\n\n\n# Step 3: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n\nWhen you run read_html(url), it returns an HTML node pointer, not human-readable content.\nThis pointer references the structure of the web page in memory, but doesn’t display actual text or data.\nIf you try to print this object directly, you’ll see something such as:\nwebpage[[1]] &lt;pointer: 0x00000225...&gt;\n\n\n\nR is showing memory addresses of HTML elements, not the content.\nThis is because the HTML content must still be parsed or extracted.\n\nUse rvest!\n\nhtml_table() : extracts data from &lt;table&gt; elements.\nhtml_text() : extracts plain text from HTML nodes.\nhtml_nodes() or html_elements() : selects multiple nodes using CSS or XPath.\nhtml_element() : selects a single node.\n\nDiscussion: Why do you think web scraping tools separate “structure” from “content”? What are the pros and cons of working with HTML nodes directly?\nFrom the webpage, grab the HTML tables using rvest::html_table().\n\n# Step 3a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\nThe result is a list containing HTML table elements.\nDiscussion: What does this data structure look like?\nSelect the desired table, the 2023 Regular Season Table, which is the second table on the webpage. Use purrr::pluck() to select the table.\n\n\n\nThis is the table we are after\n\n\n\n# Step 3b: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n\n≠≠ Discussion: Why might this index (2) break in the future? What alternatives could we use to select the correct table more reliably?\n\n\n\n\nOur first row contains more information regarding the columns than the header of the actual table. The merged cells in the header end up being repeated over the entire column group they represent, without providing useful information.\n\n# Step 4a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\nDiscussion: Why can’t we use dplyr slice()?\n\n# Step 4b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n\n# Step 4c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n\n# Step 4d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\nBecause these columns are neither labeled in the first row or the header, we must manually assign them names.\n\n\n\nNotice the unlabeled columns?\n\n\n\n# Step 4e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n\nDiscussion: What are the risks or tradeoffs in hardcoding columns like result and game_location? How could this break?\n\n\n\n\nHere we will use dplyr select and filter to drop columns that are not relevant, as well as the Bye Week where the team does not play a game.\n\n# Step 5: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n\n# Step 5a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n\n# Step 5b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n\n# Step 5c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\nDiscussion: Why convert categorical variables like score_rslt or game_location to factors? What impact could that have on modeling or plotting?\n\n\n\n\nBy putting it all together, we can input a year for the Washington Commanders and get an extracted and cleaned table out.\n\n# Step 6: Year-only function\nwas_year &lt;- function(year) {\n  # Step 1: Define team and year\nteam_name &lt;- \"was\"\n\n# Step 1a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n  \n # Step 2: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n# Step 2a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\n# Step 3: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n # Step 3a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\n# Step 3b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n# Step 3c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n# Step 3d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\n# Step 3e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n# Step 4: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n# Step 4a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n# Step 4b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n# Step 4c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\n  return(table_7)\n}\n\nTest Year Only Function\n\nhead(was_year(2022))\n\n\n\n\nNow we will do the same task but while supplying team_name as a parameter as well as year.\n\n# Step 7: Generalized function\nfn_team_year &lt;- function(team_name, year) {\n\n# Step 2a: Construct full URL\ngeneric_url &lt;- paste0(\"https://www.pro-football-reference.com/teams/\", team_name, \"/\", year, \".htm#all_games\")\n  \n # Step 3: Read HTML page\nwebpage &lt;- generic_url |&gt; rvest::read_html()\n\n# Step 3a: Extract all HTML tables\nweb_tables &lt;- webpage |&gt; rvest::html_table()\n\n# Step 3b: Pick the regular season game table (check structure visually)\nint_web_table &lt;- web_tables |&gt; purrr::pluck(2)\n # Step 4a: Use first row as column names + clean them\nfirstrow_names &lt;- int_web_table[1, ] |&gt; unlist() |&gt; as.character()\n\n# Step 4b: Assign as column names\ncolnames(int_web_table) &lt;- firstrow_names\n\n# Step 4c: Remove the first row (it's now the header)\ntable_1 &lt;- int_web_table[-1, ]\n\n# Step 4d: Clean the column names with janitor\ntable_2 &lt;- janitor::clean_names(table_1)\n\n# Step 4e: Fix problem cases with no useful data within the header or first rows\ntable_3 &lt;- table_2 |&gt; \n  rename(\n    result = x_3,\n    game_location = x_4\n)\n# Step 5: Drop irrelevant columns and rows, keep only valid games\ntable_4 &lt;- table_3 |&gt; \n  select(!(x:x_2)) |&gt; \n  filter(opp != \"Bye Week\")\n\n# Step 5a: Convert numeric-looking strings to numeric\ntable_5 &lt;- table_4 |&gt;  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\n# Step 5b: Handle factors and location labels\ntable_6 &lt;- table_5 |&gt; \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) |&gt;  as.factor()\n  )\n\n# Step 5c: Final column cleanup\ntable_7 &lt;- table_6 |&gt; \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\n  return(table_7)\n}\n\nTest Function (Team + Year)\n\nhead(fn_team_year(\"sfo\", 2024))\n\n\n\n\n\nUse ggplot2 to create simple and insightful visualizations.\n\n# Step 8: Line plot of points scored by Week\nggplot(fn_team_year(\"sfo\", 2024), aes(x = week, y = tm)) +\n  geom_line(color = \"steelblue\", linewidth = 1.2) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Points Scored Over Time\",\n    x = \"Week\",\n    y = \"Points Scored\"\n  ) +\n  theme_minimal()\n\n\n# Step 8a: Compare performance by game location\nggplot(fn_team_year(\"sfo\", 2024), aes(x = game_location, y = tm, fill = game_location)) +\n  geom_boxplot() +\n  labs(\n    title = \"Points Scored: Home vs Away\",\n    x = \"Location\",\n    y = \"Points Scored\"\n  ) +\n  theme_minimal()\n\nDiscussion: How might you visualize win/loss trends over the season? Could you include opponent information or passing yards?\nNow that you’re familiar with HTML elements and scraping, this activity will walk through extracting, cleaning, and visualizing NFL team performance data.\n\n\n\n\n\n\nScrape sports statistics from a reliable table:\n\nExample: Wikipedia table of Olympic medal counts or NBA season stats\nClean using janitor::clean_names()\nCompare scraped data from 2 sites (optional pair task)\n\n\n\n\n\n\nHow could students use scraped data in a final project?\nWhat scaffolds would help students inspect and trust their source?\n\n\n\n\n\nWhat is HTML and why it’s useful?\nExamples: Wikipedia, sports sites (NFL, Olympics)\nStructured vs unstructured web data\nReading the webpage source and locating tables/divs\nPractice: Going back and forth between R and browser to inspect structure\nTidy HTML scraping practices using rvest and janitor\nDifferent approaches:\n\nFull walkthrough\nPartial scaffold\n\nActivity:\n\nScrape 2 sources (in pairs), compare\nClean the data: name 3 needed transformations\nUse visualization and interpretation\nDiscuss hardcoding and fragile selectors"
  },
  {
    "objectID": "sessions/session_1/01_Extraction_Introduction.html",
    "href": "sessions/session_1/01_Extraction_Introduction.html",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "",
    "text": "Download starter .qmd file"
  },
  {
    "objectID": "sessions/session_1/01_Extraction_Introduction.html#logisitcs",
    "href": "sessions/session_1/01_Extraction_Introduction.html#logisitcs",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "",
    "text": "[[Say better - Begin]] - Use of CourseKata SPill (chatgpt - more info is needed) - Organization of workshop of lecture and questions throughout - Parts - Questions - There are your: - Questions - Bugs/Errors\n- The code is fillable, not for you to stress but learn the main concepts, if you are not familiar some of the tidyverse functions, I will help you understand it but I will not spend too much tide on these elements\n\n- library load as we. need\n- all packages are already install but I will display code to do so that everyone seee how its done\n\n- Putting equal '=' in function call so we all know what is needed\n[[Say better - END]]"
  },
  {
    "objectID": "sessions/session_1/01_Extraction_Introduction.html#goals-objectives-entire-workshopr",
    "href": "sessions/session_1/01_Extraction_Introduction.html#goals-objectives-entire-workshopr",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "1. Goals & Objectives (Entire workshopr)",
    "text": "1. Goals & Objectives (Entire workshopr)\nSession 1: Introduction\nUnderstand the importance of extracting dynamic data (via HTML and APIs) in modern data analysis and teaching\nSession 2: Getting Weather Data via OpenWeather API\nIn this session, we dive into OpenWeather API and learn to use packages like httr2 to execute API calls. We will also discuss URLs, queries, data structures, and more.\nSession 3: Scraping NFL Sports Data\nIn this session, we will use Pro-Football Reference to learn how to extract and clean HTML table data for use in statistical analysis and visualizations.\nSession 4: Putting it All Together (Project)\nIn this project, we will use HTML scraping joined with the OpenWeather API to create our own cloropleth map of Iowa.\n[[Change Photo to reflect something else]]"
  },
  {
    "objectID": "sessions/session_1/01_Extraction_Introduction.html#a.-goals-for-introduction",
    "href": "sessions/session_1/01_Extraction_Introduction.html#a.-goals-for-introduction",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "1.a. Goals for Introduction",
    "text": "1.a. Goals for Introduction\n\nAnalyzed static player statistics by loading an Excel file into R to filter the data and create a comparative boxplot.\nIntroduced dynamic data extraction by explaining how to use web APIs to send a request containing a query for structured JSON data from external servers.\nDemonstrated web scraping by using an R package to directly extract a data table from an HTML webpage.\nAdvocated for a modern educational approach that teaches students to actively find and extract live data rather than passively using clean, static files."
  },
  {
    "objectID": "sessions/session_1/01_Extraction_Introduction.html#conceptual-foundation",
    "href": "sessions/session_1/01_Extraction_Introduction.html#conceptual-foundation",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "2. Conceptual Foundation",
    "text": "2. Conceptual Foundation\n\nP1. Traditional Approach\n\nMy mentor, Allan, says “ask good questions”…\nStatistical Question: Who had the most impactful first season in terms of points: Michael Jordan, LeBron James, or Kobe Bryant?\n\n[Image]\n\nLet’s answer this question.\nWe’ll begin by working with a static excel file, named nba_data.xlsx, that contains per-game stats for each player’s 15 seasons in the NBA.\n\nStep 1. Fill in the code based common tidyverse packages.\n\n## EMPTY VERSION\n# library(____)      ## Data Extraction      --- E\n# library(____)       ## Data Transformation  --- T\n# library(____)     ## Data Visualization   --- V\n\n\n## FILLED VERSION\nlibrary(readxl)      ## Data Extraction      --- E\nlibrary(dplyr)       ## Data Transformation  --- T\nlibrary(ggplot2)     ## Data Visualization   --- V\n\nStep 2. Complete the appropriate function name and fill in the file name into below.\n\n## EMPTY VERSION\n# nba_df &lt;- read_*(\"____\", sheet = \"modern_nba_legends_08302019\")\n\n\n## FILLED VERSION\nnba_df &lt;- read_xlsx(\"nba_data.xlsx\", sheet = \"modern_nba_legends_08302019\")\n\n\nLet’s view the data …\n\nStep 3. Use the glimpse function to view the data\n\n## EMPTY VERSION\n\n\n## FILLED VERSION\nglimpse(nba_df)\n\n\nQ1.\nFirst, take a moment to look over the data yourself. Then, discuss with your peers if you see any issues that need cleaning.\n\n\nAns Q1.\nThe dataset is clean; the data types are properly assigned, with numeric variables stored as numbers and categorical variables stored as characters.\nNote 1: Look at the season variable\n\nNow let’s clean the focus on the data frame that we are after\n\nStep 4. Use the filter() function to select only the rows where the Season column is equal to “season_1”.\n\n## FILLED VERSION\nseason_1_df &lt;- nba_df %&gt;% \n  filter(Season == \"season_1\")\n\n\n## EMPTY VERSION\n# season_1_df &lt;- nba_df %&gt;% \n#   ___( _____ == \"season_1\")\n\n\nNow lets look at a plot of their points to answer the statistical question\n\nStep 5. Pipe the season_1_df data into ggplot, map the Name column to the x-axis and PTS to the y-axis, and then add a geom_boxplot() layer to visualize the data.\n\n## EMPTY VERSION\n# ____ %&gt;% \n#   ggplot(aes(x = ____, y = ____)) +\n#   geom_*() +\n#   theme_bw()\n\n\n## FILLED VERSION\nseason_1_df %&gt;% \n  ggplot(aes(x = Name, y = PTS)) +\n  geom_boxplot() +\n  theme_bw() \n\n\nNote 2: We could have spruced it up but here we just wanted to answer the question, if you have the urge please do so.\n\n\n\nQ2.\nWhat conclusions could be made about this plot?\n\n\nAns Q2.\nBased on the median values, Michael Jordan (MJ) had the most impressive 1st season, followed by LeBron James (LJ), and then Kobe Bryant (KB).\nThe plot also reveals that MJ’s scoring was the most variable and reached the highest peak, while KB’s point distribution was the lowest and most concentrated.\nAre there only 3 players that only played in the NBA?\n\nNow what about, Magic Johnson or Wilt Chamberlain (historic players)\n\n\n\n\nHistoric Players\n\n\n\nMaybe Luka Dončić or Ja Morant (more recent players)\n\n\n\nIf I wanted to add this data I need to go to the original source not an excel sheet to do this\n\n\n\n\nP2. Active Data Extraction\n\nA New Mindset\nShift students from passive data users to active data seekers.\nMove beyond “waiting for clean data” to learning how to access, validate, and clean it.\nTeach both the skill to extract and the capacity to teach extraction.\n\n\n\n\nNew vs Old\n\n\nNote 3: My Talking Points * (Click to P_sub 1) The core of this workshop is about a fundamental shift in how we approach data.\n\nWe want to move our students away from being passive consumers who just wait for a clean CSV file.\nThe goal is to empower them as active seekers who know how to find, get, and prepare data themselves.\nAs instructors, our responsibility is changing. We can’t just rely on pre-built packages or static datasets anymore. We need to equip our students with the skills to extract data, not just consume pre-packaged content.\n\n\n\nP3. Why This Matters Now - The Evolving Data Landscape\n\nThe digital world is not static: Websites, APIs, and file structures constantly change.\nDynamic data is everywhere: The availability of real-time data has grown exponentially, demanding new teaching strategies.\nMost courses haven’t caught up: Introductory statistics and data science courses still rely heavily on static files, limiting students’ exposure to modern data work.\n\n](https://www.statista.com/statistics/871513/worldwide-data-created/)\nNote 4: My Talking Points\n\n(Click to P_sub 2) So why is this shift so critical right now? Because the world of data has fundamentally changed.\nWebsites get redesigned, APIs get version updates, and file structures evolve. The methods we teach have to be adaptable.\nThere’s been an explosion in dynamic, frequently updated data, especially from web APIs.\nWhen I say “dynamic” data, I’m emphasizing external, real-time access. Even a flat file can be dynamic if it’s updated regularly, but the key skill is learning to get data from a source you don’t control.\nDespite this, most intro courses still use pre-cleaned, static datasets. This creates a gap between what students learn in the classroom and what they need to do in the real world. Our goal is to start closing that gap.\n\n\nQ3. (BLANK)\n\n\nAns Q3. (BLANK)\n\n\n\nP3. Static and Dynamic Sources of Data Extraction\n2.a. Static Files or Sources Extraction\n[IMAGE of this]\n\nExamples: CSV, Excel files\nTypically unchanging unless manually edited\nOften pre-loaded into classroom activities\nMay still require cleaning (e.g., column names, missing data)\nNote 4:\n\nMessy data is not always a bad thing\n2.b. Dynamic Sources Extraction\n\nDefinition: Data sources that update over time or are externally controlled (i.e., you don’t own the source)\nTwo primary types:\n\n\nApplication Programming Interface APIs – Designed to serve structured data upon request (e.g., player stats, weather)\n\n\n\n\nAPI Image\n\n\n\nHypertext Markup Language HTML/Web Pages – Seen as dynamic when content changes (especially sports, news, etc.)\n\n\n\n\nHTML Image\n\n\n\nHTML pages are primarily designed for human readability, while APIs are designed for structured machine access. Both offer pathways to dynamic data, each with different advantages and challenges.\nBridging the gap between classroom exercises and real-world data practice requires that students learn not just how to analyze data — but how to find it, extract it, and prepare it themselves.\nNote 5:\n\nHTML can be treated as static or dynamic depending on how frequently the page updates. For this workshop, we treat HTML as dynamic, especially for sports data.\n\n\n\nQ4.(BLANK)\n\n\nAns Q4.(BLANK)\n[[Transition, talk about APIs then talk about webscraping]]\n\n\n\nP4. What are Web APIs?\n\nThere are many kinds of APIs, but in this workshop, we’ll focus specifically on web APIs — tools designed to let us request and retrieve data from online sources.\nIn R, we’ll act like a piece of software making those requests, allowing us to query live data programmatically.\n\n\n\n\nFlow of Data via API (vrogue.co)\n\n\n\nAPI stand for Application Programming Interfaces\nIt is a way for software to communicate with one another\nOne way it work is that it allow programs to request a query from a data base directly from external servers in a structured format (most often JSON).\n\n\nQ5.(BLANK)\n\n\nAns Q5.(BLANK)\nJSON\n{\n  \"player\": \"LeBron James\",\n  \"points\": 27.1,\n  \"team\": \"Lakers\"\n}\n\nThe keys are players, pointsand team\nThe values for the corresponding keys are LeBron James, 27.1, Lakers\n\nNote 7:\n\nThere are a lot of acronyms\nJSON - Java Script Object Notation - javascript is web developing software (chatgpt)\nMost times, Information/data is transferred from database to user in a JSON\n(Mention Querying data base more as an action)\n\nNote 8:\n\nDescribe Image Flow of Data via API (vrogue.co)\nA user sends a request via the internet → the API talks to the server → the server queries the database → the API responds with data, often as JSON. (Mention Querying data base more as an action)\nLearning to work with web APIs teaches students more than just how to extract data — it gives them the tools to:\n\nLocate relevant APIs (e.g., weather, sports, music)\nConstruct and test their own API requests\nInterpret JSON responses (including nested structures)\nTransform the results into tidy formats ready for analysis\n\n\n(Mention Querying data base more as an action)\nP16 - More on APIs\n\nAPIs aren’t just technical tools, they’re increasingly the primary way to access and query data stored in external databases.\nIn today’s fast-changing digital environment, students must be equipped to retrieve and work with information from live, external sources, not just rely on pre-cleaned datasets.\n\nEnough talk let’s make some requests…\n\nNote 9:\nThis is what pushes students from passive observers of data to active agents in its collection, structure, and use.\nIt aligns closely with what real-world data science jobs require, especially when you’re no longer just analyzing data, but acquiring it.\nThe use of APIs requires keys, which are unique and secret codes that are used to authorize your request and identify your user and billing information.\nConsequently, keeping these codes secret is imperative.\nTo do so, store API keys in environment files which reside on your computer, and not coded into variables or available in plain text on your working files.\n\n\n\n\nP5. Using tidycensus\nP_Sub_1: Say we want to answer the following question:\nWhich 10 counties in Iowa had the highest median household income in 2022?\nP_Sub_2: Introduction to tidycensus\nThe tidycensus package is a wrapper for the U.S. Census Bureau’s APIs. It is designed to make it simple to download, manage, and map Census data within R. It handles the API requests and returns clean, tidy data frames ready for analysis.\nP_Sub_3: Step 1: Get a Census API Key\nBefore using the API, you need a key. This is a simple, one-time process.\n\nGo to the Census API Key request page: https://api.census.gov/data/key_signup.html\nFill out the short form with your organization and email address.\nYour API key will be sent to your email almost immediately. Keep it handy.\n\n\n\n\n\n\nNotes:\n\nEvery API works differently\nWe are using a library mechanism now, but we will get into the guts during the next session\n\nP_Sub_4: Step 2: Set Up Credentials\nThe tidycensus package includes a function to store your API key securely in the .Renviron file, so you only have to do this once per computer.\n\n## EMPTY VERSION\n# install.packages(\"____\") \n\n\n## FILLED VERSION\n# install.packages(\"tidycensus\") \n\n\n## EMPTY VERSION\n# The `install = TRUE` argument saves it to your .Renviron file for future use.\n# census_api_key(\"____\", install = ____)\n\n\n## FILLED VERSION\n# Replace \"YOUR_KEY_HERE\" with the key you received via email.\n# The `install = TRUE` argument saves it to your .Renviron file.\n\n# census_api_key(\"YOUR_KEY_HERE\", install = TRUE)\n\n[We are going to run this in a moment]\n⚠️ Crucially, you must restart your R session for the key to be available. Go to Session &gt; Restart R in RStudio. From now on, tidycensus will automatically find and use your key.\nP_Sub_5: Step 3: Load Required Packages\nFor this analysis, we’ll need tidycensus for data retrieval and dplyr and ggplot2 for data wrangling and visualization.\n\n## EMPTY VERSION\n# library(____)      ## Data Extraction     --- E\n# library(____)           ## Data Transformation --- T - Data Frame Manipulation\n# library(____)         ## Data Transformation --- T - String Manipulation\n# library(____)         ## Data Visualization  --- V\n\n\n## FILLED VERSION\nlibrary(tidycensus)      ## Data Extraction     --- E\nlibrary(dplyr)           ## Data Transformation --- T - Data Frame Manipulation\nlibrary(stringr)         ## Data Transformation --- T - String Manipulation\nlibrary(ggplot2)         ## Data Visualization  --- V\n\nP_Sub_6: Step 4: Find Your Variables\nThe Census Bureau offers thousands of variables. A key step is finding the specific codes for the data you need. We can use the load_variables() function to search. Let’s find the variable code for “Median Household Income” in the 2022 American Community Survey (ACS) 5-year estimates.\n\n## EMPTY VERSION\n# Load all variables from the 2022 5-year ACS dataset\n# v22 &lt;- load_variables(____, \"acs5\")\n\n# Search for the variable we want by filling in the string below\n# v22 %&gt;% \n#   filter(grepl(\"____\", label, ignore.case = TRUE))\n\n\n## FILLED VERSION\n# Load all variables from the 2022 5-year ACS dataset\nv22 &lt;- load_variables(2022, \"acs5\")\n## *********** Look at how many rows this data frame has    ************* ##\n\n\n# Search for the variable we want\nv22 %&gt;% \n  filter(grepl(\"Median Household Income\", label, ignore.case = TRUE))\n\nThe search reveals that the variable code we want is B19013_001.\nP_Sub_7: Step 5: Request Census Data\nNow we use the main function, get_acs(), to download the data. We’ll request the median household income for every county in Iowa.\n\n## EMPTY VERSION\n# Request the data for Iowa counties\n# iowa_income_df &lt;- get_acs(\n#   geography = \"____\",\n#   variables = c(med_income = \"____\"), # Provide the variable code\n#   state = \"____\",\n#   year = ____\n# )\n\n\n## FILLED VERSION\n# Request the data for Iowa counties\niowa_income_df &lt;- get_acs(\n  geography = \"county\",\n  variables = c(med_income = \"B19013_001\"), # Provide the variable code\n  state = \"IA\",\n  year = 2022\n)\n\nP_Sub_8: Step 6: Explore and Visualize the Data\nUse glimpse() to examine the data structure. You’ll see it returns a tidy data frame with columns for the estimate and the margin of error (moe).\n\n## EMPTY VERSION\n# glimpse(____)\n\n\n## FILLED VERSION\nglimpse(iowa_income_df)\n\nNow, let’s create a simple plot of the 10 counties with the highest median income.\n\n## EMPTY VERSION\n# iowa_income_df %&gt;%\n#   slice_max(order_by = ____, n = ____) %&gt;%\n#   ggplot(aes(x = ____, y = reorder(NAME, ____))) +\n#   geom_col(fill = \"dodgerblue\") +\n#   labs(\n#     title = \"____\",\n#     x = \"____\",\n#     y = \"____\"\n#   ) +\n#   theme_minimal()\n\n\n## FILLED VERSION\niowa_income_df %&gt;%\n  slice_max(order_by = estimate, n = 10) %&gt;%\n  ggplot(aes(x = estimate, y = reorder(NAME, estimate))) +\n  geom_col(fill = \"dodgerblue\") +\n  labs(\n    title = \"Top 10 Iowa Counties by Median Household Income (2022)\",\n    x = \"Median Household Income (USD)\",\n    y = \"County\"\n  ) +\n  theme_minimal()\n\n[Make Question Transition to then Web Scrapping]\n\nQ6. (BLANK)\n\n\nAns Q6. (BLANK)\n\n\n\nP6. What is Web Scraping?\n\n\nWebsites are structured using HTML (Hypertext Markup Language), which acts as the backbone for displaying and organizing content on the internet.\nWhen data is arranged in rows and columns like sports stats, schedules, or financial figures, HTML tables offer a clear and structured way to present that information directly on the page.\nTables make it easy for both humans and computers to interpret patterns, compare values, and extract key insights.\n\nNote 12:\n\nWe don’t want to waste time copying and pasting tables into a CSV, then reformatting and cleaning them again in R.\nIdeally, we want to access the data directly and bring it into R in a structured format, where we expect to do some cleaning, but we skip the unnecessary manual steps.\n\nP_Sub_1: P18 - HTML Code Visual\nBelow is an image of code for html table and the actual table that it would produce\n\n\n\nProcess of HTML Scraping (sstm2.com)\n\n\nNote 13:\n\nHighlight the following concepts:\n\nBeginning and the end of table\nThe column name\nEach Row\nHow it translate into a human readable table\n\nEmphasize that we’re only focusing on &lt;table&gt; tags for this workshop\n\n[[Transition Notes 2: Now lets see one of the libraries that allows us to scrape in R]]\n\nQ7. (BLANK)\n\n\nAns Q7. (BLANK)\n\n\n\nP7. Using htmltabs\nP_Sub_1: A New Question\nOur Census data tells us about income, but what if we want to answer a related historical question that isn’t in that dataset?\nOf the top 10 wealthiest counties, when was each one founded and what is its county seat?\nTo answer this, we need to get data from another source. A perfect place to look is Wikipedia. This is where web scraping comes in.\n\nP_Sub_2: Introduction to htmltab\n\n# First, install the 'devtools' package if you haven't already:\n# install.packages(\"devtools\")\n\n# Then, use devtools to install 'htmltab' directly from GitHub:\n# devtools::install_github(\"gato365/htmltab\")\n\nWeb scraping is the process of extracting information directly from websites. The htmltab package is a tool designed for one specific kind of scraping: pulling clean data tables out of an HTML web page.\nNote 14:\n\nYou don’t need to install this package right now, but if you do, be aware that it’s no longer available on CRAN (the official R package repository).\nIt used to be, but now it must be installed from GitHub instead. Here’s how to do that:\n\nP_Sub_3: Step 1: Load the Package\nFirst, we need to load the htmltab library.\n\nlibrary(htmltab)\n\nP_Sub_4: Step 2: Identify the Target URL\nNext, we need the URL of the page we want to scrape. It’s always a good idea to visit the URL in a web browser first to inspect the page and see what tables are there.\n\n## EMPTY VERSION\n# url &lt;- \"____\"\n\n\n## FILLED VERSION\nurl_doc &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n\n\nNote 15: Lets go to the url via Web Broswer\nP_Sub_5: Step 3: Scrape the HTML Table\nThe htmltab() function requires two main arguments: the url and which, which is the position of the table on the page (e.g., is it the 1st table, 2nd, etc.?). Often, you have to guess the number.\nLet’s try to get the main table of counties. Is it table 1 or 2?\n\n## EMPTY VERSION\n# Try guessing the table number\n# iowa_wiki_df &lt;- htmltab(doc = url, which = ____)\n\n# Let's look at the result\n# head(iowa_wiki_df)\n\n\n## FILLED VERSION\n# Let's try table 1... this doesn't look right.\niowa_wiki_df_1 &lt;- htmltab(doc = url_doc, which = 1)\nhead(iowa_wiki_df_1)\n\n# Let's try table 2... this looks perfect!\niowa_wiki_df &lt;- htmltab(doc = url_doc, which = 2)\n\n\n\n\n\n\n\nNote 15:\n\n\n- Unless you know HTML and want to look at the source code or you what exactly a table looks like you will have to guess sometimes\n\n\n- We can get the warning to go away by …\n\n\nP_Sub_6: Step 4: Explore the Scraped Data\n\n\nNow that we have the correct table, let’s explore it with glimpse() to see the column names and data types.\n\n\n::: {.cell}\n\n\n{.r .cell-code} ## EMPTY VERSION # glimpse(____) :::\n\n\n::: {.cell}\n\n\n{.r .cell-code} ## FILLED VERSION glimpse(iowa_wiki_df) :::\n\n\n\n\nNote 16:\nThis table is considered static because the county information doesn’t change frequently.\nHowever, for data that updates regularly like daily baseball statistics—it’s better to use a more robust method for extracting the data instead of relying on htmltab.\nUnlike an API, which often returns clean data types, web scraping can result in messy column names (like County seat[3]) and data stored as characters instead of numbers. This is a normal part of the process.\n\nP_Sub_7: Step 5: Join the API and Scraped Data\nTo answer our driving question, we can now join our iowa_income_df (from the Census API) with our iowa_wiki_df (from web scraping). We will need to rename the columns of the scraped data to prepare for the join.\n\n## EMPTY VERSION\n# # First, select and rename columns from the scraped data\n# iowa_details_df &lt;- iowa_wiki_df %&gt;%\n#   select(NAME = `____`, seat = `____`, established = `____`)\n# \n# # Now join it with our income data\n# final_df &lt;- left_join(____, ____, by = \"NAME\")\n# \n# # View the result!\n# final_df\n\n\n## FILLED VERSION\n# First, select and rename columns from the scraped data\niowa_details_df &lt;- iowa_wiki_df %&gt;%\n  select(NAME = `County`, seat = `County seat`, established = `Est.`)\n\n# Get just the top 10 counties from our census data\ntop_10_income_df &lt;- iowa_income_df %&gt;%\n  slice_max(order_by = estimate, n = 10) %&gt;% \n  mutate(NAME = str_remove(NAME,\", Iowa\"))\n\n# Now join it with our income data\nfinal_df &lt;- left_join(top_10_income_df, iowa_details_df, by = \"NAME\")\n\n# View the result!\nfinal_df\n\nCheck out article for more details of scraping data from the web:\nWeb Scraping in the Statistics and Data Science Curriculum: Challenges and Opportunities - Mine Dogucu & Mine Çetinkaya-Rundel\n\nP23.\n\nMuch like APIs, lots of relevant and useful information is available directly on webpages, which are readable by humans rather than APIs which are designed for machine access.\nBy learning this skill, students are able to:\n\nLocate relevant sources (e.g., sports data from Pro Football Reference)\nUnderstand how websites deliver and organize content\nTransform and clean data for analysis and visualization\n\nOften times, HTML tables contain unexpected structures or data types (images, links, etc) and can present a challenge that develops not only data cleaning skills, but intention, planning, and adaptability when handling and analyzing difficult data.\n\n\n[[Transition]]\n\nQ8. (BLANK)\n\n\nAns Q8. (BLANK)"
  },
  {
    "objectID": "sessions/session_1/01_Extraction_Introduction.html#session-1-activity-census-api-and-html-in-practice",
    "href": "sessions/session_1/01_Extraction_Introduction.html#session-1-activity-census-api-and-html-in-practice",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "3. Session 1 Activity: Census API and HTML in Practice",
    "text": "3. Session 1 Activity: Census API and HTML in Practice\nThis activity will give you a chance to apply the skills you’ve just learned. We’ll start by fetching live demographic data for Iowa, visualize it, and finally, join it with data scraped from a Wikipedia page.\nNote: This assumes you have already set up your Census API key.\n\nP8. Task 1: Get County-Level Census Data\nYour first task is to get demographic data for all counties in Iowa using tidycensus. Let’s grab two variables at once: median age (B01002_001) and total population (B01003_001). After retrieving the data, use glimpse() to inspect its structure.\n\n## EMPTY VERSION\n# iowa_df &lt;- get_acs(\n#   geography = \"____\",\n#   variables = c(\n#     median_age = \"____\", # Median Age Code\n#     total_pop = \"____\"   # Total Population Code\n#     ), \n#   state = \"____\",\n#   year = 2022\n# )\n# \n# ____(iowa_df)\n\n\n## FILLED VERSION\niowa_df &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_age = \"B01002_001\",\n    total_pop = \"B01003_001\"\n    ), \n  state = \"IA\",\n  year = 2022\n)\n\nglimpse(iowa_df)\n\n\nQ9.\nTake a look at the output from glimpse(). How is this data structured differently than a typical “wide” dataset with one row per county? What does the moe column represent?\n\n\nAns Q9.\nThe data is in a long format, with one row for each variable (median_age, total_pop) per county, rather than one row per county with different columns for each variable. This is a common, tidy format for API results.\nThe moe column stands for Margin of Error. Because the American Community Survey (ACS) is a sample-based survey and not a full count, the moe provides a measure of the estimate’s uncertainty.\n\n\n\nP9. Task 2: Wrangle and Visualize the Data\nNow, let’s turn that raw data into an insight. Your task is to create a scatter plot to see the relationship between a county’s population and its median age. You will need to:\n\nUse pivot_wider() to transform the data from a long to a wide format, creating separate columns for median_age and total_pop.\nPipe this into ggplot to create a scatter plot.\n\n\n\n## EMPTY VERSION\n# iowa_df %&gt;% \n#   select(____, NAME, variable, estimate) %&gt;% # Select only needed columns\n#   pivot_wider(names_from = ____, values_from = ____) %&gt;% \n#   ggplot(aes(x = ____, y = ____)) +\n#   geom_point() + \n#   theme_bw()\n\n\n## FILLED VERSION\niowa_df %&gt;% \n  select(GEOID, NAME, variable, estimate) %&gt;% # Select only needed columns\n  pivot_wider(names_from = variable, values_from = estimate) %&gt;% \n  ggplot(aes(x = total_pop, y = median_age)) +\n  geom_point(alpha = 0.7) +\n  scale_x_log10() + # Use a log scale for population\n  labs(\n    title = \"Population vs. Median Age in Iowa Counties\",\n    x = \"Total Population (Log Scale)\",\n    y = \"Median Age\"\n  ) +\n  theme_bw()\n\n\nQ10.\nBased on your plot, what relationship, if any, do you observe between a county’s population and its median age in Iowa?\n\n\nAns Q10.\nThe plot generally shows a negative correlation. Counties with smaller populations (further to the left) tend to have a higher median age. Conversely, the counties with the largest populations, like Polk County (Des Moines), often have a lower median age. This might suggest that younger people are more concentrated in urban centers.\n\n\n\nP10. Task 3: Join with Scraped HTML Data\nAPIs give us great data, but sometimes we need to supplement it. Let’s grab the county seat for each Iowa county from Wikipedia and join it with our Census data.\nThe URL is: \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\nYour task is to scrape the main table, select and rename the relevant columns, and then perform a left_join to add the county seat to your tidycensus data.\n\n## EMPTY VERSION\n# library(htmltab)\n# \n# # 1. Scrape the data\n# url &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n# scraped_df &lt;- htmltab(doc = url, which = ____)\n# \n# # 2. Clean and select scraped data\n# seats_df &lt;- scraped_df %&gt;% \n#   select(NAME = `____`, seat = `____`)\n# \n# # 3. Widen the census data (from previous step)\n# iowa_wide_df &lt;- iowa_df %&gt;% \n#   select(GEOID, NAME, variable, estimate) %&gt;%\n#   pivot_wider(names_from = variable, values_from = estimate)\n# \n# # 4. Join the two datasets\n# combined_df &lt;- left_join(____, ____, by = \"NAME\")\n# \n# head(combined_df)\n\n\n## FILLED VERSION\n\n\n# 1. Scrape the data\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\nscraped_df &lt;- htmltab(doc = url, which = 2)\n\n# 2. Clean and select scraped data\nseats_df &lt;- scraped_df %&gt;% \n  select(NAME = `County`, seat = `County seat[3]`)\n\n# 3. Widen the census data (from previous step)\niowa_wide_df &lt;- iowa_df %&gt;% \n  select(GEOID, NAME, variable, estimate) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate)\n\n# 4. Join the two datasets\ncombined_df &lt;- left_join(iowa_wide_df, seats_df, by = \"NAME\")\n\nhead(combined_df)\n\n\nQ12.\nWhat was the biggest challenge in joining the data from tidycensus and the scraped Wikipedia table?\n\n\nAns Q12.\nThe biggest challenge is ensuring the join key is clean and matches between the two datasets. tidycensus provides clean county names (e.g., “Polk County, Iowa”), while Wikipedia’s table might just say “Polk”. In this case, htmltab was smart enough to just grab “Polk County”, but we had to rename the columns (County to NAME) to make the join work. In other cases, you might need to use functions from stringr to clean and standardize the names before you can successfully join the tables."
  },
  {
    "objectID": "sessions/session_1/01_Extraction_Introduction.html#reflection-make-questions-within-coursekata-to-solidyfy-approach",
    "href": "sessions/session_1/01_Extraction_Introduction.html#reflection-make-questions-within-coursekata-to-solidyfy-approach",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "4. Reflection (make questions within CourseKata to solidyfy approach)",
    "text": "4. Reflection (make questions within CourseKata to solidyfy approach)\n\nWhat did we learn?\nHow does this connect to the original Goals & Objectives of the session?\nHow do you see yourself using this in your classroom?\nWhat kinds of APIs or HTML sources would be most relevant for your students?"
  },
  {
    "objectID": "sessions/session_1/01_Extraction_Introduction.html#misc.-questionsideas",
    "href": "sessions/session_1/01_Extraction_Introduction.html#misc.-questionsideas",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "5. Misc. Questions/Ideas",
    "text": "5. Misc. Questions/Ideas\n\nSet expectations and workshop goals\nWhy data extraction matters: relevance to real-world education\nOverview of the layout / table of contents\nDiscuss libraries used (tidyverse, rvest, httr, etc.)\nBest practices (e.g., avoiding hardcoding, consistent comments)\nAdapting to changing APIs/websites\nAnecdote: Spotify example of lost API access\nExplain tidy data: snake_case column names, correct data types\nEmphasize code flexibility — developers can change APIs overnight\nActivity: Scaffolding + Code review using example(s)"
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_filled_07032025.html",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_filled_07032025.html",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "",
    "text": "[[Say better - Begin]] - Use of CourseKata SPill (chatgpt - more info is needed) - Organization of workshop of lecture and questions throughout - Parts - Questions - There are your: - Questions - Bugs/Errors\n- The code is fillable, not for you to stress but learn the main concepts, if you are not familiar some of the tidyverse functions, I will help you understand it but I will not spend too much tide on these elements\n\n- library load as we. need\n- all packages are already install but I will display code to do so that everyone seee how its done\n\n- Putting equal '=' in function call so we all know what is needed\n[[Say better - END]]\n\n\n\nSession 1: Introduction\nUnderstand the importance of extracting dynamic data (via HTML and APIs) in modern data analysis and teaching\nSession 2: Getting Weather Data via OpenWeather API\nIn this session, we dive into OpenWeather API and learn to use packages like httr2 to execute API calls. We will also discuss URLs, queries, data structures, and more.\nSession 3: Scraping NFL Sports Data\nIn this session, we will use Pro-Football Reference to learn how to extract and clean HTML table data for use in statistical analysis and visualizations.\nSession 4: Putting it All Together (Project)\nIn this project, we will use HTML scraping joined with the OpenWeather API to create our own cloropleth map of Iowa.\n[[Change Photo to reflect something else]]"
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_filled_07032025.html#logisitcs",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_filled_07032025.html#logisitcs",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "",
    "text": "[[Say better - Begin]] - Use of CourseKata SPill (chatgpt - more info is needed) - Organization of workshop of lecture and questions throughout - Parts - Questions - There are your: - Questions - Bugs/Errors\n- The code is fillable, not for you to stress but learn the main concepts, if you are not familiar some of the tidyverse functions, I will help you understand it but I will not spend too much tide on these elements\n\n- library load as we. need\n- all packages are already install but I will display code to do so that everyone seee how its done\n\n- Putting equal '=' in function call so we all know what is needed\n[[Say better - END]]"
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_filled_07032025.html#goals-objectives-entire-workshopr",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_filled_07032025.html#goals-objectives-entire-workshopr",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "",
    "text": "Session 1: Introduction\nUnderstand the importance of extracting dynamic data (via HTML and APIs) in modern data analysis and teaching\nSession 2: Getting Weather Data via OpenWeather API\nIn this session, we dive into OpenWeather API and learn to use packages like httr2 to execute API calls. We will also discuss URLs, queries, data structures, and more.\nSession 3: Scraping NFL Sports Data\nIn this session, we will use Pro-Football Reference to learn how to extract and clean HTML table data for use in statistical analysis and visualizations.\nSession 4: Putting it All Together (Project)\nIn this project, we will use HTML scraping joined with the OpenWeather API to create our own cloropleth map of Iowa.\n[[Change Photo to reflect something else]]"
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_filled_07032025.html#a.-goals-for-introduction",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_filled_07032025.html#a.-goals-for-introduction",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "1.a. Goals for Introduction",
    "text": "1.a. Goals for Introduction\n\nAnalyzed static player statistics by loading an Excel file into R to filter the data and create a comparative boxplot.\nIntroduced dynamic data extraction by explaining how to use web APIs to send a request containing a query for structured JSON data from external servers.\nDemonstrated web scraping by using an R package to directly extract a data table from an HTML webpage.\nAdvocated for a modern educational approach that teaches students to actively find and extract live data rather than passively using clean, static files."
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_filled_07032025.html#conceptual-foundation",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_filled_07032025.html#conceptual-foundation",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "2. Conceptual Foundation",
    "text": "2. Conceptual Foundation\n\nP1. Traditional Approach\n\nMy mentor, Allan, says “ask good questions”…\nStatistical Question: Who had the most impactful first season in terms of points: Michael Jordan, LeBron James, or Kobe Bryant?\n\n[Image]\n\nLet’s answer this question.\nWe’ll begin by working with a static excel file, named nba_data.xlsx, that contains per-game stats for each player’s 15 seasons in the NBA.\n\n\nStep 1. Fill in the code based common tidyverse packages.\n\n## EMPTY VERSION\n# library(____)      ## Data Extraction      --- E\n# library(____)       ## Data Transformation  --- T\n# library(____)     ## Data Visualization   --- V\n\n\n## FILLED VERSION\nlibrary(readxl)      ## Data Extraction      --- E\nlibrary(dplyr)       ## Data Transformation  --- T\nlibrary(ggplot2)     ## Data Visualization   --- V\n\nStep 2. Complete the appropriate function name and fill in the file name into below.\n\n## EMPTY VERSION\n# nba_df &lt;- read_*(\"____\", sheet = \"modern_nba_legends_08302019\")\n\n\n## FILLED VERSION\nnba_df &lt;- read_xlsx(\"nba_data.xlsx\", sheet = \"modern_nba_legends_08302019\")\n\n\nLet’s view the data …\n\nStep 3. Use the glimpse function to view the data\n\n## EMPTY VERSION\n\n\n## FILLED VERSION\nglimpse(nba_df)\n\n\nQ1.\nFirst, take a moment to look over the data yourself. Then, discuss with your peers if you see any issues that need cleaning.\n\n\nAns Q1.\nThe dataset is clean; the data types are properly assigned, with numeric variables stored as numbers and categorical variables stored as characters.\nNote 1: Look at the season variable\n\n\nNow let’s clean the focus on the data frame that we are after\n\nStep 4. Use the filter() function to select only the rows where the Season column is equal to “season_1”.\n\n## FILLED VERSION\nseason_1_df &lt;- nba_df %&gt;% \n  filter(Season == \"season_1\")\n\n\n## EMPTY VERSION\n# season_1_df &lt;- nba_df %&gt;% \n#   ___( _____ == \"season_1\")\n\n\nNow lets look at a plot of their points to answer the statistical question\n\nStep 5. Pipe the season_1_df data into ggplot, map the Name column to the x-axis and PTS to the y-axis, and then add a geom_boxplot() layer to visualize the data.\n\n## EMPTY VERSION\n# ____ %&gt;% \n#   ggplot(aes(x = ____, y = ____)) +\n#   geom_*() +\n#   theme_bw()\n\n\n## FILLED VERSION\nseason_1_df %&gt;% \n  ggplot(aes(x = Name, y = PTS)) +\n  geom_boxplot() +\n  theme_bw() \n\n\nNote 2: We could have spruced it up but here we just wanted to answer the question, if you have the urge please do so.\n\n\n\nQ2.\nWhat conclusions could be made about this plot?\n\n\nAns Q2.\nBased on the median values, Michael Jordan (MJ) had the most impressive 1st season, followed by LeBron James (LJ), and then Kobe Bryant (KB).\nThe plot also reveals that MJ’s scoring was the most variable and reached the highest peak, while KB’s point distribution was the lowest and most concentrated.\n\nAre there only 3 players that only played in the NBA?\n\nNow what about, Magic Johnson or Wilt Chamberlain (historic players)\n\n\n\n\nHistoric Players\n\n\n\nMaybe Luka Dončić or Ja Morant (more recent players)\n\n\n\nIf I wanted to add this data I need to go to the original source not an excel sheet to do this"
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_filled_07032025.html#session-1-activity-census-api-and-html-in-practice",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_filled_07032025.html#session-1-activity-census-api-and-html-in-practice",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "3. Session 1 Activity: Census API and HTML in Practice",
    "text": "3. Session 1 Activity: Census API and HTML in Practice\nThis activity will give you a chance to apply the skills you’ve just learned. We’ll start by fetching live demographic data for Iowa, visualize it, and finally, join it with data scraped from a Wikipedia page.\nNote: This assumes you have already set up your Census API key.\n\nP8. Task 1: Get County-Level Census Data\nYour first task is to get demographic data for all counties in Iowa using tidycensus. Let’s grab two variables at once: median age (B01002_001) and total population (B01003_001). After retrieving the data, use glimpse() to inspect its structure.\n\n## EMPTY VERSION\n# iowa_df &lt;- get_acs(\n#   geography = \"____\",\n#   variables = c(\n#     median_age = \"____\", # Median Age Code\n#     total_pop = \"____\"   # Total Population Code\n#     ), \n#   state = \"____\",\n#   year = 2022\n# )\n# \n# ____(iowa_df)\n\n\n## FILLED VERSION\niowa_df &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_age = \"B01002_001\",\n    total_pop = \"B01003_001\"\n    ), \n  state = \"IA\",\n  year = 2022\n)\n\nglimpse(iowa_df)\n\n\nQ9.\nTake a look at the output from glimpse(). How is this data structured differently than a typical “wide” dataset with one row per county? What does the moe column represent?\n\n\nAns Q9.\nThe data is in a long format, with one row for each variable (median_age, total_pop) per county, rather than one row per county with different columns for each variable. This is a common, tidy format for API results.\nThe moe column stands for Margin of Error. Because the American Community Survey (ACS) is a sample-based survey and not a full count, the moe provides a measure of the estimate’s uncertainty.\n\n\n\n\nP9. Task 2: Wrangle and Visualize the Data\nNow, let’s turn that raw data into an insight. Your task is to create a scatter plot to see the relationship between a county’s population and its median age. You will need to:\n\nUse pivot_wider() to transform the data from a long to a wide format, creating separate columns for median_age and total_pop.\nPipe this into ggplot to create a scatter plot.\n\n\n\n## EMPTY VERSION\n# iowa_df %&gt;% \n#   select(____, NAME, variable, estimate) %&gt;% # Select only needed columns\n#   pivot_wider(names_from = ____, values_from = ____) %&gt;% \n#   ggplot(aes(x = ____, y = ____)) +\n#   geom_point() + \n#   theme_bw()\n\n\n## FILLED VERSION\niowa_df %&gt;% \n  select(GEOID, NAME, variable, estimate) %&gt;% # Select only needed columns\n  pivot_wider(names_from = variable, values_from = estimate) %&gt;% \n  ggplot(aes(x = total_pop, y = median_age)) +\n  geom_point(alpha = 0.7) +\n  scale_x_log10() + # Use a log scale for population\n  labs(\n    title = \"Population vs. Median Age in Iowa Counties\",\n    x = \"Total Population (Log Scale)\",\n    y = \"Median Age\"\n  ) +\n  theme_bw()\n\n\nQ10.\nBased on your plot, what relationship, if any, do you observe between a county’s population and its median age in Iowa?\n\n\nAns Q10.\nThe plot generally shows a negative correlation. Counties with smaller populations (further to the left) tend to have a higher median age. Conversely, the counties with the largest populations, like Polk County (Des Moines), often have a lower median age. This might suggest that younger people are more concentrated in urban centers.\n\n\n\n\nP10. Task 3: Join with Scraped HTML Data\nAPIs give us great data, but sometimes we need to supplement it. Let’s grab the county seat for each Iowa county from Wikipedia and join it with our Census data.\nThe URL is: \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\nYour task is to scrape the main table, select and rename the relevant columns, and then perform a left_join to add the county seat to your tidycensus data.\n\n## EMPTY VERSION\n# library(htmltab)\n# \n# # 1. Scrape the data\n# url &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\n# scraped_df &lt;- htmltab(doc = url, which = ____)\n# \n# # 2. Clean and select scraped data\n# seats_df &lt;- scraped_df %&gt;% \n#   select(NAME = `____`, seat = `____`)\n# \n# # 3. Widen the census data (from previous step)\n# iowa_wide_df &lt;- iowa_df %&gt;% \n#   select(GEOID, NAME, variable, estimate) %&gt;%\n#   pivot_wider(names_from = variable, values_from = estimate)\n# \n# # 4. Join the two datasets\n# combined_df &lt;- left_join(____, ____, by = \"NAME\")\n# \n# head(combined_df)\n\n\n## FILLED VERSION\n\n\n# 1. Scrape the data\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_counties_in_Iowa\"\nscraped_df &lt;- htmltab(doc = url, which = 2)\n\n# 2. Clean and select scraped data\nseats_df &lt;- scraped_df %&gt;% \n  select(NAME = `County`, seat = `County seat[3]`)\n\n# 3. Widen the census data (from previous step)\niowa_wide_df &lt;- iowa_df %&gt;% \n  select(GEOID, NAME, variable, estimate) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate)\n\n# 4. Join the two datasets\ncombined_df &lt;- left_join(iowa_wide_df, seats_df, by = \"NAME\")\n\nhead(combined_df)\n\n\nQ12.\nWhat was the biggest challenge in joining the data from tidycensus and the scraped Wikipedia table?\n\n\nAns Q12.\nThe biggest challenge is ensuring the join key is clean and matches between the two datasets. tidycensus provides clean county names (e.g., “Polk County, Iowa”), while Wikipedia’s table might just say “Polk”. In this case, htmltab was smart enough to just grab “Polk County”, but we had to rename the columns (County to NAME) to make the join work. In other cases, you might need to use functions from stringr to clean and standardize the names before you can successfully join the tables."
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_filled_07032025.html#reflection-make-questions-within-coursekata-to-solidyfy-approach",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_filled_07032025.html#reflection-make-questions-within-coursekata-to-solidyfy-approach",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "4. Reflection (make questions within CourseKata to solidyfy approach)",
    "text": "4. Reflection (make questions within CourseKata to solidyfy approach)\n\nWhat did we learn?\nHow does this connect to the original Goals & Objectives of the session?\nHow do you see yourself using this in your classroom?\nWhat kinds of APIs or HTML sources would be most relevant for your students?"
  },
  {
    "objectID": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_filled_07032025.html#misc.-questionsideas",
    "href": "sessions/session_1/video_session_1_notes/01_Extraction_Introduction_filled_07032025.html#misc.-questionsideas",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "5. Misc. Questions/Ideas",
    "text": "5. Misc. Questions/Ideas\n\nSet expectations and workshop goals\nWhy data extraction matters: relevance to real-world education\nOverview of the layout / table of contents\nDiscuss libraries used (tidyverse, rvest, httr, etc.)\nBest practices (e.g., avoiding hardcoding, consistent comments)\nAdapting to changing APIs/websites\nAnecdote: Spotify example of lost API access\nExplain tidy data: snake_case column names, correct data types\nEmphasize code flexibility — developers can change APIs overnight\nActivity: Scaffolding + Code review using example(s)"
  },
  {
    "objectID": "sessions/session_1/census_api_instructions.html",
    "href": "sessions/session_1/census_api_instructions.html",
    "title": "Census API",
    "section": "",
    "text": "Introduction to tidycensus\nThe tidycensus package is a wrapper for the U.S. Census Bureau’s APIs. It is designed to make it simple to download, manage, and map Census data within R. It handles the API requests and returns clean, tidy data frames ready for analysis.\n\nStep 1: Get a Census API Key\nBefore using the API, you need a key. This is a simple, one-time process.\n\nGo to the Census API Key request page: https://api.census.gov/data/key_signup.html\nFill out the short form with your organization and email address.\nYour API key will be sent to your email almost immediately. Keep it handy.\n\n\nStep 2: Set Up Credentials\nThe tidycensus package includes a function to store your API key securely in the .Renviron file, so you only have to do this once per computer.\n\n## FILLED VERSION\n# install.packages(\"tidycensus\") # Run this once if needed\nlibrary(tidycensus)\n\n\n## EMPTY VERSION\n# install.packages(\"tidycensus\") # Run this once if needed\n# library(____)\n\n\n## FILLED VERSION\n# Replace \"YOUR_KEY_HERE\" with the key you received via email.\n# The `install = TRUE` argument saves it to your .Renviron file.\n\n# census_api_key(\"YOUR_KEY_HERE\", install = TRUE)\n\n\n## EMPTY VERSION\n# The `install = TRUE` argument saves it to your .Renviron file for future use.\n# census_api_key(\"____\", install = ____)\n\n[We are going to run this in a moment]\n⚠️ Crucially, you must restart your R session for the key to be available. Go to Session &gt; Restart R in RStudio. From now on, tidycensus will automatically find and use your key.\n\nStep 3: Load Required Packages\nFor this analysis, we’ll need tidycensus for data retrieval and dplyr and ggplot2 for data wrangling and visualization.\n\n## FILLED VERSION\nlibrary(tidycensus)      ## Data Extraction     --- E\nlibrary(dplyr)           ## Data Transformation --- T\nlibrary(ggplot2)         ## Data Visualization  --- V\n\n\n## EMPTY VERSION\n# library(____)      ## Data Extraction     --- E\n# library(____)           ## Data Transformation --- T\n# library(____)         ## Data Visualization  --- V\n\n\nStep 4: Find Your Variables\nThe Census Bureau offers thousands of variables. A key step is finding the specific codes for the data you need. We can use the load_variables() function to search. Let’s find the variable code for “median household income” in the 2022 American Community Survey (ACS) 5-year estimates.\n\n## FILLED VERSION\n# Load all variables from the 2022 5-year ACS dataset\nv22 &lt;- load_variables(2022, \"acs5\")\n## *********** Look at how many rows this data frame has    ************* ##\n\n\n# Search for the variable we want\nv22 %&gt;% \n  filter(grepl(\"Median Household Income\", label, ignore.case = TRUE))\n\n# A tibble: 25 × 4\n   name        label                                           concept geography\n   &lt;chr&gt;       &lt;chr&gt;                                           &lt;chr&gt;   &lt;chr&gt;    \n 1 B19013A_001 Estimate!!Median household income in the past … Median… tract    \n 2 B19013B_001 Estimate!!Median household income in the past … Median… tract    \n 3 B19013C_001 Estimate!!Median household income in the past … Median… tract    \n 4 B19013D_001 Estimate!!Median household income in the past … Median… tract    \n 5 B19013E_001 Estimate!!Median household income in the past … Median… county   \n 6 B19013F_001 Estimate!!Median household income in the past … Median… tract    \n 7 B19013G_001 Estimate!!Median household income in the past … Median… tract    \n 8 B19013H_001 Estimate!!Median household income in the past … Median… tract    \n 9 B19013I_001 Estimate!!Median household income in the past … Median… tract    \n10 B19013_001  Estimate!!Median household income in the past … Median… block gr…\n# ℹ 15 more rows\n\n\n\n## EMPTY VERSION\n# Load all variables from the 2022 5-year ACS dataset\n# v22 &lt;- load_variables(____, \"acs5\")\n\n# Search for the variable we want by filling in the string below\n# v22 %&gt;% \n#   filter(grepl(\"____\", label, ignore.case = TRUE))\n\nThe search reveals that the variable code we want is B19013_001.\n\nStep 5: Request Census Data\nNow we use the main function, get_acs(), to download the data. We’ll request the median household income for every county in Iowa.\n\n## FILLED VERSION\n# Request the data for Iowa counties\niowa_income_df &lt;- get_acs(\n  geography = \"county\",\n  variables = c(med_income = \"B19013_001\"), # Provide the variable code\n  state = \"IA\",\n  year = 2022\n)\n\nGetting data from the 2018-2022 5-year ACS\n\n\nWarning: • You have not set a Census API key. Users without a key are limited to 500\nqueries per day and may experience performance limitations.\nℹ For best results, get a Census API key at\nhttp://api.census.gov/data/key_signup.html and then supply the key to the\n`census_api_key()` function to use it throughout your tidycensus session.\nThis warning is displayed once per session.\n\n\n\n## EMPTY VERSION\n# Request the data for Iowa counties\n# iowa_income_df &lt;- get_acs(\n#   geography = \"____\",\n#   variables = c(med_income = \"____\"), # Provide the variable code\n#   state = \"____\",\n#   year = ____\n# )\n\n\nStep 6: Explore and Visualize the Dataset\nUse glimpse() to examine the data structure. You’ll see it returns a tidy data frame with columns for the estimate and the margin of error (moe).\n\n## FILLED VERSION\nglimpse(iowa_income_df)\n\nRows: 99\nColumns: 5\n$ GEOID    &lt;chr&gt; \"19001\", \"19003\", \"19005\", \"19007\", \"19009\", \"19011\", \"19013\"…\n$ NAME     &lt;chr&gt; \"Adair County, Iowa\", \"Adams County, Iowa\", \"Allamakee County…\n$ variable &lt;chr&gt; \"med_income\", \"med_income\", \"med_income\", \"med_income\", \"med_…\n$ estimate &lt;dbl&gt; 63172, 64750, 64049, 50684, 54973, 79444, 62329, 75759, 84727…\n$ moe      &lt;dbl&gt; 4508, 10683, 2577, 4665, 6341, 4092, 2004, 5710, 3606, 4553, …\n\n\n\n## EMPTY VERSION\n# glimpse(____)\n\nNow, let’s create a simple plot of the 10 counties with the highest median income.\n\n## FILLED VERSION\niowa_income_df %&gt;%\n  slice_max(order_by = estimate, n = 10) %&gt;%\n  ggplot(aes(x = estimate, y = reorder(NAME, estimate))) +\n  geom_col(fill = \"dodgerblue\") +\n  labs(\n    title = \"Top 10 Iowa Counties by Median Household Income (2022)\",\n    x = \"Median Household Income (USD)\",\n    y = \"County\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n## EMPTY VERSION\n# iowa_income_df %&gt;%\n#   slice_max(order_by = ____, n = ____) %&gt;%\n#   ggplot(aes(x = ____, y = reorder(NAME, ____))) +\n#   geom_col(fill = \"dodgerblue\") +\n#   labs(\n#     title = \"____\",\n#     x = \"____\",\n#     y = \"____\"\n#   ) +\n#   theme_minimal()\n\nThis process—finding variables, requesting data by geography, and getting a clean data frame—is the core workflow of tidycensus, making it incredibly powerful for demographic analysis."
  },
  {
    "objectID": "about.html#what-is-data-extraction",
    "href": "about.html#what-is-data-extraction",
    "title": "About",
    "section": "",
    "text": "Data extraction refers to retrieving and transforming data from sources like web APIs, HTML tables, or other online services into usable form. This session will cover:\nSession Table of Contents\n\n\n\n\nSession\n\n\nTopic\n\n\n\n\n\n\n1\n\n\nIntroduction to APIs & Data Extraction\n\n\n\n\n2\n\n\nWorking with Weather APIs (Querying public APIs, Parsing JSON)\n\n\n\n\n3\n\n\nScraping Sports Data via HTML Tables\n\n\n\n\n4\n\n\nFinal Project: Hands-on Extraction & Reflection"
  },
  {
    "objectID": "about.html#about-dr.-immanuel-williams",
    "href": "about.html#about-dr.-immanuel-williams",
    "title": "About",
    "section": "👤 About Dr. Immanuel Williams",
    "text": "👤 About Dr. Immanuel Williams\n\n\n\n\n\n🎓 Education & Background\n\nPh.D. in Educational Psychometrics (Statistics & Psychology) — Rutgers University\n\nM.S. in Statistics — Rutgers University\n\nB.S. in Mathematics — University of Maryland, Baltimore County\n\n\n\n🔬 Research & Publications\n\n“Culturally Relevant Data in Teaching of Statistics and Data Science Courses” (Journal of Statistics & Data Science Education, 2023; Travis Weiland & Immanuel Williams) – explores how contextualized, culturally meaningful datasets can enhance student engagement.\nR Shiny app for teaching confidence intervals using NBA data (2017) – integrates interactive visualizations to support statistical learning.\nAdditional investigations into statistics education, culturally relevant pedagogy, and data science integration in instruction.\n\n\n\n🏫 Professional Profile\n\nAssistant Professor, Department of Statistics, Cal Poly, San Luis Obispo :contentReferenceoaicite:4\n\nCEO & curriculum developer, GATO365 Learning Center, focused on empowering diverse students in data science and statistics"
  },
  {
    "objectID": "about.html#contact-personal-interests",
    "href": "about.html#contact-personal-interests",
    "title": "About",
    "section": "📬 Contact & Personal Interests",
    "text": "📬 Contact & Personal Interests\n\nEmail: imwillia@calpoly.edu\n\nOffice: Cal Poly, Room 25‑113\n\nInterests: Teaching data science, R Shiny apps, culturally relevant pedagogy, live data extraction, saxophone performance\n\n\nThis page highlights both my teaching philosophy around Extraction, Transformation, and Visualization (ETv) and my background in developing curricula and research centered on engaging, real-world data."
  },
  {
    "objectID": "sessions/session_2/02_Extraction_Weather_Data_API.html#reflection-make-questions-within-coursekata-to-solidyfy-approach",
    "href": "sessions/session_2/02_Extraction_Weather_Data_API.html#reflection-make-questions-within-coursekata-to-solidyfy-approach",
    "title": "Session 2: Weather Data - OpenWeatherAPI",
    "section": "4. Reflection (make questions within CourseKata to solidyfy approach)",
    "text": "4. Reflection (make questions within CourseKata to solidyfy approach)"
  },
  {
    "objectID": "sessions/session_2/02_Extraction_Weather_Data_API.html#misc-questions",
    "href": "sessions/session_2/02_Extraction_Weather_Data_API.html#misc-questions",
    "title": "Session 2: Weather Data - OpenWeatherAPI",
    "section": "5. Misc Questions",
    "text": "5. Misc Questions"
  },
  {
    "objectID": "sessions/session_1/01_Extraction_Introduction.html#workshop-logistics",
    "href": "sessions/session_1/01_Extraction_Introduction.html#workshop-logistics",
    "title": "Session 1: Introduction To Extraction Workshop - HTML & APIs",
    "section": "0. Workshop Logistics",
    "text": "0. Workshop Logistics\n\nHow We’ll Work Together\n\nInteractive Format: This workshop is designed to be interactive, with a mix of short lectures followed by hands-on activities.\nQuestions Are Highly Encouraged: Curiosity is key to learning, so please ask anything that comes to mind. To keep us on track and ensure all topics are covered, we’ll handle questions as follows:\n\nIf your question is about a topic we will get to later, I’ll make a note of it and we will address it when we reach that section.\nIf a question is beyond the scope of this particular workshop, I’ll let you know, and we can discuss it after the session.\n\n\n\n\nOur Technical Setup\n\nPlatform: All of our work today will be done within the CourseKata platform. This environment is pre-configured to ensure everything runs smoothly for everyone.\nPackages and Libraries: The platform comes with almost all required R packages pre-installed. We will only need to install one package before we need it, and I will guide you through that simple process.\nFill-in-the-Blank Exercises: Our coding activities use a “fill-in-the-blank” style. The goal is to help you focus on the core concepts without getting bogged down by minor syntax details.\nClear Function Calls: For maximum clarity, we will explicitly name all arguments in function calls (e.g., read_html(url = my_url)). This makes it easy to see exactly what each part of the function is doing."
  },
  {
    "objectID": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html#p1.-review-of-key-functions-and-concepts",
    "href": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html#p1.-review-of-key-functions-and-concepts",
    "title": "Session 3: NFL Sports Data",
    "section": "P1. Review of Key Functions and Concepts",
    "text": "P1. Review of Key Functions and Concepts\nOf course. Here is a structured guide with concise explanations and sample data to effectively teach these R concepts for a web scraping workshop. The goal is to give attendees a clear understanding of what the code does without getting lost in details.\n\n\n\n\n1. Extracting Items with pluck 🤏\npluck() from the purrr package is for safely pulling out a single piece of data from a list or a data frame column that contains lists.\nFake Data: Imagine you scraped data and have a list of authors for each book.\n\nlibrary(tibble)\nlibrary(purrr)\n\nbook_data &lt;- tibble(\n  title = c(\"The Hobbit\", \"Dune\"),\n  details = list(\n    list(author = \"Tolkien\", year = 1937),\n    list(author = \"Herbert\", year = 1965)\n  )\n)\n\nbook_data\n\nCode Demo: Let’s “pluck” the author from the first book’s details.\n\n# Get the 'author' from the first element of the 'details' column\npluck(book_data, \"details\", 1, \"author\")\n\nThis is like telling R: go into book_data, find the details column, go to its first element, and grab the value named author.\n\n\n\n\n\n2. Cleaning Data with janitor 🧹\nThe janitor package is your best friend for cleaning up messy data.\nFake Data: Data scraped from a website often has messy column names and extra empty spaces.\n\nlibrary(janitor)\n\nmessy_data &lt;- tibble(\n  `First Name` = c(\"gandalf\", \"frodo\", NA),\n  `Last Name` = c(\"the grey\", \"baggins\", NA),\n  `AGE` = c(\"2019\", \"50\", NA)\n)\n\nmessy_data\n\n\n\n\nCode Demos:\n\nclean__names() makes column names neat and consistent (snake_case).\n\n\ncleaned_data &lt;- clean_names(messy_data)     # colnames(cleaned_data)     \ncleaned_data\n\n\nremove_empty() gets rid of empty rows or columns.\n\n\n# The third row is empty, so it gets removed     \nremove_empty(cleaned_data, which = \"rows\")     \n\n\nadorn_totals() is powerful for quickly adding sums.\n\n\n# Let's use a numeric data frame     \nsales &lt;- tibble(         \n  product = c(\"A\", \"B\"),         \n  q1_sales = c(100, 200),         \n  q2_sales = c(150, 250))  %&gt;%  \n  adorn_totals(sales, where = \"row\")     \n\nsales\n\n\n\n\n\n\n3. Simple Renaming with rename ✍️\nThe rename() function from dplyr is the most direct way to rename one or more specific columns. The pattern is always new_name = old_name.\nFake Data: Let’s use our cleaned data from before.\n\nlibrary(dplyr)\n\n#&gt; # A tibble: 3 × 3\n#&gt;   first_name last_name age\n#&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;\n#&gt; 1 gandalf    the grey  2019\n#&gt; 2 frodo      baggins   50\n#&gt; 3 NA         NA        NA\n\nCode Demo: Let’s change age to character_age.\n\ncleaned_data %&gt;%\n  rename(character_age = age)\n\n\n\n\n\n\n4. Advanced Renaming with rename_with 🔁\nrename_with() is for changing many column names at once using a pattern. It’s perfect when you want to apply the same rule to multiple columns.\nFake Data: Imagine your scraped data has a weird prefix on several columns.\n\ndata_with_prefix &lt;- tibble(\n  `_name` = c(\"Aragorn\", \"Legolas\"),\n  `_race` = c(\"Human\", \"Elf\"),\n  `id` = c(1, 2)\n)\n\nCode Demo: Let’s remove the leading underscore _ from every column that starts with one.\n\nlibrary(stringr)\n\ndata_with_prefix %&gt;%\n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))\n\n\nrename_with(...): “I want to rename some columns.”\n~ str_replace(., \"^_\", \"\"): The rule is to replace a ^ (start of the text) followed by _ with nothing (\"\"). The . is a placeholder for the column name.\n.cols = starts_with(\"_\"): “Only apply this rule to columns that start with _.”\n\n\n\n\n\n\n5. Selecting Columns with ! and : 🎯\nYou can use select() from dplyr to keep or remove columns.\n\n: (colon) selects a range of columns.\n! (bang) removes a column.\n\nFake Data:\n\ncharacter_stats &lt;- tibble(\n  name = c(\"Gimli\", \"Samwise\"),\n  race = c(\"Dwarf\", \"Hobbit\"),\n  weapon = c(\"Axe\", \"Frying Pan\"),\n  age = c(139, 33)\n)\n\nCode Demos:\n\n# 1. Select all columns FROM 'race' TO 'weapon'\ncharacter_stats %&gt;% \n  select(race:weapon)\n\n\n# 2. Select everything EXCEPT the 'age' column\ncharacter_stats %&gt;% \n  select(!age)\n\n\n\n\n\n\n6. Understanding Regular Expressions (regex) 🧐\nA regular expression (or regex) is a pattern used to find and match text. It looks weird, but it’s just a set of rules.\nLet’s break down this regex for finding numbers: ^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\n\n\n\n\n\n\n\n\nPart\nMeaning\nExample Matches\n\n\n\n\n^\nStart of the string\n\n\n\n\\\\s*\nZero or more whitespace characters (\\s)\n(space)\n\n\n-?\nAn optional hyphen (-)\n-\n\n\n\\\\d*\nZero or more digits (\\d)\n123, ``\n\n\n\\\\.?\nAn optional literal dot (`.`)\n`.`\n\n\n`\\d+`\nOne or more digits (`\\d`)\n`45`, `6`\n\n\n`\\s*`\nZero or more whitespace characters\n` ` (space)\n\n\n`$`\nEnd of the string\n\n\n\n\nIn English: This pattern looks for strings that start (^), might have some spaces (\\s*), might have a minus sign (-?), might have some digits (\\d*), might have a decimal point (\\.?), must have at least one digit (\\d+), might have more spaces (\\s*), and then must end ($).\nThis pattern will match \"5\", \" -10.5 \", and \".5\" but will not match \"5a\", \"-$5\", or \"1.2.3\".\n\n\n\n\n\n7. across() for Mass Changes ⚡️\nacross() is a super-powerful dplyr function that lets you apply the same operation to multiple columns at once. It’s almost always used inside mutate().\nFake Data: Scraped data often reads numbers as text.\n\nscraped_measurements &lt;- tibble(\n  id = c(\"A\", \"B\", \"C\"),\n  width = c(\"10.5\", \"8\", \"9.1\"),\n  height = c(\" 5.2 \", \"-3\", \"7.7\"),\n  comment = c(\"ok\", \"good\", \"perfect\")\n)\n\nCode Demo: Let’s convert every column that looks like a number into an actual numeric type. We’ll use the regex from before.\n\nnumeric_data &lt;- scraped_measurements %&gt;%\n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.)))\n\nstr(numeric_data)\n\n\nmutate(across(...)): We are changing columns.\nwhere(~ all(grepl(...))): This part selects the columns. It tests every column (where) to see if (~) all of its values match (grepl) our numeric regex pattern. It finds width and height.\n~ as.numeric(.): This part is the action. It takes the selected columns (.) and applies the as.numeric function to them.\n\n\n\n\n\n\n8. Conditional Logic with case_when 🤔\ncase_when() is for creating a new column based on a set of if/then rules. It’s much easier to read than nested ifelse() statements. The syntax is condition ~ value.\nFake Data: Let’s use the numeric data we just created.\n\nnumeric_data\n\nCode Demo: Let’s create a size_category based on the width column.\n\nnumeric_data %&gt;%\n  mutate(\n    size_category = case_when(\n      width &gt;= 10   ~ \"Large\",\n      width &gt;= 9    ~ \"Medium\",\n      TRUE          ~ \"Small\"\n    )\n  )\n\n\ncase_when() checks each rule in order. The first one that is TRUE wins.\nwidth &gt;= 10 ~ \"Large\": If width is 10 or more, the value is “Large”.\nwidth &gt;= 9 ~ \"Medium\": If not, but width is 9 or more, it’s “Medium”.\nTRUE ~ \"Small\": This is the catch-all. If none of the above rules were met, the value is “Small”. TRUE always evaluates to true, so it works like an else statement."
  },
  {
    "objectID": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html#p2.-html-concepts",
    "href": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html#p2.-html-concepts",
    "title": "Session 3: NFL Sports Data",
    "section": "P2. HTML Concepts",
    "text": "P2. HTML Concepts\n\nHow HTML Builds a Web Page\nTo understand web scraping, we first need to see how a web browser reads HTML code to display a page.\n\n1. The Rendered Page (What You See)\nThis is the final, visual webpage that you interact with in a browser. It’s designed to be human-readable.\n\n\n\n\n\n\n\n\n\n\n2. The HTML Code (What the Computer Sees)\nBehind every webpage is the raw HTML code. This code acts as the blueprint, telling the browser what content to display and how to structure it.\n\n\n\n\n\n\n\n\nFinding a Specific Element: The Table\nLet’s focus on a specific part of the page: the data tables.\n\n\n\n\n1. The Tables on the Rendered Page\nWhen we scrape, our goal is to extract specific information, like the data in these two tables.\n\n\n\n\n\n\n\n\n\n\n2. The &lt;table&gt; Tags in the HTML Code\nTo extract those tables, we need to find the code that creates them. In HTML, tables are defined by &lt;table&gt; tags, which contain all the data and structure for a specific table.\n\n\n\n\n\n\n\n\n\n\n\nAnatomy of an HTML Table\nAn HTML table is built from a set of nested tags that define its structure and content.\n\n&lt;table&gt;&lt;/table&gt; This tag is the main container that wraps around all the content for an entire table.\n&lt;thead&gt;&lt;/thead&gt; This tag defines the header section of a table, where you place the row containing the column titles.\n&lt;th&gt;&lt;/th&gt; A table header cell, used for column titles. Text inside a &lt;th&gt; tag is typically bold and centered by default.\n&lt;tr&gt;&lt;/tr&gt; A table row, which groups together all the cells (&lt;th&gt; or &lt;td&gt;) that belong in a single horizontal line.\n&lt;td&gt;&lt;/td&gt; A table data cell, which contains the individual pieces of data in the table’s body."
  },
  {
    "objectID": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html#p3.-desired-rvest-functions",
    "href": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html#p3.-desired-rvest-functions",
    "title": "Session 3: NFL Sports Data",
    "section": "P3. Desired rvest functions",
    "text": "P3. Desired rvest functions\n\nP_sub_1: Reading the HTML File\nFirst, we load our libraries and read the HTML file into an R object. This step remains the same.\n\n\nlibrary(rvest)\nlibrary(purrr)\nlibrary(here)\n\n\n# Read the local HTML file into an R object\niowa_html_page &lt;-  here(\"sessions\",\"session_3\",\"fake_iowa_data.html\") %&gt;% \n  read_html()\n\n\n\n\n\n\nP_sub_2: Extracting the First Table with pluck()\nInstead of using [[1]], we can pipe the output of html_table() directly into pluck(). We tell pluck() to retrieve the first element from the list of tables.\n\n# Use pluck() to get the first table from the list\nproduction_df &lt;- iowa_html_page  %&gt;% \n  html_table() %&gt;% \n  pluck(1)\n\nprint(production_df)\n\n\n\n\n\n\nP_sub_3: Extracting the Second Table with pluck()\nWe can use the exact same process to get the second table. We simply change the index in pluck() from 1 to 2 to retrieve the second element from the list.\n\n# Use pluck() to get the second table\nusage_df &lt;- page %&gt;% \n  html_table() %&gt;% \n  pluck(2)\n\nprint(usage_df)"
  },
  {
    "objectID": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html#p4.-extract-nfl-data",
    "href": "sessions/session_3/03_Extraction_of_Data_NFL_HTML.html#p4.-extract-nfl-data",
    "title": "Session 3: NFL Sports Data",
    "section": "P4. Extract NFL Data",
    "text": "P4. Extract NFL Data\nLet’s go to this html page.\nhttps://www.pro-football-reference.com/teams/was/2023.htm#games\nOur desired table is the 2023 Regular Season Table for the Washington Commanders, which is the second table on the webpage.\n\n\n\nThis is the table we are after\n\n\n\nStep 1: Retrieve the Game Data\nFirst, we define our parameters (team and year), build the full URL, read the HTML from the page, and then use rvest and purrr to extract the specific table containing the game data we want.\n\n# ## EMPTY VERSION\n# library(httr2)\n# library(rvest)\n# library(glue)\n# library(purrr)\n# \n# # Define team and year\n# team_name &lt;- \"____\"\n# year &lt;- ____\n# \n# # Construct full URL\n# generic_url &lt;- glue(\"https://www.pro-football-reference.com/teams/{____}/{____}.htm#all_games\")\n# \n# # Read HTML page and extract tables\n# webpage &lt;- read_html(____)\n# web_tables &lt;- html_table(____)\n# \n# # Pluck the second table (regular season games)\n# game_table &lt;- pluck(____, ____)\n\n\n## FILLED VERSION\nlibrary(httr2)\nlibrary(rvest)\nlibrary(glue)\nlibrary(purrr)\n\n# Define team and year\nteam_name &lt;- \"was\"\nyear &lt;- 2023\n\n# Construct full URL\ngeneric_url &lt;- glue(\"https://www.pro-football-reference.com/teams/{team_name}/{year}.htm#all_games\")\n\n# Read HTML page and extract tables\nwebpage &lt;- read_html(generic_url)\nweb_tables &lt;- html_table(webpage)\n\n# Pluck the second table (regular season games)\ngame_table &lt;- pluck(web_tables, 2)\n\n\n\n\nStep 2: Clean the Column Headers\nScraped tables often have messy headers. The first row of this table contains the real column names. We will extract that row, set it as the new headers, remove the now-redundant first row, and finally use the janitor package to standardize the names into a clean format.\nNote: Sometimes we have to refer to base R syntax (like colnames() and [-1,]) because modern tidyverse methods may not be the most direct tool for a specific, low-level task like this.\n\n# ## EMPTY VERSION\n# library(janitor)\n# \n# # Use first row as column names\n# firstrow_names &lt;- ____[1, ] %&gt;% \n#   unlist() %&gt;% \n#   as.character()\n# \n# # Assign as column names\n# colnames(____) &lt;- ____\n# \n# # Remove the first row and clean names\n# table_1 &lt;- ____[-1, ]\n# table_2 &lt;- clean__names(____)\n\n\n## FILLED VERSION\nlibrary(janitor)\n\n# Use first row as column names\nfirstrow_names &lt;- game_table[1, ] %&gt;% \n  unlist() %&gt;% \n  as.character()\n\n# Assign as column names\ncolnames(game_table) &lt;- firstrow_names\n\n# Remove the first row and clean names\ntable_1 &lt;- game_table[-1, ]\n# table_2 &lt;- clean__names(table_1)\n\n\n\n\nStep 3: Tidy and Transform the Data\nThe final step is to reshape the data into a truly usable format. This involves renaming poorly named columns, removing irrelevant columns and bye weeks, converting columns that look like numbers from text to numeric, and correctly setting categorical variables (factors).\n\n# ## EMPTY VERSION\n# library(dplyr)\n# library(stringr)\n# \n# # Rename, select, and filter\n# table_4 &lt;- table_2 %&gt;% \n#   rename(\n#     result = ____,\n#     game_location = ____\n#   ) %&gt;% \n#   select(!(____:____)) %&gt;% \n#   filter(opp != \"____\")\n# \n# # Convert types\n# table_6 &lt;- table_4 %&gt;%  \n#   mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.))) %&gt;% \n#   mutate(\n#     result = as.factor(____),\n#     game_location = case_when(\n#       game_location == \"@\" ~ \"____\",\n#       game_location == \"\" ~ \"____\",\n#       TRUE ~ ____\n#     ) %&gt;% as.factor()\n#   )\n# \n# # Final column name cleanup\n# final_games_df &lt;- table_6 %&gt;% \n#   rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"____\"))\n\n\n## FILLED VERSION\nlibrary(dplyr)\nlibrary(stringr)\n\n# Rename, select, and filter\ntable_4 &lt;- table_2 %&gt;% \n  rename(\n    result = x_3,\n    game_location = x_4\n  ) %&gt;% \n  select(!(x:x_2)) %&gt;% \n  filter(opp != \"Bye Week\")\n\n# Convert types\ntable_6 &lt;- table_4 %&gt;%  \n  mutate(across(where(~ all(grepl(\"^\\\\s*-?\\\\d*\\\\.?\\\\d+\\\\s*$\", .x))), ~ as.numeric(.))) %&gt;% \n  mutate(\n    result = as.factor(result),\n    game_location = case_when(\n      game_location == \"@\" ~ \"away\",\n      game_location == \"\" ~ \"home\",\n      TRUE ~ game_location\n    ) %&gt;%  as.factor()\n  )\n\n# Final column name cleanup\nfinal_games_df &lt;- table_6 %&gt;% \n  rename_with(~ str_replace(., \"^_\", \"\"), .cols = starts_with(\"_\"))"
  }
]